[{
      "id": 0,
      "url": "/blog/foss-licensing-usage/",
      "title": "DocOps Lab Licensing and Usage",
      "content": "Let&#8217;s talk about permissively licensed software and how it is best used and managed downstream. Most people&#8201;&#8212;&#8201;including software developers&#8201;&#8212;&#8201;interact with most software as consumers or end users. Nevertheless, open source software often seems to be packaged for other developers. This is a guide for new developers or non-devs who need to incorporate FOSS (free open source software) into their own projects. This blog entry is meant to help demystify the world of runtime-based open source software. It is meant to empower rather than intimidate, just like the best open source software does. DocOps Lab software is typically released in both source code and binary/packaged forms, for users' convenience. The code is released on an interactive platform (GitHub) so you can investigate it, extend it, or help fix and improve it. The binaries and packages are released so you can install and execute it without including the source code in your own projects. This way even if you share your project as open source, any open source software it uses unmodified can be referenced as a dependency rather than packaged in your codebase. Likewise, DocOps Labs' documentation is similarly shared as both source code and rendered HTML/PDF, also under these exact same conditions. Use it as you wish, help improve it, even re-brand it&#8201;&#8212;&#8201;the only requirement is maintaining the &#8220;upstream&#8221; chain of attribution. We mainly care about this for accountability, not credit (which we don&#8217;t care about) or control (which we fully relinquish). Our licensing says you can use it however you want, but if you re-share the code, it must remain under the same license and be attributed to DocOps Lab. Note that you do NOT have to share it or changes you make to it, and also note that we do not really care if you keep the attribution or not, whether you release a modified version or not. We just want to make it easy for you to use our tools, and we want to be able to point to a common codebase that we can all work on together (or not). How to Use DocOps Lab Software If you want to use our software, there is a near certainty that it will not &#8220;just work&#8221;, so to speak, &#8220;out of the box&#8221;, so to speak. Modified vs Packaged There are a few ways to take advantage of DocOps Lab software and content. The most common way would be as a compiled-and-packaged dependency in your own project, perhaps as a Ruby gem a Docker image. This way, you do not need to fuss over licensing at all. If you do want to modify the code, you can fork it and make changes, then use your own modified version as a dependency in your own project. You do not need to re-share or contribute back your changes. Modified code can be used in many ways as well, including copied and pasted snippets, or even just as inspiration for your own code. But of course you can also make a wholesale copy, modify it, and rebuild and package it for yourself, then cite that dependency in your Gemfile or Dockerfile. The goal of any open source project should be a balance between extensibility and configurability. Most users needing to customize should be able to do so by tweaking powerful settings or an application programming interface (API) or a domain-specific language (DSL) without needing to modify the project&#8217;s source code. Yet for users with extraordinary needs, or perhaps after the project has been abandoned, the ability to fork and modify the code is essential. However, source code modification is not the topic of the day. In fact, it&#8217;s the opposite. We are going to review all the ways the kinds of &#8220;runtime tools&#8221; we use most in docs-as-code platforms can be modified and used without touching their source code. Make FOSS Work for You There are many ways to interact with and amend the behavior of software before you should ever consider hacking the source. True programmers do their best to avoid modifying the source code of products they wish to use but not to maintain. CLIs Command-line interfaces can be run from any UNIX-like shell environment and manipulated with arguments, options, flags, and environment variables. This is likely the fastest way to leverage any DocOps Lab tools. CLI commands can be scripted. You can string them together, run them conditionally, run them in loops, and so forth. Commands are simply instructions, and instructions can be concatenated into scripts. The output can be piped to other commands for further processing. CLIs can be used in combination with configuration, domain-specific languages, and extensions. APIs There are two broad categories of APIs: native and remote. Native APIs are libraries or modules designed to be used in a given programming environment, such as Java, Python, JavaScript, or in our case mostly Ruby. Most people in tech are more familiar with remote APIs, specifically RESTful APIs, but also GraphQL, SOAP, and other kinds. In both cases, an API is a system that awaits input in an expected format and returns output in a documented format&#8201;&#8212;&#8201;usually some kind of data structure. In the case of remote APIs, the output is typically JSON delivered over HTTP. Configs Most command-line tools can be configured with local files that stablish a baseline of settings and behaviors. Configuration files, or &#8220;configs&#8221; , are a kind of interface, though they tend to be application-specific in structure, syntax, and nomenclature. DocOps Lab prefers YAML-formatted configuration files, but sometimes we use INI format. Configs are constrained by whatever the product&#8217;s developers predicted users would want to do. It often means establishing persistent preferences for the types of things CLIs and APIs allow you to indicate at runtime using arguments. Good Configs are well documented, but truthfully configuration docs often get de-emphasized, especially in open source projects. DSLs Domain-specific languages (DSLs) are mini programming syntaxes designed to express concepts in a particular domain, usually with modest scope. DSLs can be their very own syntax, or they can use an existing syntax like YAML or XML in a predefined manner, expecting particular keys with a given range of values to script or configure user-defined outcomes for a given set of inputs. Examples of DSLs you might be familiar with include: GitHub Actions workflow files Kubernetes manifests Dockerfiles SCSS/SASS and LESS for CSS pre-processing DocOps Lab prefers to use YAML-based DSLs wherever possible, as they are easy to read and write, and widely supported. Templates Templates are a kind of DSL that define how input textual and variable data is to be transformed into output content. Templates use a syntax that is interpreted by a rendering engine. This engine accepts data from an outside source (YAML or JSON or a relational database) and merges it into the template structure to produce predictable output. Templates are often used in DocOps Lab projects to generate HTML, Markdown, AsciiDoc, YAML, or other text-based formats. DocOps Lab prefers Liquid templates and uses an extended version of Jekyll&#8217;s Liquid 4 engine. Extensions/Plugins Forward-thinking developers instill their software with &#8220;hooks&#8221; so that downstream devs can extend the product&#8217;s behavior using its native source syntax or a DSL. While some DSLs and templating systems are used to perform tasks the product developer predicted most users would need, extensions are for needs the developer did not anticipate or has not yet implemented. Extensions are often packaged as plugins or modules that can be installed alongside the main product. Developers themselves often provide plugins for optional features that might not be optimal as a built-in aspect of the core product. Other times, plugins are created by third-party developers to add functionality the original developers did not envision. These &#8220;community extensions&#8221; sometimes get merged into the official product. Any user can develop an extension and not share it. This is common for extensions that solve an internal use case but are not designed for public consumption. A local extension can even include hard-coded data or proprietary logic that would not be appropriate for open source sharing. Licensing Choices and Implications In some ways, the distinctions between open source licenses do not matter very much. If you just want to use some FOSS utility or another, even for professional or commercial purposes, most licenses will allow you to do so with no strings attached. Licensing starts to matter when you want to package or copy source code into your own projects, especially if you intend to share your own project as open source. Why Licenses? Software you intend to share needs a license. What many people misunderstand is that unlicensed software you find on GitHub or elsewhere is still copyrighted, and you technically are NOT supposed to reuse it in your own projects. Truthfully, a lot of shared code just happens to be lacking a license out of neglect, and it&#8217;s fine for you to reuse. Most of the engineering shops I&#8217;ve worked at disallow incorporating unlicensed code even if the developer obviously intended to share it. You can always post an Issue on a project&#8217;s GitHub repo to ask the author to add a permissive license. So we have to have a license to make it clear our code is freely copyable and reusable. Why MIT? By some stroke of coincidence or culture or convention, nearly everything in our particular toolchain (meaning the applications we use to develop our own) is MIT licensed, so it makes sense to use the same license for our own code. MIT License totally permissive; it should not trouble anyone&#8217;s manager or CTO or Legal Department, and that&#8217;s that. There are lots of other considerations deep thinkers fuss over, but I have not found time to care much about the nuance politics of FOSS licensing. MIT works. Apache works. BSD works. Knock yourself out. Why CC BY 4.0? We release some extra documentation like websites and educational materials under Creative Commons Attribution 4.0 International (CC BY 4.0) because it is similarly permissive like MIT and widely understood, and because we quite like CC and their efforts in the world of knowledge sharing. CC BY indicates a requirement to note what changes are made if you redistribute the so-licensed content, so for stuff like Docs-as-Code School or this the main DocOpsLab.org website. What Does All This Mean? In practical terms, you can use DocOps Lab software and documentation in your own projects, whether personal or commercial, without worrying about licensing fees or restrictions. You can modify it, adapt it, and even redistribute it, and we don&#8217;t even care about credit, so long as you leave some trail back to the source. The only real requirement is that if you do redistribute the source code itself, it must remain under the same license."
    },{
      "id": 1,
      "url": "/blog/release-the-lab/",
      "title": "Release the Lab!",
      "content": "Today I release a major update to the DocOps Lab website, reporting for the first time on all the projects underway or planned, including their statuses. The site is heavily data driven, mainly based on a complex YAML file maintained for the past 2+ years to track projects, their relationships, progress, and so forth. Now it informs a static website built with Jekyll and Asciidoctor, which is hosted on GitHub Pages. Check out the Git repository and README for more details. Also this MetaBlog post explores the source data and site generation in more detail. The site now contains &#8220;reports&#8221;, which group and arrange project status updates by various criteria, such as the type of application, the technologies involved, and the target launch date. Certain featured projects have their own profile pages, such as AYL DocStack and Docs-as-Code School, providing more details. I am keeping this blog post short as a way of encouraging myself to post more frequently. I will try to keep the progress of various DocOps Lab projects updated on the site, and I will also do my best to post more detailed updates here on the blog."
    },{
      "id": 2,
      "url": "/blog/single-sourcing-for-ai-agents/",
      "title": "Building Docs for AI Agents from Single-Sourced Content",
      "content": "I build alternate versions of my developer documentation, specially for consumption by LLM-backed coding agents. These documents begin life as AsciiDoc files oriented toward human users, but I selectively transclude content into AI-specific documentation files. Then the files are converted to Markdown (via HTML) for better compatibility with LLMs. I then use a tool that lets me sync those agent-oriented docs from a central source into all my code repositories. This way any LLM agents have ready access to a library of specific skills or protocols they may be called upon to use, without overwhelming them with entire sets of remote, HTML-laden documents in every session. If this sounds convoluted, hear me out. Source Content The source content for my AI-agent-oriented docs lives in the same AsciiDoc files I use for human developer documentation. In my experience, LLMs are more adept at consuming examples and bulleted lists, and they prefer these in Markdown format with some HTML tags. However, complex tech docs are best authored in a structured format like AsciiDoc, reStructuredText, DITA, or MadCap Flare. For me, this means authoring in AsciiDoc, converting to HTML, and then reverting to Markdown. Why not just write the AI docs in Markdown? If all your docs are already sourced in pure Markdown of one flavor or another, you have a head start in this process. You can basically just show the source files to your LLM agent&#8230;&#8203; so long as your docs do not need to be assembled. But for structured authoring with strict single sourcing and transclusion, source files are incomplete until they are rendered to another format. Assuming you wish to source your AI-agent docs along with people docs, you will probably want to render each document, and you further want to slim down your docs. Most advanced static-site generators (SSGs) and doc generators can render complex docs from Markdown-like sources, but conditional transclusion usually requires mixing in preprocessor templates. Whereas formats like AsciiDoc, reStructuredText, DITA, and Flare support this feature natively. And because we love AsciiDoc at DocOps Lab, we&#8217;re usually going to find a way to avoid actually having to author in (or even read) Markdown. Assuming you want to selectively include content for AI agents, use AsciiDoc tagging to indicate sections or blocks to include or exclude. For example, my original, people-focused documentation on how to interact with Git does not assume much prior knowledge of Git commands. Whereas LLMs definitely know how to use Git; they are basically experts. So all I need to convey is the specific procedures preferred for DocOps Lab projects. Here is how I tag the relevant content in my AsciiDoc source files: Original AsciiDoc source snippet // tag::repo-state[] [[repo-state]] == Repository State Development is done on development _trunk_ branches named like `dev/x.y`, where `x` is the major version and `y` is the minor. To start development on a new release version: .... git checkout main git pull origin main git checkout -b dev/1.2 git checkout -b chore/bump-version-1.2.0 git commit -am \"Bumped version attributes in README\" git checkout dev/1.2 git merge chore/bump-version-1.2.0 git push -u origin dev/1.2 .... // end::repo-state[] From there it is just a matter of creating a set of AsciiDoc files that use the include:: directive to pull in the tagged content. This way I can skip verbose introductory or beginner-oriented content that is unnecessary for LLMs. Using include directive to embed single-sourced content [[basics]] == The Basics . Follow proper branching procedures as outlined in &lt;&lt;repo-state&gt;&gt;. . Commit messages should be concise and easy for users to edit. + See &lt;&lt;commit-messages&gt;&gt; for guidance. . Always prompt user to approve commits before pushing. . Use `gh` for interacting with GitHub whenever possible. + See &lt;&lt;gh-cli&gt;&gt; for more information. include::../task/development.adoc[tag=repo-state] Generating AI-Agent Docs The best way to get Markdown from AsciiDoc files is to perform an HTML conversion and then downgrade to Markdown. There are numerous tools for carrying this latter step, not the least of them the beloved Pandoc. I have modified a Ruby library called ReverseMarkdown to accommodate AsciiDoc&#8217;s richer semantics. My extension is available as scripts/mark_down_grade.rb in this very repo. Here I include a window into the current state of one such document (source, rendered), which may change over time as I refine the AI-agent docs: Unresolved directive in single-sourcing-for-ai-agents.adoc - include::../gems/docopslab-dev/docs/agent/skills/git.md[] I am quite happy with the twice-converted Markdown output. Having spent a decade publishing AsciiDoc to HTML and PDF, this experience of publishing to Markdown has been fun. Distribution This is the extra-credit section of the blog entry. It only pertains to organizations or projects that maintain multiple repos or authors who need to lint textual content across multiple projects with a single voice. This matter of AI-oriented docs came about as a side effect of my need to centrally maintain a series of helper utilities like code and text linters backed by customizable libraries. In order to make sure all of my many concurrent projects have access to the latest customizations and configurations of the tools they all depend on, I built a common dependency across all my repos, just for managing these shared assets. The specifics of this tool are not all that important; I will leave them for a separate post. The trick is to use whatever resources are available to you to ensure your docs and helper tooling are consistent across your team and accessible to all AI agents. Once you have a library of topical documents for your AI agents to use, make sure they are aware of them by indicating their location in your project&#8217;s AGENTS.md or CLAUDE.md file."
    },{
      "id": 3,
      "url": "/blog/true-single-sourcing/",
      "title": "TRUE Single Sourcing with YAML and AsciiDoc",
      "content": "Documentation and application should derive all key data from a true single source of truth (TSST) defined once and conveyed across all product and documentation builds. This approach conveys more advantages than you might think at first gloss, including testability and cooperative design through Git-tracked specification and definition. My method is to use AsciiDoc README.adoc and YAML files to assign all kinds of key product data, including structured data for interface definition and reference documentation. This post is a doozie, so let&#8217;s TOC it out. Table of Contents The Example of OpenAPI Why Stop There? About Those Other Interface Types The Advantages of YAML-based TSST Single Sourcing in `README.adoc` Generating Docs with Templating Engines Truth and Purism The Example of OpenAPI There are few universally known technologies across all of programming and software technical writing, but one of them is OpenAPI Specification (OAS), a standardized data format used to &#8220;describe&#8221; or &#8220;define&#8221; server application interfaces that honor the standard RESTful HTTP architecture and protocol. That is, one &#8220;language&#8221; can be used to detail just about everything anyone would need to know about how a given REST API is supposed to work, at least in terms of what endpoints do what with a given set of data and a given method (POST, GET, PUT, DELETE). OAS is a great example of a &#8220;true single source of truth&#8221; (TSST) when used to define the API itself as well as the documentation downstream developers use to make informed connections to the API. Defining relatively complex interfaces in YAML can apply to much more than just REST APIs. If you&#8217;re having trouble recalling just what OpenAPI code looks like, here&#8217;s a simple example: OpenAPI Example openapi: 3.1.0 info: title: Sample API description: A simple API to illustrate OpenAPI concepts version: '2' servers: - url: https://api.example.com/v1 paths: /items: get: summary: Retrieve a list of items responses: '200': description: A JSON array of items content: application/json: schema: type: array items: type: object properties: id: type: integer name: type: string score: type: integer post: summary: Create a new item requestBody: required: true content: application/json: schema: type: object properties: name: type: string score: type: integer This code defines two operations on one endpoint of a hypothetical API, but it should serve to illustrate. At its best, OpenAPI is a YAML data format that can be used to generate documentation, client libraries, server stubs, and more. At the very least, it can be used as an authoritative reference for API development and testing. In fact, non-developers, such as technical writers and product managers, can use or even contribute to an OpenAPI document (OAD). The RESTful interface architecture is relatively simple, and anyone who comes to understand it can help design and define such an API using OAS. There is no reason at all to leave this to developers, though they may have critical feedback during the planning or implementation stages. Why Stop There? Defining relatively complex interfaces in YAML can apply to much more than just REST APIs. Full-stack application development involves much more &#8220;coding&#8221; than most would consider actual &#8220;programming&#8221;. Consider interface design and database design, for instance. Both are best done in code, but neither is really programming, per se. Now, I have admittedly never seen a non-developer design a database schema, but I can readily imagine savvy technical writers and product managers creating sensible YAML documents to convey structured data when a relational database would be overkill. And I certainly have seen non-developers contribute to REST API design via OAS. So that non-programming coding category certainly includes editing YAML files, and I would imagine it even includes templating languages such as Jinja, Liquid, and Handlebars. These can involve data processing and logic, but they are relatively simple and purpose-built for text transformation. Liquid was specifically designed for non-developers, and generative-AI coding tools are adept at writing templates in nearly all popular syntaxes. This means those savvy non-programmers can help author data files and help turn them into good, auto-generated reference documentation. This non-programmer involvement is but one of the key advantages of stepping away from &#8220;native&#8221; (Python, Java, Rust, Javascript, Ruby, Golang, etc) programming code and into a truly cross-language, human-writeable data format like YAML. While YAML is second only to JSON and XML in terms of current popularity, it is wildly more user- and Git-friendly than the leading formats. Meanwhile, YAML-like formats such as TOML, CSON, HJSON are lesser known alternatives. About Those Other Interface Types It turns out YAML is a terrific format for all kinds of interface definition coding. It can be used for defining YAML/JSON configuration files. It can be used to define command-line interfaces (CLIs), HTML forms, file/directory structures, and much more, always allowing for extensive auxiliary metadata for each element so defined, whether it be a REST API endpoint or a form input field. Here is an example of how I use YAML to define YAML-formatted configuration files for my Ruby applications: properties: log_level: type: String desc: The logging level for the application. dflt: info opts: [debug, info, warn, error, fatal] output_format: type: String desc: The format for output data. opts: [json, yaml, xml] dflt: json max_retries: type: Integer desc: The maximum number of retry attempts for failed operations. span: '0..5' dflt: 3 This definition supports some automated validation, and it allows me to generate documentation directly from this very source. `log_level`:: The logging level for the application. [horizontal] Default::: `info` Options::: `debug`, `info`, `warn`, `error`, `fatal` `output_format`:: The format for output data. [horizontal] Default::: `json` Options::: `json`, `yaml`, `xml` `max_retries`:: The maximum number of retry attempts for failed operations. [horizontal] Default::: `3` Range::: `0-5` The Advantages of YAML-based TSST So what are the key advantages of using YAML-based TSST? Enables truly cooperative design and definition. Non-programmers can contribute to the design and definition of interfaces, data structures, and more. Sources documentation right where the interface is defined. Developers are used to this for REST and native APIs, though the latter is usually (sensibly) handled in the language&#8217;s official or dominant &#8220;inline&#8221; format. For less language-specific interfaces, YAML is a great way to define the interface and its documentation in one place. Informs automated testing. Integration tests can ingest YAML definition data to test against a single data source maintained by all stakeholders. For these reasons, YAML is my go-to source format for defining nearly all interfaces, as I will explore in this blog in future posts. Single Sourcing in `README.adoc` The other place I love to define global application data is in the root README.adoc file of the project. Only data that appears in the README is optimally stored here, since YAML is a more flexible and precise data-serialization format. But user-defined AsciiDoc attributes are a great way to ensure that ALL documentation and even the product itself are deriving data from the same single source. For example, all of my Ruby APIs and CLIs derive their canonical version number from an attribute in the README.adoc file. It&#8217;s called this_prod_vrsn, and I can express it anywhere in the documentation as {this_prod_vrsn}, as well as ingest it into the product at build time. require 'asciidoctor' doc = Asciidoctor.load_file('README.adoc', safe: :safe) ATTRS = doc.attributes VERSION = ATTRS['this_prod_vrsn'] AsciiDoc attributes unfortunately do not support nested data structures or even Arrays, but they are sufficient for core data such as default values, general product data, and anything else you might wish to report in your README itself as well as throughout the product and user documentation. One big advantage of AsciiDoc attributes is that they are inheritable like native variables. :product_base_url: https://example.org :product_api_url: {product_base_url}/api Generating Docs with Templating Engines You may have been wondering how our YAML data turned into AsciiDoc source code in the previous examples. The trick is a templating processor, such as those that parse syntaxes like Liquid and render textual output from the input data provided. {% for property in properties %} property[0]:: {{ property[1].desc }} [horizontal] Default::: `{{ property[1].dflt }}` {% if property[1].opts %} Options::: {% for opt in property[1].opts %}`{{ opt }}`{% unless forloop.last %}, {% endunless %}{% endfor %} {% endif %} {% if property[1].span %} Range::: `{{ property[1].span | replace:\"..\" , \"-\" }}` {% endif %} {% endfor %} That is all the markup that is required to generate the AsciiDoc source code shown earlier, which I&#8217;ll repeat here again for convenience. `log_level`:: The logging level for the application. [horizontal] Default::: `info` Options::: `debug`, `info`, `warn`, `error`, `fatal` `output_format`:: The format for output data. [horizontal] Default::: `json` Options::: `json`, `yaml`, `xml` `max_retries`:: The maximum number of retry attempts for failed operations. [horizontal] Default::: `3` Range::: `0-5` Truth and Purism It&#8217;s a funny coincidence that &#8220;TSST&#8221; reads and sounds like a scolding. The very concept is strict and somewhat cold, I have to admit. It is unwise to be a &#8220;purist&#8221; about nearly anything, especially in software development, so of course there may be exceptions where an instance of product datum has to be defined twice. But it is a great principle to aim for, as it offers true benefits along the design &#8594; definition &#8594; documentation &#8594; validation pipeline."
    },{
      "id": 4,
      "url": "/blog/why-github/",
      "title": "Why DocOps Lab Chooses GitHub over GitLab",
      "content": "I quite like GitLab for a number of reasons, but before I ever knew it existed, I was already deeply embedded in GitHub&#8217;s ecosystem. The TL;DR of this blog post is that GitHub&#8217;s network effects and ubiquity outweigh GitLab&#8217;s technical advantages for my own open source projects. I was not particularly bothered by Microsoft&#8217;s acquisition of GitHub, despite a general bias against Microsoft for political and technical reasons. At that time, it did not seem like MS was particularly interested in changing GitHub&#8217;s culture or direction, and MS being more invested in open source was a welcome development. But the recent immersion of GitHub into MS&#8217;s AI division, with GH no longer having a CEO or even the pretense of independence, I find myself wishing I was rooted on another platform. For this reason, I had a stop-and-think about this subject before I released a new bunch of codebases on the platform. I gave it enough serious thought and did enough research to feel like it makes a nice, short blog post that maybe nobody should be particularly influenced by. The Comparison There are a lot of &#8220;content marketing&#8221; articles out there that claim to help you decide for your own case. This article is not one of those. I never know how much of those articles is sponsored content or SEO gaming or just AI slop that may or may not be true. Really I am just documenting my own decision for accountability. My decision took into account a peculiar use case: different (non-developer) audience, strong technological opinions, and a need to grow a community around the work. My own research involved a marathon ChatGPT 5 session evaluating numerous aspects of the two platforms. Here is a table ChatGPT made to represent the assessment that I led it to through interrogation and testing of its assessments. Criterion GitHub GitLab Win Account friction Ubiquitous accounts New signup needed Discoverability Strong social graph Limited visibility PR/MR workflow Simple &amp; familiar Powerful but heavier AsciiDoc Support Modest Strong Issues/Discussions Familiar, lightweight Powerful but heavy Browser editing Seamless Full IDE, slower Incentives Global profile value Siloed I weighed a couple of these criteria a lot, including AsciiDoc support and PR/MR workflow. (I even dislike the term &#8220;pull request&#8221; and would much prefer GitLab&#8217;s &#8220;merge request&#8221;.) GitLab has better aesthetic (&#8220;front end&#8221;) support for AsciiDoc rendering, and it supports the powerful include:: directive. Then again, since I want all of my frameworks and strategies to be GitHub friendly, I really should not actually take advantage of major (&#8220;back end&#8221;) features that GitHub does not support. When it comes to actually generating user-facing documentation, neither platform matters. This criteria is just about how it renders READMEs and displays browseable source files. (Truthfully, README files should not employ transclusion, anyway.) The same is basically true for workflow features like pull request/merge request process or UI tools. Either people can use the excellent tools and interfaces GitHub provides, or there really is no reason to believe a push for code-like practices by non-developers is viable. I rightly will not be able to control or really even influence where people choose to host their own projects that employ the techniques DocOps Lab exists to promote. Most will choose GitHub, so everything we are trying to prove and demonstrate should probably happen on GitHub. The Conclusion In the end, I have decided that since I have no gravity of my own to draw people to a platform, I should not add variables (&#8220;friction&#8221;) to this endeavor that might disadvantage it any further than its starting point. If my work somehow engenders a draw of its own, perhaps the community could make a project of migrating to a platform that is not owned by a publicly traded corporation in the first place. For now, it seems like GitHub is the best path to popularizing tools and techniques that most people will invariably practice on GitHub."
    },{
      "id": 5,
      "url": "/docs/xref-attrs/",
      "title": "Xref_attrs",
      "content": ""
    },{
      "id": 6,
      "url": "/docs/cla/",
      "title": "DocOps Lab Contributor License Agreement (CLA)",
      "content": "This Contributor License Agreement (CLA or Agreement) sets the terms under which individuals may contribute to DocOps Lab projects. By adding your name or GitHub username to this document, you indicate agreement to it. By contributing to any DocOps Lab project that links prominently to this CLA document from its README file&#8217;s \"Contributing\" section, you assert that you have read and that you agree to the terms of this Agreement. Table of Contents 1. Principles 2. Scope 3. Contributor Identity 4. Licensing of Contributions 5. Copyright 6. Patents 7. Warranties 8. Records 9. Amendments 10. Code of Conduct 11. Freedom of Association 12. Attribution &amp; Recognition 13. Dispute Resolution 14. Acceptance 1. Principles DocOps Lab is a free as in freedom software organization. All projects are licensed under OSI-approved (typically MIT) or Creative Commons permissive licenses. We reaffirm alignment with open source community norms: Transparency of terms Permanence of license grants Respect for contributor freedom 2. Scope Projects covered All repositories under the DocOps Lab organization on GitHub are covered by this Agreement. Contributions covered All source code, documentation, and related assets of or contributed to DocOps Lab are covered by this Agreement. Who it applies to All contributors, including DocOps Lab members, maintainers, and users, are covered by relevant portions of this Agreement, to the extent it applies to their relationship with DocOps Lab. 3. Contributor Identity Capacity declaration Upon contributing source code or content, contributors must specify whether they act: in a personal capacity as part of an employer&#8217;s work, with employer approval under other constraints or conflicts Authority Contributors affirm they have the legal right and authority to submit contributions, free of encumbrances or conflicting obligations. LLM usage declaration Contributors must disclose use of large language models (LLMs), &#8220;AI agents&#8221;, &#8220;chat bots&#8221;, or similar tools in generating code or content that gets contributed. Private, behind-the-scenes usage of LLMs or similar tools to perform tasks that do not generate publishable code or content need not be disclosed. See also DocOps Lab Generative \"AI\" Guidance for DocOps Lab&#8217;s policy on generative AI usage. 4. Licensing of Contributions Declared licenses Contributions are licensed under the repository&#8217;s declared license or licenses. Acceptance Contributors accept all licenses present in the repository. Permanence DocOps Lab attests that all licenses are permanent. They will not be changed to more restrictive forms. Permissiveness DocOps Lab attests that all licenses are permissive, in that they do not constrain fair-use or commercial reuse of the source code or its rendered artifacts. No Relicensing DocOps Lab will not relicense or subvert contributions into restrictive or commercial forms. 5. Copyright Assignment Contributors relinquish individual copyright in their contributions. Ownership DocOps Lab holds sole copyright in all accepted contributions but instantly reissues permissively under an attribution-only license. Attribution Individual attribution is preserved in Git commit history. A byline may be preserved where such a byline exists in a document. Downstream licensees are encouraged but not obligated to preserve individual attribution. Licenses do require downstream attribution of DocOps Lab copyright. 6. Patents Non-assertion pledge Contributors agree not to assert patents against DocOps Lab or downstream users for uses permitted under project licenses. Contributor freedom Contributors remain free to patent or commercialize independent works based on contributions to DocOps Lab projects, provided that DocOps Lab&#8217;s licensed use and all downstream use under that license remain immune from liability or assertion. 7. Warranties All parties affirm freely contributed original work. Contributor warranties Contributions are affirmed as: Original, OR Properly licensed with DocOps Lab approval, AND Not knowingly infringing third-party rights, AND Provided without warranty DocOps Lab warranties Existing and future source code are affirmed as: Original or properly licensed Not knowingly infringing third-party rights Provided without warranty 8. Records Tracking Contributor agreements will be publicly tracked in the DocOps Lab administrative repository. Methods DocOps Lab may also use GitHub bots or commit sign-offs to automate agreement tracking. Confirmation NOT REQUIRED Active signing is NOT required to constitute Agreement. See Acceptance below. 9. Amendments Versioning CLA versions are tracked in Git. Notice Contributors will be notified before their next contribution if the CLA is amended. New rights or obligations Any amendment affecting contributors' rights, responsibilities, or warranties will result in a new version of this document. Contributors retain the prerogative to agree to or decline the new version before further contributions. 10. Code of Conduct DocOps Lab adheres to a Code of Conduct that promotes a welcoming and inclusive environment for all contributors. Contributors agree to respect these principles and to engage in constructive, respectful dialogue with other contributors. 11. Freedom of Association Voluntary Contributors are free to associate or withdraw from contributing at any time. Past contributions remain under project license terms. Severable This Agreement does not obligate future contributions. 12. Attribution &amp; Recognition Documentation Attribution appears in Git history. Code Attribution appears in Git history. Recognition DocOps Lab may feature contributors in credits, websites, or other venues outside the repositories themselves, at its discretion. Contributor identity To indicate how they wish to be identified, contributors should append to this document by way of Git commit and pull request, their: identifiers (optional) name or alias pronouns, titles, certs, etc GitHub handle (required) 13. Dispute Resolution Preferred dispute process Disputes are to be addressed first informally within the community. Mediation If unresolved, disputes may be mediated by disinterested and mutually chosen members of the broader open-source community. Legal forum Legal proceedings are a last resort. Jurisdiction defaults to the contributor&#8217;s home jurisdiction, if remote participation is permitted, or else the most convenient remote venue acceptable to all parties. 14. Acceptance Declaration of agreement (repeated) By contributing to any DocOps Lab project that links prominently to this CLA document from its README file&#8217;s \"Contributing\" section, you assert that you have read and that you agree to the terms of this Agreement."
    },{
      "id": 7,
      "url": "/docs/contributing/",
      "title": "DocOps Lab Contributor&#8217;s Guide",
      "content": "DocOps Lab encourages contributions from anyone interested in improving our projects. We are even open to initiatives around new projects that align with Mission and values. This guide outlines the process for contributing code, documentation, and other improvements to our repositories and community. Just like DocOps Lab software is aimed at non-developers, so too do we want to make contributing as accessible as possible. We welcome contributions from people of all skill levels, including those who may be new to coding or open source. See DocOps Lab Development Process (General) for detailed contribution procedures. This document focuses on a more general approach to the contribution process, whereas that guide is highly technical and focused on actually working with DocOps Lab codebases. Each DocOps Lab project repository should have a Contributing Section in its README.adoc file. Always consult that once you&#8217;re familiar with general policies and actually ready to push code. Getting Started Contributors are welcome to use any valid toolchain to develop and commit code and docs to DocOps Lab projects. This section provides an overview of the broad technological strategies for contributing. For details on the very concept of contributing to open-source projects, see GitHub&#8217;s see GitHub&#8217;s official contribution guide. Option 1: Web UI for Quick and Dirty Changes Use GitHub&#8217;s Web interface and optionally Copilot to fork the prime repo (DocOps/&lt;repo-id&gt;) and make a change. Run the tests until it&#8217;s right, then issue a pull request (PR) to the prime repository. This method is suitable for small changes, such as fixing typos or making minor code adjustments in one or a few files. It is also the easiest method for total beginners. We recommend following this thorough guide to GitHUb Web UI contributions. Option 2: Install the Development Environment For more substantial contributions, set up a local development environment suitable to the project you want to work on. A base Dockerized Ruby environment should be suitable, as well. The latest DocOps Box to create a base image that should suffice for developing any DocOps Lab codebase. If you are initializing a *new DocOps Lab project*, see DocOps Lab Dev-tooling Setup for comprehensive setup details. Option 3: Use a Cloud Environment Also for larger contributions, consider using a cloud-based development environment like GitHub Codespaces or Gitpod. We do not specifically instruct this or provide template spaces at this time, but we may do so upon request. Contributing Code (Including Docs) All contributions of functional code, data, or documentation (which is also code) must go through the poorly named &#8220;Pull Request&#8221; (PR) process via GitHub. This process allows for review, discussion, and testing before changes are merged into the main codebase. See the separate DocOps Lab Generative \"AI\" Guidance document for LLM-specific policy info. Contributing Product Code Code contributions need not be perfect or expert level, but contributors should have reason to believe a contribution is correct and useful before issuing a PR. Prospective contributions are encouraged to open an issue in the repository to discuss the proposed change before starting work. They are also encouraged to read the project&#8217;s README.adoc to understand the contribution needs and flow plus localized coding standards. Contributing Documentation Code Documentation contributions are welcome and encouraged. This includes improving existing documentation, adding new content, or fixing typos and formatting issues. While opening a Bug issue to report a problem with the existing docs is always welcome, critics can become contributors by forking the repository, making changes, and issuing a PR. Documentation is a great place to get started with open-source contributions, and DocOps Lab is committed to being helpful and encouraging to anyone trying to lend a hand. Be sure to check any relevant README files, including a docs/README.adoc file if present or the &#8220;Documentation&#8221; section of the main README.adoc in the project root. Contributing Test Code Tests are also code. They can assess: product source product executables documentation source converted docs output Contributors are welcome to add tests for existing code or docs, or to improve existing tests. This includes unit tests, integration tests, and end-to-end tests. It also includes demo environments and sample data used for testing. Look for a specs/tests/README.adoc file or similar in the repository for local guidance on testing frameworks and practices. Contributor License Agreement (CLA) All contributions to DocOps Lab projects must comply with our CLA. This Agreement ensures that contributions are made under a consistent license, protecting both contributors and the project. All contributors should read the whole Agreement, which is very brief and simple, but here is a listing of the key points that pertain to contributors' rights and responsibilities: Key Points of the DocOps Lab CLA Capacity declaration Upon contributing source code or content, contributors must specify whether they act: in a personal capacity as part of an employer&#8217;s work, with employer approval under other constraints or conflicts Authority Contributors affirm they have the legal right and authority to submit contributions, free of encumbrances or conflicting obligations. LLM usage declaration Contributors must disclose use of large language models (LLMs), &#8220;AI agents&#8221;, &#8220;chat bots&#8221;, or similar tools in generating code or content that gets contributed. Private, behind-the-scenes usage of LLMs or similar tools to perform tasks that do not generate publishable code or content need not be disclosed. See also DocOps Lab Generative \"AI\" Guidance for DocOps Lab&#8217;s policy on generative AI usage. Acceptance Contributors accept all licenses present in the repository. Assignment Contributors relinquish individual copyright in their contributions. Ownership DocOps Lab holds sole copyright in all accepted contributions but instantly reissues permissively under an attribution-only license. Non-assertion pledge Contributors agree not to assert patents against DocOps Lab or downstream users for uses permitted under project licenses. All parties affirm freely contributed original work. Contributions are affirmed as: Original, OR Properly licensed with DocOps Lab approval, AND Not knowingly infringing third-party rights, AND Provided without warranty DocOps Lab adheres to a Code of Conduct that promotes a welcoming and inclusive environment for all contributors. Contributors agree to respect these principles and to engage in constructive, respectful dialogue with other contributors. Preferred dispute process:: Disputes are to be addressed first informally within the community. Mediation If unresolved, disputes may be mediated by disinterested and mutually chosen members of the broader open-source community. Legal forum Legal proceedings are a last resort. Jurisdiction defaults to the contributor&#8217;s home jurisdiction, if remote participation is permitted, or else the most convenient remote venue acceptable to all parties. Declaration of agreement (repeated) By contributing to any DocOps Lab project that links prominently to this CLA document from its README file&#8217;s \"Contributing\" section, you assert that you have read and that you agree to the terms of this Agreement. Read the FULL AGREEMENT at CLA. Community Decorum (Code of Conduct) DocOps Lab commits to and requires contributors foster a welcoming and inclusive environment for all participants. Our social guidelines are simple, direct, and included here in their entirety to emphasize their importance. Purpose DocOps Lab is a community of people collaborating on free and open projects. We want our workspaces&#8201;&#8212;&#8201;virtual and IRL&#8201;&#8212;&#8201;to be welcoming, respectful, and productive for all contributors. Standards Positive behavior includes Offering help and guidance Respecting different skills, backgrounds, and viewpoints Giving and accepting constructive feedback gracefully Focusing on collaboration and shared goals Unacceptable behavior includes Personal attacks or insults Harassment or unwanted attention Dismissing or undermining others&#8217; contributions Disruptive, aggressive, or exclusionary conduct Violating project policies or guidelines Responsibilities Maintainers Responsible for clarifying standards, addressing issues, and applying actions fairly. Contributors Responsible for following this Code of Conduct in all project spaces, including GitHub repos, issue trackers, and community forums. Users/participants Expected to follow this Code of Conduct in all project spaces, including GitHub repos, issue trackers, and community forums. Enforcement Reporting Concerns may be reported privately to DocOps Lab maintainers via the Lab repository or by direct message to an admin. Process Reports will be reviewed promptly and addressed in a way that seeks fairness, confidentiality, and community health. Extra-organizational Handling DocOps Lab encourages contributors and participants to seek appropriate mediation or intervention from outside DocOps Lab. Anyone uncomfortable with this Enforcement process . Consequences Responses to violations may range from a discussion or warning to suspension from participation or even removal of access in serious cases. Escalation Actions involving potential liability or criminality will be referred accordingly. Scope This Code of Conduct applies to all DocOps Lab spaces. That means any online forums and any in-person events organized by the project. Commitment DocOps Lab is committed to fostering a respectful, collaborative environment. Your voluntary participation helps keep this community healthy and open to all."
    },{
      "id": 8,
      "url": "/docs/generative-ai-usage/",
      "title": "DocOps Lab Generative &#8220;AI&#8221; Guidance",
      "content": "DocOps Lab is sensitive to the impact of generative AI on many aspects of modern life. These concerns include impacts on human psychology, professional community, and even the totality of digital output. We are separately worried about the impact these technologies on employment. But as a volunteer-only operation, we are actually more concerned in that regard about the impact of being a volunteer-run producer of open source material, which we yet believe we can justify. While we have myriad other ethical concerns about the subject of AI in general, this document focuses specifically on the use of large language models (LLMs) and similar tools to generate code and content that DocOps Lab eventually publishes. Policy for Non-publishing AI Usage DocOps Lab approves of and offers no caveats about LLM usage that automates rote tasks and chores that do not create output to be published. We acknowledge our marginal contribution to resource usage and environmental impact, but so far we believe it is justified. Maybe someday this topic will get a better treatment, but for now it must suffice that we are not against LLM usage on these grounds at this time. Likewise, there are many negative aspects of this technology that simply do not apply to our usage of it. We are concerned with LLM usage that replaces human interaction, which is more appropriately subject to an AI policy. Therefore, we strive to never use these tools to stand-in for people we have access to and should instead be reaching out to directly. As a matter of principle, we also do not share AI-generated code or content with coworkers or fellow professionals without notifying them of how the bot/agent/etc contributed to it. We do use AI tools to perform onerous, repetitive, time-consuming tasks that pretty much anyone could perform, given enough time and support. In our case, we cannot hire someone to do that work anyway, and honestly we don&#8217;t want to pay people just to do the work we like least. We also judiciously use AI to help organize our work and workflows, and we use it to generate code and content we do not share with the world. DocOps Lab will likely develop clearer policies about non-content, non-coding, non-publishing use of AI tools, but for now we encourage pro-social protocols that favor human interaction wherever possible. If allowance for AI assistance enables more people to get involved, the social gain will be worth the known downsides of current AI technology. The rest of this document is about the use of LLMs to generate text or code content that actually gets shared with the broader world, including coworkers and colleagues, as well as future model training and RAG (retrieval-augmented generation) libraries. LLM-assisted Publishing Policy DocOps Lab does not share unreviewed AI output with the outside world, period. Such matter is kept from public code repositories, documentation sites, and the rest of our public footprint. Whether we are talking about generated examples, tables, code tests, configurations, Bash scripts, sentences of prose, or anything else that gets committed to a codebase and/or shared with the public and future LLMs, we put human eyes on it and stand by it with reasonable confidence. We do this before pushing it even for final human review; it should not be up to reviewers to detect and question potentially AI-generated matter. Why share LLM-generated content at all? This policy statement may cause you to wonder why DocOps Lab would contribute any LLM-assisted code or docs to the world. Why not just use a robots.txt file to deny LLMs access to our repos and docs? Truthfully, we have no choice. We make the kind of software people engage LLMs to use. If the LLMs can&#8217;t learn about our products, they are of no help&#8201;&#8212;&#8201;or worse yet, they&#8217;ll be more likely to hallucinate &#8220;help&#8221; that frustrates our users. Also, admittedly, we need LLM assistance to get the work done, both in terms of quantity and quality. If they don&#8217;t help, our software and docs would not ship in the first place. But in addition, as a better defense, our policy is that we only share improved output, at least compared to what we could do before or without this technology. In the end, we think this is a positive contribution, not a drag or anything drifting toward &#8220;Internet death&#8221; or &#8220;model decay&#8221; or &#8220;sloppification&#8221;. We use it distinctly and with care, and it seems to improve our output; yes in terms of quantity, but without sacrificing and hopefully enhancing quality. Review Criteria for Publicly Released Matter As a general rule of thumb, everything we produce that is affected by AI must have been enhanced or improved by the AI&#8217;s contributions. While this can apply to content we would not otherwise have created, finalized, or published, even in such cases we must apply due diligence. It&#8217;s not enough that we can generate output that &#8220;looks good enough&#8221; or &#8220;works well enough&#8221;&#8201;&#8212;&#8201;it has to be good. By this, we mean AI output should be at least as good as or better than output we would be able to produce without those tools. We do not release code or content that is inferior, compared to what we can produce ourselves, in terms of: accuracy clarity logic style humanity security maintainability compliance with standards or best practices If we cannot confidently assert that the AI-assisted output meets or exceeds our own capabilities in these areas, we must either improve it further or refrain from releasing it altogether. Example Cases Unit and regression tests Automated tests are probably the most sensitive case type so far. This is frankly because we are not that great at writing unit tests; they may not have gotten written at all if not for LLM assistance. This poses a risk that we are publishing sub-par tests that LLMs might then learn from and propagate. We are committed to improving these tests over time, but for now they receive non-expert evaluation for basic adequacy. Our tests are not making the broad body of testing practices better, but hopefully they are not poisoning the well significantly. Redundant code or content The material that is most likely to inadvertently violate this policy is that which goes unreviewed holistically in the context of the larger project, often specifically because an LLM has produced something that we do not realize is duplicative. For example, sometimes an LLM will produce a block of code that is redundant, and we will not notice. Similarly, LLMs sometimes repeat a sentence or phrase from higher up in the page; we review it and it&#8217;s correct and seems important, but we don&#8217;t realize it&#8217;s already been stated. This kind of stuff is particularly hard to lint for, as well. To be fair, this is a common kind of error in these types of projects even when no LLMs are involved. People only marginally better than LLMs at keeping track of and detecting repetitive logic and content. Documentation and Disclosure Every project&#8217;s README.adoc file should include a disclosure statement if any part of the content was generated by tools that fall into the broad category of &#8220;artificial intelligence&#8221;. People who work with open source repositories and educational material have a right to know how and in what ways any authored material was influenced by non-human, non-idempotent &#8220;contributors&#8221;."
    },{
      "id": 9,
      "url": "/docs/mission/",
      "title": "Operate the Docs: DocOps Lab Purpose and Approach",
      "content": "Making true operators out of professional writers and document wranglers is the DocOps Lab mission. Table of Contents 1. DocOps Lab Philosophy 1.1. Conventions 1.1.1. Strategies 1.1.2. Tactics 1.2. Tooling 1.2.1. Utilities 1.2.2. Automation 1.3. Principles 1.3.1. Usability 1.3.2. Reusability 1.3.3. Interoperability 1.3.4. Extensibility 1.3.5. Innovation 2. DocOps Lab Commitments 2.1. Free, Open-Source (Tools and Docs) 2.2. Open Standards 2.3. Open Education 2.4. Open Community 1. DocOps Lab Philosophy DocOps Lab is a problem-solving philosophy that employs tech (technologies and techniques) to optimize the way professionals create, manage, and publish digital documents with complex sourcing, processing, and delivery requirements. Document operators are people who employ the techniques and technologies preferred by programmers when it comes to the documentation they rely on and are responsible for. This section explores exemplary tech, as well as further principles used to evaluate solutions. The Conventions and Tooling in this document are representative rather than exhaustive, and they are subject to change over time. The Principles, however, are foundational and may evolve but should be definitive, enduring, and difficult to undo. 1.1. Conventions Problem-solving techniques are generally known to DocOps Lab as conventions, broken down into two types: Strategies and Tactics. 1.1.1. Strategies Approaches to addressing general problems at scale serve to guide assessment and analysis, mapping solutions to entire documentation sets by organizing, categorizing, versioning, and other broad sets of techniques. Examples of DocOps strategies To map docs to product versions, use a framework for grouping and categorizing types of divergence in both the deployed/delivered product and the documentation output. To enable authors to indicate and readers to readily identify the purpose of a piece of information, use semantic typing for documents/topics, blocks, and inline elements. To store, define, and manage \"`small data`\" for configurations, component profiling, and so forth, use Git-trackable, text-editable flat files. To turn complex data into rich documents, use dynamic templates that blend data with document markup. 1.1.2. Tactics Specific methods for implementing strategies. Examples of DocOps tactics Version-mapping tactics: Use SemVer for sequential changes/releases. Use editions for parallel versions of products and docs. Use localization for international translations and regional variants. Semantic typing tactics: Use AsciiDoc in particular ways for semantic authoring, such as inline roles. Apply our hybrid Ditataxis categorization to define and organize document collections. Flat-file data tactics: Use YAML for configuration and profiling data. Use CSV for tabular data. Use Google Sheets to edit tabular data. Dynamic templating tactics: Use Liquid templates and engine to blend data into document markup. Use Regular Expressions patterns to extract and filter data from logs. 1.2. Tooling Problem-solving technologies generally fall under the umbrella category of &#8220;tooling&#8221;, which is industry jargon for software, including how it is scripted and configured. 1.2.1. Utilities Hands-on tools that enable day-to-day authoring and management operations. Examples of DocOps utilities Git for version control. Docker for containerized environments. VS Code for text editing. Pandoc for migrating to standardized formats. 1.2.2. Automation Scripted routines and always-available platforms that execute common tasks in a predictable manner. Examples of DocOps automation GitHub Actions for continuous integration and deployment. Rake and Bash for task-level automation. Vale for text/markup linting. Jekyll for static-site generation. Redocly CLI for OpenAPI rendering. 1.3. Principles When it comes to evaluating, selecting, and innovating on conventions and tooling, DocOps Lab applies a set of principles that guide our decisions and practices. Unlike the conventions and tooling, these principles are relatively exhaustive, definitive, and immutable, especially in combination with DocOps Lab Commitments. 1.3.1. Usability DocOps Lab emphasizes tools and techniques that non-expects can grasp and use effectively and without intensive training. Operators will still need to be comfortable with technology, but they do not need to know anything about programming to get started. In fact, DocOps Lab tools are not for programmers or developers, other than how they may incidentally or by design overlap with their particular needs.[1] We specialize in tools with: low-code or no-code interfaces sensible command-line interfaces YAML-based configuration lightweight markup languages text transformation by templating scriptable with YAML- and Liquid-based DSLs (domain-specific languages) While full-fledged scripting languages and programming environments are a robust fallback for DocOps platforms, DocOps Lab focuses on maximizing the range of what can be accomplished with minimal coding and straightforward configuration. The principle of usability also means accessibility in the more traditional sense. At this time, true accessibility is a goal rather than a reality for DocOps Lab, but we are working to make our tooling truly open to people with disabilities. 1.3.2. Reusability DocOps approaches will work in more than one place with only circumstantial modifications. This is a core approach to software development, but it is also a key principle of technical documentation and formal documents at all levels. Reusability implies DRY (&#8220;Don&#8217;t Repeat Yourself&#8221;) in terms of content, data, and configuration. It also means that the same tools and conventions can be used across different projects, industries, and problem spaces. This principle broadly covers universality, portability, and repeatability. Basically, tools need to be able to work equally in a wide range of environments and circumstances. Our commitment to containerization (with Docker, etc) is a key part of this principle, as it allows users to run the same environment anywhere Docker is supported. 1.3.3. Interoperability Similar to some of the sub-principles of reusability, interoperability means technology and techniques that fit and work together in a cohesive manner. DocOPs favors tool integration and data exchange mechanisms that enable seamless workflows across different systems. This also means effective compatibility across operating systems (Windows, MacOS, and Linux). Examples of DocOps interoperability Git and GitHub for version control of document sources, tooling configurations, scripted automation, and rendered artifacts. LiquiDoc and Jekyll standardize around Liquid, YAML, and AsciiDoc generation, also supporting Markdown, JSON, CSV, and other formats. OpenAPI format for REST API documentation, as it is an open standard that also uses YAML and covers nearly everything HTTP interfaces can do, and numerous tools can generate output from it in various forms. The goal of interoperability is a smooth experience with the shortest possible toolchain and technology stack. 1.3.4. Extensibility Tools and conventions that can be extended, configured, or otherwise customized to meet highly specific yet unforeseen needs without substantially altering the prime source of tooling or documentation. Users should always be able to make changes that build upon rather than alter the upstream source. What works in one peculiar use case should be adaptable to others. Example of DocOps extensibility DocOps Lab maintains a &#8220;content typing&#8221; system which itself extends both the Ditaxis and the DITA topic typing systems. This &#8220;Ditataxis++&#8221; system defines a set of standard content types and subtypes, each with its own purpose, structure, and conventions. Continue reading this example of extensibility For a downstream user to add, remove, or change a content type, they simply create a new YAML file that conforms to the open-standard format for defining semantical taxonomies. This way they can use DocOps Lab software to apply their local patches and produce documentation and even linter code that leverages the whole collection of content types, to be included in a style guide and even automated docs testing. In fact, the same library provides the same kind of definitional data for the original DITA and Ditaxis systems, so any team can select a standard through simple configuration and use it instead of our preferred Ditataxis++. Extensibility also includes scalability. Solutions must be able to grow along with the teams and enterprises that use them, in terms of complexity and sheer amounts of content or contributors. Whenever possible, extensibility should not cost any more than the labor required to implement and use the changes. 1.3.5. Innovation When conventions and tools are not enough, we innovate. For DocOps Lab, this is the extensibility of our own approach: modify and build upon what exists to accommodate new challenges. Innovation means developing new methods and/or technologies that extend our current conventions and tools. Whenever possible, these solutions should broadly accommodate or modularly integrate with other approaches and toolchains. That is to say, solutions should always meet the other principles. Innovation also means doing things differently when appropriate; it means not being afraid to break out of conventions and standards that have grown inadequate or obsolete. Favoring a trusty standard makes sense until you find it simply too constraining. 2. DocOps Lab Commitments DocOps Lab adheres to radically open principles. For the DocOps Lab mission, developing and propagating conventions, tooling, and the skills to execute them absolutely requires a wide-open program of collaboration and sharing. Your organization may have different priorities and constraints, but DocOps Lab commits to these methods as a matter of principle so users and contributors can count on them. 2.1. Free, Open-Source (Tools and Docs) All of the tools we develop, and nearly all of the tools we use or recommend[2], are fully free and open-source software (FOSS). This goes for the documentation we produce, too. You are welcome to reuse and repurpose it to whatever end you see fit, with attribution but no requirement to re-share what you do with it. Unrelated third parties can even use our course content to teach their own classes, even charge a fee, so long as they credit the content appropriately. 2.2. Open Standards Industries and professions that eschew proprietary methods and adopt open standards see greater integration, interoperability, innovation, and adoption of best practices[3][4]. Our interoperability and extensibility principles depend almost entirely on open standards. And while we cannot claim DocOps Lab never eschews an established standard over a preferred or even custom alternative, at least we only ever introduce new open standards, and we only make our own when the leading solution really does not suffice for our needs. Example of DocOps open standards Advanced documentation systems tend to need to point to data objects inside flat files (JSON, YAML, etc). There are (at least) three competing open standards for this: JSON Pointer, JSON Path, and JMESPath. DocOps Lab is introducing a standard called URIx, which is a universal way to use any of these three standards to indicate data, whether it be in the same files as the reference, inside a local file, or inside a remote file. 2.3. Open Education DocOps Lab is committed to teaching the principles and practices of docs-as-code to as many people as possible. This is the whole purposes of the Docs-as-Code School project: using DocOps tech to teach DocOps tech. All course content is openly licensed for anyone to use, adapt, and repurpose&#8201;&#8212;&#8201;you can even teach it yourself, modified or unmodified. The only thing we charge for is real-time access to instructors and professional mentors. All content, including some that is student generated, will remain freely available. 2.4. Open Community For DocOps Lab, community is the least defined element with the greatest potential. Everyone can impact this part. There is a Zulip chat group, GitHub discussions, and a broader community with Write the Docs, its international conferences, local chapters, and vibrant Slack community. There are even the GitHub Issues boards on our repositories. However community pans out for DocOps/docs-as-code broadly and those participating in DocOps Lab in particular, openness and active participation are surely the keys to success. Talking about DocOps Lab projects is contributing. The DocOps Lab Contributor&#8217;s Guide applies to everyone who participates. 1. DocOps CLIs and APIs are meant to be integrated with other applications, usually by programmers or IT personnel, but the main user base for all our projects is digital document professionals, which of course includes programmers. 2. Some platforms we use or recommend MAY offer paid plans, and their offerings may not be fully open source, but in all cases such providers are strongly open-source supportive. 3. On the difference between open source and open standards: https://www.ibm.com/think/topics/open-standards-vs-open-source-explanation 4. On the efficacy of open standards: https://www.dynatrace.com/news/blog/open-source-software-and-open-standards/ and https://www.4ipcouncil.com/application/files/3615/4357/3178/4iP_Council_-_Proprietary-vs-Open-Standards_-_Nov18.pdf (PDF)."
    },{
      "id": 10,
      "url": "/docs/privacy/",
      "title": "DocOps Lab Privacy Assurances",
      "content": "DocOps Lab is committed to the strictest protections of user privacy. This document outlines our privacy practices and the measures we take to ensure your data remains secure. DocOps Lab software does not collect any data from users through telemetry or other means, personally identifiable or otherwise. If you use remote or third-party resources via DocOps Lab software, such as REST APIs, CI/CD platforms, or content delivery networks (CDNs), those services may collect data as outlined in their own privacy policies."
    },{
      "id": 11,
      "url": "/docs/asciidoc-styles/",
      "title": "Documentation Style Guide",
      "content": "DocOps Lab is an AsciiDoc shop. With a few exceptions, all technical documentation is sourced in AsciiDoc format using a particular (standards-compliant) syntax style. Structured/reference documentation is typically stored in YAML-formatted files, often with AsciiDoc-formatted text blocks. Some documentation in DocOps Lab projects is written in Markdown format, such as documents intended for AI consumption (such as for agent orientation/instruction or for RAG retrieval). Automated Style Enforcement DocOps Lab projects using the docopslab-dev tool automatically enforce documentation style guidelines. This is done using Vale, a prose and source-syntax linter. To check documentation style: Check prose for style issues bundle exec rake labdev:lint:text Check for AsciiDoc markup syntax issues bundle exec rake labdev:lint:adoc Check both syntax markup and prose bundle exec rake labdev:lint:docs See DocOps Lab Dev-tooling Setup for more on the docopslab-dev tool. For Vale configuration details, see Vale Configuration and Usage. DocOps Lab maintains a general-audience style guide in the AYL DocStack project repository and website. That guide is reproduced here. General AsciiDoc Syntax Guidelines DocOps Lab documentation largely follows the conventions outlined in the Recommended Practices andWriter&#8217;s Guide documents maintained by the Asciidoctor project. Reinforcements and exceptions: Use .adoc extensions execpt for Liquid templates used to render AsciiDoc files, which use .asciidoc. Use one sentence per line formatting. Let hard-returns signal spaces between sentences. Also do this for major colon- or semicolon-delimited sentences. Use ATX-style titles and section headings. For DRYness, use attributes for common URLs and paths (see Attribute Formatting). DocOps Lab Specific Syntax Guidelines Inline Syntax Inline Semantics The main purpose of inline semantics is to provide a clear indication of the role of the text to the reader&#8201;&#8212;&#8201;including artificial readers. We can convey semantics by way of: declaration by element, role, or class text style based on declaration browser effects based on declaration and additional data We use the following inline semantic coding in DocOps Lab publications. Syntax Preferences Use inline semantics liberally, even if you only insert the heavier syntax on a second or third pass. Formatting with simple *, _, and ` characters on first drafting makes lots of sense&#8201;&#8212;&#8201;or even missing some of these altogether until the second pass. But before you merge new text documents into your codebase, add role-based inline semantics wherever they are supported. Let the reader know and make use of special text, most importantly any verbatim inline text. Even if you are not ready to add such fine-grained tests to your pipeline, consider the value of having all your commands for a given runtime app labeled ahead of time (such as .app-ruby), and the advantage to the reader, as well. Block Syntax Block Semantics Use semantic indicators deliberately. The more you assert about a block of text you are writing, the better the placement and content of that block will be. Semantic assertions reside in the source markup, which may convey means of interpreting that same data visually in the output, as an indication to the reader. For instance, warning admonitions should only deliver warning content, and the user should clearly see that a warning is interrupting the flow of the content in which it appears. [WARNING] ==== Avoid misusing or overusing admonition blocks. ==== Semantic notations in our source remind us to treat the content properly. [WARNING] ==== Avoid misusing or overusing admonition blocks. This will be hypocritically violated throughout this guide. ==== True as it may be, the second sentence in that admonition should be removed from the block. It can either be its own block, or it can be allowed to fade into the surrounding content. Sometimes the entire admonition may end up deserving this treatment. Use Delimited Blocks Generally, use explicit boundary lines to wrap significant blocks, rather than relying on other syntax cues to establish the &#8220;type&#8221; of block is intended. These lines are called linewise delimiters. For example, use the following syntax to wrap the contents of an admonition block: Example 1. Example admonition block syntax with linewise delimiter [NOTE] ==== The content of an admonition block should be sandwiched between `====` lines. Use one-sentence-per-line even in admonitions. ==== The standard linewise delimiters for various AsciiDoc blocks are as follows: ==== For admonitions and examples ---- For code listing (verbatim) blocks .... For literal (verbatim) blocks **** For sidebar blocks |=== For tables ____ For quote blocks ++++ For raw/passthrough blocks -- For open blocks For code listings, literals, or really any block that might contain text that could be confused with the delimiter, vary the length by using a greater number of delimiter characters on the outer block. Example &#8220;example&#8221; block containing an admonition block [example] ======== [NOTE] ==== This is an example block containing an admonition block. ==== ======== Exception: Brief admonitions Some blocks do not require delimiters. In cases of repeated, nearly identical blocks, containing just one line of content, you can use the single-line syntax where it is available. Example single-line admonition block syntax NOTE: This is a single-line admonition block. Exception to this exception We do not recommend the same-line syntax for admonition blocks other than NOTE and TIP. For IMPORTANT, CAUTION, and WARNING, use at least the 2-line syntax, if not explicit delimiters. [IMPORTANT] This is a critical notice, but it's not warning you of danger. Exception: Single-line terminal commands Another common case is 1-line terminal commands, for which this guide recommends using a literal block with a prompt role added. [.prompt] echo \"Hello, world!\" The single preceding space notation affirms the use of a literal block for any consecutive lines of content preceded by a single space. For multi-line terminal commands/output, use the &#8230;&#8203;. syntax to distinguish the block. Exception to the exceptions Whenever additional options must be set for a block, such as a title or role, use the linewise delimiter syntax&#8201;&#8212;&#8201;even in one-liner cases. [.prompt,subs=\"+attributes\"] .... echo \"Hello, {what}!\" .... Example Blocks Use example blocks liberally. If something fits the description of being an example&#8201;&#8212;&#8201;especially if the words &#8220;example&#8221; or &#8220;sample&#8221; are used in the title, caption, or surrounding text referring to a given block of anything&#8230;&#8203; then wrap it in an example block. Instances of the following block types may commonly be instances of examples, and just as commonly they may not be. figures (diagrams, illustrations, screenshots) tables code listings literal blocks (sample prompts, logs, etc) rich-text snippets (rendered results, a user story, etc) Whenever any such instances are examples, prepend and append them with example blocks, and prefer to title them at the exampple-block level rather than the inner-content level. Example of a code block treated as an example :example-caption: Example .require statement in Ruby ==== [source,ruby] ---- require 'jekyll' ---- ==== Special Syntax Attributes Attribute Formatting AsciiDoc attributes are often used to store reusable matter. In certain contexts, attributes should follow a formatting convention that makes them easier to name and recall. Boolean Attributes Use toggles to set or conditionalize states such as: intended audience type or role audience-agent audience-beginner `` target platform or format env-github site-gen-jekyll backend-pdf These kinds of attributes are passed depending on how the AsciiDoc is converted. Platform and format indicators tend to get argued by the converter at runtime. But you can also look check for statuses that might be set in previous files depending on the use-case of the output. Testing for existence of a target platform ifdef::audience-level-beginner[] As a beginner, you will see extra content in parts of this guide. If you are an expert, skip to the &lt;&lt;expert-guide&gt;&gt;. endif::[] Testing for non-existence of a target audience type. ifndef::audience-agent[] This content is _not_ to appear in docs generated for AI agents. endif::[] It is generally advised to create two versions of any such indicator that may need to be resolve a variable placeholder later. Setting open-ended key and boolean simultaneously :audience-level: beginner :audience-level-beginner: true Later we can reference the {audience-level}, which might be overwritten by an attribute passed at runtime. URL Attributes Format URL-storing attributes like so: :syntax_area_descriptive-slug_form: Where: syntax_ is one of href_ (external) xref_ (local) none (skip it&#8201;&#8212;&#8201;presumed to be a straight URL) area_ is a component or category like docs_ or pages_, mainly to ensure unique slugs across divisions form is the way the resource is presented: link (includes linked text and the URL) url (just the URL) Examples :docopslab_hub_url: https://github.com/DocOps :href_docopslab_aylstack_url: {docopslab_hub_url}/aylstack/ :href_docopslab_aylstack_link: link:{href_docopslab_aylstack_url}[AYL DocStack] Vale Configuration and Usage Vale configuration and styles are managed in coordination with the link:`docopslab-dev` gem. Our implementation of Vale allows for local project overrides while maintaining a centralized database of styles. Linting for documentation quality and consistency, both AsciiDoc markup syntax and prose quality/correctness. This tool provides a custom styles package and a modified configuration system, enabling multi-file merging. Base config .config/.vendor/docopslab/vale.ini (from source) Project config .config/vale.local.ini (inherits via BasedOnStyles) Ephemeral config .config/vale.ini (merged from base and target) Sync command bundle exec rake labdev:sync:vale Consumer Mode (Other Projects) For all other projects, the gem works in a standard package consumption mode: The project&#8217;s vale.ini should list all desired packages, including a URL to the stable, published DocOpsLabStyles.zip. The labdev:sync:styles task simply runs vale sync in the proper context, downloading all listed packages into a local .vale/styles directory. The labdev:sync:vale task updates both the base config and the styles package. The .config/vale.ini for consumer projects (based on the gem&#8217;s template) should look like this: # CONSUMER MODE CONFIG StylesPath = .vale/styles # List all packages, including the URL to the central DocOpsLabStyles package. # TODO: Update with the real URL. Packages = RedHat, proselint, write-good, https://example.com/path/to/DocOpsLabStyles.zip [*.adoc] BasedOnStyles = RedHat, DocOpsLab-Authoring, DocOpsLab-AsciiDoc This dual-mode system provides a robust workflow for both developing and consuming the centralized Vale styles. For full Vale configuration settings (&#8220;keys&#8221;) reference, see the Vale documentation. For information on managing DocOps Lab&#8217;s Vale styles, see the docopslab-dev gem README."
    },{
      "id": 12,
      "url": "/docs/bash-styles/",
      "title": "Bash Scripting Styles and Conventions",
      "content": "A guide to writing clean, consistent, and maintainable Bash scripts, based on best practices. Bash Version Use Bash 4.0 or later to take advantage of modern features like associative arrays and improved string manipulation. File and Script Structure Shebang Always start your scripts with a shebang. For scripts that require Bash-specific features, use #!/usr/bin/env bash. #!/usr/bin/env bash Script Header Include a header comment block at the top of your script. This block should briefly explain: The script&#8217;s purpose. Any dependencies required to run it. License information. Usage examples or a pointer to more detailed documentation. Code Organization Structure your script into logical sections to improve readability. The preferred order is: Global Constants and Variables: Define all global variables and constants at the top. Function Definitions: Group all functions together. Argument Parsing: Handle command-line arguments and flags. Main Logic: The main execution block of the script. Often a case statement that dispatches commands. #!/usr/bin/env bash # # script-name # # Brief description of what the script does. # Depends on: curl, jq # --- GLOBAL VARIABLES --- readonly SCRIPT_VERSION=\"1.0.0\" LOG_FILE=\"/var/log/script-name.log\" # --- FUNCTION DEFINITIONS --- my_function() { # ... } # --- ARGUMENT PARSING --- if [[ \"$1\" == \"--help\" ]]; then # ... fi # --- MAIN LOGIC --- main() { # ... } main \"$@\" Naming Conventions Global Variables and Constants Use SCREAMING_SNAKE_CASE. Use readonly for constants. readonly MAX_RETRIES=5 APP_CONFIG_PATH=\".env\" Local Variables Use snake_case and local declaration. local user_name=\"$1\" Function Names Use snake_case. get_user_details() Variables and Data Declaration and Scoping Always use local to declare variables inside functions. This prevents polluting the global scope and avoids unintended side effects. my_function() { local file_path=\"$1\" # Good: variable is local to the function count=0 # Bad: variable is global by default } Quoting Always quote variable expansions (\"$variable\") and command substitutions (\"$(command)\") to prevent issues with word splitting and unexpected filename expansion (globbing). # Good: handles spaces and special characters in filenames echo \"$file_name\" touch \"$new_file\" # Bad: will fail if file_name contains spaces echo $file_name touch $new_file Files created by DocOps Lab should never include spaces, but this habit is important for dealing with user input or external data. Arrays Use standard indexed arrays for lists of items. Use associative arrays (declare -A) for key-value pairs (i.e., maps). # Indexed array local -a packages=(\"git\" \"curl\" \"jq\") echo \"First package is: ${packages[0]}\" # Associative array declare -A user_details user_details[\"name\"]=\"John Doe\" user_details[\"email\"]=\"john.doe@example.com\" echo \"User email: ${user_details[\"email\"]}\" Functions Syntax Use the function_name() { &#8230;&#8203; } syntax for clarity. Arguments Access arguments using positional parameters ($1, $2, etc.). Use \"$@\" to forward all arguments. log_message() { local level=\"$1\" local message=\"$2\" echo \"[$level] $message\" } log_message \"INFO\" \"Process complete.\" Returning Values To return a string or data, use echo or printf and capture the output using command substitution. get_user_home() { local user=\"$1\" # ... logic to find home directory ... echo \"/home/$user\" # Returns string via stdout } To return a status, use return with a numeric code. 0 means success, and any non-zero value (1-255) indicates failure. check_file_exists() { if [[ -f \"$1\" ]]; then return 0 # Success else return 1 # Failure fi } Conditionals Use [[ &#8230;&#8203; ]] for conditional tests. It is more powerful, prevents many common errors, and is easier to use than the older [ &#8230;&#8203; ] or test builtins. # Good if [[ \"$name\" == \"admin\" &amp;&amp; -f \"$config_file\" ]]; then # ... fi # Avoid if [ \"$name\" = \"admin\" -a -f \"$config_file\" ]; then # ... fi For dispatching based on a command or option, case statements are often cleaner than long if/elif/else chains. case \"$command\" in build) build_image ;; run) run_container ;; *) echo \"Error: Unknown command '$command'\" &gt;&amp;2 exit 1 ;; esac Error Handling Use set -o errexit (or set -e) at the top of your script to make it exit immediately if a command fails. Use set -o pipefail to cause a pipeline to fail if any of its commands fail, not just the last one. Print error messages to standard error (stderr). Exit with a non-zero status code on failure. #!/usr/bin/env bash set -o errexit set -o pipefail echo \"Error: Something went wrong.\" &gt;&amp;2 exit 1 Practices to Avoid Avoid eval The eval command can execute arbitrary code and poses a significant security risk if used with external or user-provided data. It also makes code difficult to debug. Avoid it whenever possible. Modern Bash versions provide safer alternatives like namerefs (declare -n) for indirect variable/array manipulation. Avoid Backticks Use $(...) for command substitution instead of backticks (`...`). It is easier to read and can be nested. # Good current_dir=\"$(pwd)\" # Avoid current_dir=`pwd`"
    },{
      "id": 13,
      "url": "/docs/cli-styles/",
      "title": "Command-line Interface Styles",
      "content": "DocOps Lab tooling revolves around command-line interfaces (CLIs). Ruby Application CLIs These are the main interfaces we provide for users of our Ruby-based applications. Most of our Ruby CLIs are built with the Thor CLI framework. Ruby CLI Models Thor-based CLIs generally follow this model: cliapp [subcommand] [arguments] [options] Where both subcommand and arguments are optional, as of course are options. CliGraphy (the Future) Eventually, DocOps Lab will integrate our language-agnostic CliGraphy proper extension of Thor for defining Ruby CLIs. At that point, our CLIs will be defined before they are programmed, using CliGraphy to model the command-line interface in a structured way. CliGraphy definitions will be coded in YAML-formatted documents, similar to an OpenAPI documents (OAD). This particular form of CFGYML will be called CLI YAML-based Modeling Language (CLIYML). Rake CLIs We use Rake for internal repo tasks and chores, including build operations, test-suite execution, unconventional testing, code linting and cleanup, etc. Users of our released products should never be asked to use rake commands during the normal course of daily operations, if ever. Rake is less versatile than Thor, but it is simpler for executing straightforward methods and series of methods. It likewise requires (and permits) considerably less application-specific creativity and customization. Innovative UIs are not justified for internal tooling. Our developer-facing utilities are fairly robust, but the UI for executing them need not be. At DocOps Lab, we save inventive interfaces for domain-optimized operations. Rake CLI Model rake domain:action:target[option1,option2] Where both domain` and target are optional, as of course are arguments that go in the braces. Think of the domain as a component &#8220;scope&#8221; within the codebase or project. Domains either indicate a distinct module or component within the codebase or general tasks using upstream dependencies. No domain means local, project-specific tasks. Example 3-part task with an optional argument rake labdev:lint:docs[README.adoc] In the above case, the domain is from the docopslab-dev library/gem. Example 3-part task with a local domain reference rake gemdo:build The above command has a local domain gemdo for referencing commands that affect a gem that happens to be embedded in a larger repo. A code repo containing more than one gem might use: rake gemdo:build:gemname Bash CLIs Bash scripts are often used for simple CLIs that wrap around more complex operations. Most repo-wide chores that do not require specialized Ruby-based tools like Asciidoctor or other gems are handled with Bash scripts (The significant exception to this are multi-repo libraries like the DocOps Lab Devtool.) The one truly major Bash CLI we maintain is docksh, our Docker shell utility for launching properly configured containers for development, testing, and deployment (sourced in box). Bash CLI Model Base CLIs are relatively open ended. Developers should consider how the script might change, but unless it is intended to be elaborate from the start, there is not much reason to fuss over complicated structures. See DocOps Lab Bash Coding Guide for details about implementing Bash CLIs. Let&#8217;s examine our typical Bash script CLI structure: ./bashscript.sh [arguments] [options] If a Bash script is likely to eventually need to encompass multiple arguments or options, consider making it a Rake task and invoking Ruby scripts, instead. General CLI Principles Most of our user-facing applications are Ruby gems, and most of those are intended to be used via three primary interfaces: An application specific, openly designed CLI utility. An application configuration file. Subject-matter content or domain-specific data of some kind. By way of these three interfaces, users can operate the application in a way that is optimized for their particular use case. CLIs should allow for runtime configuration overrides and even runtime content/data overrides. But most of all they should focus on conveniently putting power in users' hands. This means leaving the CLI model open to the task at hand, but it also means adhering to some conventions that apply generally to both Ruby and Bash CLIs. When NOT to Use a CLI Even when an application offers a mature, well-designed CLI, there are times when either an application programming interface (API) or a domain-specific language (DSL) is preferable. Typically we want to keep complicated shell commands out of core products and CI/CD pipelines, in favor of native or RESTful APIs or else config-driven or DSL-driven utilities. Semantic CLI Namespaces When designing CLIs, consider the namespaces of the elements we use: subcommands, arguments, and options/flags. Subcommands should be verbs or nouns that declare operations or contexts. At each position, these elements should be organizable into meaningful categories. Arguments should be meaningful nouns that represent the primary subject or subjects of the command. General CLI Conventions The definitive reference on CLI design is the CLI Guidelines project. Option format Use spaces rather than = to assign values to options. Flag forms such as --option-name value are preferred over --option-name=value. Provide long- and short- form flag aliases for common options. For ex: -h and --help, -c and --config. Use --no- prefix for negated boolean flags when applicable. For ex: --no-cache to disable caching. Command structure Use subcommand only with apps that perform categorically diverse operations, Prefer flag combinations when possible. Subcommands signal a shift in execution context, and thus they can be greatly helpful when needed. Otherwise, reserve the first argument slot for something a meaningful arbitrary argument. A CLI with very handy subcommands git fetch git commmit git merge No subdomain needed rhx 1.2.1 --config test-config.yml --mapping apis/jira.yml --verbose --fetch --yaml rhx 1.2.1 --config test-config.yml --html And yes, of course you can combine fixed subdomains with arbitrary arguments. git diff README.adoc Avoid using Unix-style argument structures. Arbitrary arguments should come before options, even if that is counter-intuitive. Typically in our apps, users are modifying commands that get executed on the same target, so if the target is an arbitrary file path or version number, it should closely follow the command as an early argument. Preferred argument order cliname targetfile --option1 value1 --option2 value2 --verbose --force This structure lets users more conveniently change the parts of the command-line that will need more frequent changing. Accommodate Unix-style CLIs by adding named options for every arbitrary argument supported. The trick is to enable those cases where the subject path or code is what gets changed most often. rhx --yaml --version 1.2.6 rhx --yaml --version 1.3.1"
    },{
      "id": 14,
      "url": "/docs/code-commenting/",
      "title": "Code Commenting Guidance",
      "content": "Employing good code commenting practices is more important than ever in the age of LLM-assisted programming. Existing comments are a model for future comments, and poor commenting hygiene is contagious. In order to maximize the usefulness of code comments for both human and AI readers, DocOps Lab projects follow specific commenting conventions, including purpose and style constraints. LLM-backed tools and linters are used to review comments and enforce adherence to these conventions, but developer attention is critical. Comments are unlikely to be improved upon after initially merged. Code Comments Orientation To begin, we will standardize our understanding of what types of comments are applied to what kinds of code. Kinds of Comments Code comments come in several distinct types. documentation Code comments used to build downstream-facing reference docs for methods, classes, functions, data objects, and so forth. Docstrings are specifically comments used in the generation of rich-text reference docs. That they also happen to &#8220;document&#8221; the code to which they are adjacent is secondary. expository Code comments that explain the purpose or function of code blocks, algorithms, or complex logic, strictly in natural language. Also called &#8220;inline comments&#8221;, these arbitrary remarks are mainly what is governed by this protocol guide. rationale Comments that explain the reasoning behind a particular implementation choice, design pattern, or data structure. status Stability and lifecycle markers: DEPRECATED:, EXPERIMENTAL:, INTERNAL:, UNSTABLE:. May also include planned removal dates, version gates, feature flags. admonition Developer-facing warnings, notes, or tips embedded in code. Use WARNING:, NOTE:, and TIP: prefixes to mark these comments distinctly. task Comments like TODO: and FIXME: are used to mark code that needs further work. instructional Code comments left in template, stub, or sample files for interactive use. These comments tend to be intended for a downstream user who will interact directly with the file or one based on it. label Comments that simply annotate sections of code by category or general purpose, to help with demarcation and navigation. These comments are usually brief and may use special formatting to stand out. directive In some languages, we use special character patterns to signify that a comment has a special purpose, other than for generating reference docs. These comments may mark code for special parsing, content transclusion, or other operations. In AsciiDoc, comments like // tag::example[] and // end::example[] are used to mark content for inclusion elsewhere. The popular linter Vale recognizes HTML comments like &lt;!-- vale off -&#8594; and &lt;!-- vale on -&#8594; to disable and re-enable content linting. sequential/collection Comments that number or order logical stages in a complex or lengthy process or members of a set. Usually something like # STEP 1:, # PHASE 1:, and so forth, or else # GROUP A:, # SECTION 1:, etc. Always use uppercase for these markers (ex: # STEP: not # Step:). Flavors of Code Ruby The most robust environment for code comments, Ruby supports RDoc/YARD-style documentation comments that can be used to generate reference documentation. See Ruby Commenting Protocols for more. Bash We make extensive use of comments in Bash scripts, but Bash has no standard for documentation comments or structured comments. AsciiDoc Comments in .adoc files tend to be labels, tasks, and directives (AsciiDoc tags). AsciiDoc files tend not to have expository comments, since the content is already documentation. YAML/SGYML YAML files use copious label and instructional comments to help downstream users navigate and understand large or complex data structures. Comments can also be used to annotate nesting depth. See YAML Commenting Protocols for more. Liquid Our use of Liquid comments is inconsistent at best. Part of the problem is their terrible format with explicit {% comment %} and {% endcomment %} tags. While Liquid 5 has greatly improved that, DocOps Lab tooling is standardized on Liquid 4 at this time. HTML We don&#8217;t code much HTML directly. It is mostly either converted from lightweight markup or rendered by Liquid templates (or JavaScript). Comments are usually to mark nested objects for convenience, to label major structures or to highlight/clarify obscure asset references, or as directives such as &lt;!-- vale off -&#8594;, which disables content linting. JavaScript We are not a JavaScript shop, but we do write a good bit of vanilla JavaScript. Comments are used mainly to establish our bearings in the code and therefor are sometimes heavier than with other languages. CSS/SCSS We mainly write CSS as SCSS, and commenting is mainly to express the intent upon compiling. General Style Rules Do not use em dashes or en dashes or (` - `). Use colons (`: `) to prefix a comment with a classification. Use semicolons (`; `) to break up clauses. Use sentence-style capitalization. Do not use terminal punctuation (periods, exclamation points, question marks) unless the comment is multiple sentences. Hard wrap comments around 110-120 characters. Use one-sentence per line. Try not to wrap anything except full sentences. When wrapping multi-line sentences, indent subsequent lines by an additional space. Expository Comments: Use and Abuse Arbitrary inline comments used to explain code should be used consistently and only when they add value. Arbitrary comments can often add value, under an array of conditions that may be more art than science. We must be forgiving and understanding of occasional or even frequent misfires in various developers' subjective takes on what is useful. This guide exists to help with comment evaluation. Principles Expository comments (and their authors) should adhere to these principles: 1) Express purpose, not implementation. Comments should explain why code exists or what it is intended to do, rather than how it does it. (Rationale comments are available for explaining design/engineering choices, if necessary.) 2) Summarize peculiar or complex implementation (without violating #1). Expository comments may include a brief reference to an explicit design choice. Still not a rationale comment (too brief, in passing) nor a task comment (no further action prescribed), just a nod to an unusual or non-obvious implementation detail. 3) Use natural, imperative language. Comments should not contain code, and they should be formatted as English clauses or sentences. Comments should be phrased as commands or instructions, focusing on the action being performed, from the perspective of what the code is to do. 4) Be concise. Comments should be as brief as possible. Multi-sentence comments should be the exception. In fact, comments should not typically be complete sentences. 5) Maintain relevance and accuracy. Comments should be reviewed and updated as code changes to ensure they remain accurate and relevant. 6) Never cover straightforward code (except&#8230;&#8203;). Not all blocks need comments at all. The main criterion is whether the code&#8217;s purpose or function would not be immediately clear from the code itself to a newcomer with beginner or intermediate knowledge of the language and little familiarity with the application architecture. Exception: Sometimes an oddity or pivotal point needs to be highlighted even in otherwise straightforward code. 7} Do not use comments as notes to reviewers. Temporary comments intended to guide code reviewers should be avoided. Code used to help with flag logical points or communicate during pair programming or pre-commit review should be denoted as admonitions (such as `# LOGIC: ` or `# REVIEW: `) or `# TEMP: ` and removed before merging. General Examples Unnecessary Comments # Create destination directory if needed FileUtils.mkdir_p(File.dirname(target_path)) This code does exactly and only what the English comment says. In fact, the comment is muddier than the code. The code will create any necessary parent directories, whereas the comment only mentions the destination directory itself and does not explain if needed. In mkdor_p, the if needed means if the ancestor directories do not exist. # Determine if we should copy the file file_existed_before_copy = File.exist?(target_path) This comment is trying to explain the purpose of the line it precedes, but this is unnecessary. The code itself merely sets a variable to a Boolean value. Not only is the direct purpose of the variable clear from its name and the code making up its value, but the purpose of the variable is only relevant in the context of later code that uses it. Comments that Add Value end # Public helper methods accessible to LogIssue class def normalize_source_path source_file normalized = source_file.gsub(/#excerpt$/, '').gsub(%r{/$}, '') normalized.gsub(%r{^\\./}, '') end def normalize_problem_path reported_path, source_file A comment preceded and followed by blank lines indicates that it references or labels multiple subsequent blocks. (Or it is be part of a series of such comments that tend to and in its own case may yet still cover multiple blocks each.) This categorizes sections for user convenience. It also helps LLM-backed tools to find relevant sections more easily. # Try to convert absolute path back to relative path if missing_path =~ %r{/home/[^/]+/[^/]+/work/[^/]+/(.+)$} || missing_path =~ %r{/([^/]+/[^/]+\\.adoc)$} @path = Regexp.last_match(1) end Summarizing a complex Regex pattern is vital. Conveying the intent of the pattern is far more important than explaining its mechanics. Style Expository comments have a subject: the code they refer to, typically in the form of a line or block. In nearly all cases, comments should immediately precede the subject code. Example of comment preceding subject code # Validate all inputs individually inputs.each do |input| # ... end In some languages, comments can be placed inline. This should be used sparingly. We most commonly do this in YAML files. Example of inline comment in YAML inputs: # List of inputs to validate - input1 - input2 We also do this in JavaScript files. Example of inline comment in JavaScript let count = calculated; // Start with the dynamic value Examples Good comment examples # Calculate the factorial of a number using recursion # Handle the base case # Never call factorial with a negative number # Validate all inputs individually Good comments are descriptive and purely abstract. They express an instruction and/or a principle to be adhered to or enforced within the subject block. Bad comment examples (too simple/unnecessary) # Initialize the result to 1 # Loop through numbers from 1 to n # Return the result Bad comment examples (non-imperative form) # Calculates the factorial of a number Comment Protocols by Language Ruby Commenting Protocols All public-facing methods and classes should be documented with YARD documentation comments. For expository comments, follow the Principles outlined above. API Documentation Comments Many of our Ruby gems provide public APIs that are documented using YARD. Private methods and classes may also be documented, but this is not required. Never describe a method just by what it returns or what parameters it takes. Describe what the method does behind the scenes or what its summarized purpose. When documenting Ruby classes and methods with YARD, follow these patterns: class descriptions Keep class descriptions focused on the class&#8217;s primary responsibility and role within the system. Avoid overselling capabilities or implementation details. method descriptions Lead with what the method accomplishes, not just its signature. Example: \"Processes the provided attributes to populate Change properties\" rather than \"Initializes a new Change object.\" capitalization consistency When referring to class objects conceptually or as an instance (not variable names), use CamelCase names. Use lowercase for most instances where the term refers to a real-world object or concept. voice consistency Use descriptive, present-tense &#8220;voice&#8221; for API documentation and YARD comments. Exceptions On rare occasions, comments are used to denote deep nesting in large files. Annotating end keywords that wrap up large blocks/statements end # method my_method end # class MyClass end # module MyModule end # module SuperModule end # module OurCoolGem Whenever possible, even when deep nesting is warranted, keep files small enough that such labels won&#8217;t be need, all else being equal. YAML Commenting Protocols YAML files often contain extensive comments to help users understand the structure and purpose of the data. Comments should be used to label sections, explain complex structures, and provide hints or assistance for downstream/later users populating data fields. Examples of YAML comments # General data inputs: - name: input1 # required - name: input2 # optional config: # Settings for the application itself setting1: value1 # Enable feature X (which is not called setting1 and thus needs translation) body: | # Use AsciiDoc format This is content for the body of something. We sometimes use comments to categorize a large Array or Map for navigation, even if the data is included in all members of the Array. Example of YAML section label # POSTS - slug: first-post title: My First Post type: post - slug: second-post title: My Second Post type: post # - etc # PAGES - slug: about title: About Me type: page # - etc This is used when it makes no sense to nest data under parent keys like posts: and pages:, yet users will still need to navigate through large collections."
    },{
      "id": 15,
      "url": "/docs/docker/",
      "title": "Dockerfile and Docker Image Management",
      "content": "DocOps Lab projects make extensive use of Docker. All runtime projects provide have their own Docker image hosted on Docker Hub and sourced in their own repo&#8217;s Dockerfile. This way a reliable executable is available across all platforms and environments. Some of our CI/CD pipelines will be &#8220;Dockerized&#8221; to provide consistent builds and tests across numerous repos. The DocOps Box project maintains an elaborate Dockerfile and image/container management script (docksh) that can help manage multiple environments. This is most advantageous for non-Ruby/non-programmer users building a complex documentation codebase in the Ruby/DocOps Lab ecosystem or using multiple DocOps Lab or similar tools across numerous multiple codebases. Application Dockerfiles and Images Each runtime application project has its own Dockerfile in the root of its repository. This Dockerfile defines the image that will be built and pushed to Docker Hub for use by anyone needing to run the application. Some Dockerfiles combine multiple applications, such as the issuer-rhx image, which combines both the Issuer and ReleaseHx applications."
    },{
      "id": 16,
      "url": "/docs/git-commit-styles/",
      "title": "Git Commits Style Guide",
      "content": "General Style (Conventional Commits) DocOps Lab loosely follows the Conventional Commits specification for Git commit messages. Enforcement is not strict, but using Conventional Commits style is encouraged for consistency and clarity. Most DocOps Lab projects do not base Changelog/Release Notes generation on commit messages. The basic outline for a Conventional Commit message is: &lt;type&gt;[optional scope]: &lt;description&gt; [optional body] [optional footer(s)] Commit Description The commit description should be concise and to the point, summarizing the change in 50 characters or less. Use the past tense rather than imperative mood (e.g., \"Added feature X\" instead of \"Add feature X\"). Commit Types Use present-tense descriptive verbs (&#8220;adds widget&#8221;, not &#8220;added&#8221; or &#8220;add&#8221;) feat: &#8230;&#8203; for new features OR improvements fix: &#8230;&#8203; for bugfixes chore: &#8230;&#8203; for version bumps and sundry tasks with no product impact docs: &#8230;&#8203; for documentation changes test: &#8230;&#8203; for test code changes refactor: &#8230;&#8203; for code restructuring with no functional changes style: &#8230;&#8203; for formatting, missing semi-colons, etc; no functional changes perf: &#8230;&#8203; for performance improvements auto: &#8230;&#8203; for changes to CI/CD pipelines and build system Commit Body Conventions Use the body to explain what and why vs. how. Reference issues and pull requests as needed. Use bullet points (- text) and paragraphs as needed for clarity. Do not hard-wrap lines, but do: use 1-sentence per line keep sentences short"
    },{
      "id": 17,
      "url": "/docs/github-issues/",
      "title": "GitHub Issues Types and Tasks",
      "content": "DocOps Lab projects use GitHub Issues to track work items and user reports. We also use our own tools to manage GH Issues: Issuer for bulk-posting issues ReleaseHx for generating release notes and changelogs from issues. See Using GitHub Issues for DocOps Lab Projects for more about managing issues in DocOps Lab projects. Issue Types Task A specific piece of work that does not directly lead to a change to the product. Used for research, infrastructure management, and other sundry/chore tasks not necessarily associated with repository code changes. Bug Reports describing unexpected behavior or malfunctions in the product. Bug issues are used directly and become bugfixes (no technical type change) once resolved. Feature Requests or ideas for new functionality in the product. Improvement Enhancements of existing features or capabilities. Epic An issue or collection of issues with a common goal that may involve work performed across release versions (&#8220;milestones&#8221;). Issue Labels All DocOps Lab projects use a common convention around GitHub issue labels to categorize and manage issues. Project-specific Labels component:&lt;part&gt; Label prefix for arbitrarily named product aspects, modules, interfaces, or subsystems. Common components include component:docker, component:cli, and component:docs (see next section). These correspond to the part property in ReleaseHx change records. Standard Documentation Labels component:docs Indicates the issue pertains to documentation infrastructure, layout, deployment, but not core content. documentation The issue relates to documentation content updates or improvements. needs:docs The issue requires documentation updates as part of its resolution. Documentation updates will likely be in a sub-issue with a documentation label. needs:note The issue requires a note in the release history when resolved. Release notes are appended to the description body under ## Release Note. changelog The issue summary should be included in the changelog for the next release, even if no release note is included. Admonition Labels REMOVAL Removes functionality or features. DEPRECATION Announces planned removal of functionality or features in a future release. (Only appropriate for documentation issues.) BREAKING Includes one or more changes that are not backward-compatible. SECURITY Addresses or documents a security vulnerability or risk. Other Standard Labels question User or community member inquiries about the product or project. priority:high Indicates that the issue is important and should be prioritized for release as soon as possible. priority:low The issue is not urgent and can be addressed in a future release. priority:stretch Issue is slated for the next release but can be bumped if it&#8217;s holding up releasee. wontfix The issue will not be addressed. Comment from maintainers should explain why. duplicate The issue is a duplicate of another issue, which should be linked in the comments. posted-by-issuer Indicates that the issue was created by the Issuer tool. good first issue Designates an issue suitable for new contributors to the project. help wanted Indicates that maintainers are seeking assistance from the community to resolve the issue."
    },{
      "id": 18,
      "url": "/docs/infrastructure/",
      "title": "Development &amp; Deployment Infrastructure",
      "content": "This document addresses a standardized codebase structure and deployment configuration that is common across most DocOps Lab projects. While nearly every project will differ from this in some ways, developers and writers should strive to maintain consistency and conform to these conventions wherever possible when contributing to DocOps Lab projects. Common Project Paths DocOps Lab projects tend to contain many of the same files across codebases. Documentation of these files in particular will be added when possible, but for now this basic guide will have to suffice. Documentation Paths Only two files are required in every DocOps Lab project, though most projects should contain most of these files, depending on the nature of the codebase. A docs/ or _docs/ directory is close to the third universal requirement, necessary by the time a project reaches version 1.0.0. README.adoc Project documentation in AsciiDoc format, providing an overview and instructions. DocOps Lab READMEs typically include single-sourcing data for the product as AsciiDoc attributes. See the Sourcerer project. LICENSE The project&#8217;s license file, specifying the terms under which the code can be used and distributed. Almost always MIT License. docs/ / _docs/ Directory for additional documentation, guides, and related materials. Typically docs/ for product user documentation, whereas _docs/ is for (a) repos that are mainly for websites or (b) internal engineering documentation files (more often found at docs/_docs/). Both might be present in the case of a website that hosts docs and has its own docs. A docs/ directory will typically have its own Gemfile, configs, and assets for Jekyll, Yard, and other generators. A _docs/ directory is usually a content-only subordinate to the main project and its content, and may not have separate configs or assets. Configuration .config/ Configuration files for tooling used in development, building, or QA/testing. Not always used. .config/releasehx.yml Configuration file for ReleaseHx, a tool for generating release notes and changelogs. .config/jekyll.yml Configuration file for Jekyll docs publication. For Jekyll extensions (themes and plugins), this file is typically ./_config.yml to conform to Jekyll defaults. .config/vale.ini Configuration file for Vale, a linter for prose, defining linting rules and styles. .config/.vendor/ Directory for upstream configuration files, mostly or entirely managed by docopsab-dev gem. These files are not tracked in Git but are synced with upstream sources and maintained by DocOps Lab. Containerization Dockerfile Dockerfile for building the project&#8217;s Docker image, defining the environment and dependencies. .dockerignore Specifies files and directories to ignore when building the Docker image. docker-compose.yml Defines and runs multi-container Docker applications, if applicable. Ruby Files These files are common to Ruby-based DocOps Lab projects. The Gemfile and Gemfile.lock may be present in non-Ruby codebases that use Ruby development dependencies, such as ReleaseHx. Gemfile Ruby Bundler file, specifying gem dependencies for the project. Gemfile.lock Generated by Bundler, this file locks the gem versions used in the project. .ruby-version Specifies the Ruby version used in the project. &lt;gemname&gt;.gemspec Ruby gem specification file, defining the gem&#8217;s metadata and dependencies. Automation Paths Rakefile Ruby Rakefile for defining tasks and automation scripts. scripts/ Custom scripts for automating tasks related to development, testing, and deployment. See Common Automation Scripts below. .github/workflows/ GitHub Actions workflows for CI/CD, defining automated build, test, and deployment processes. Quality Assurance Paths Any files containing requirements, specifications, definitions, schemas, or tests should be stored in the specs/ directory, as detailed in DocOps Lab Testing &amp; Specifications. specs/ General directory for content that specifies, defines, or tests elements of the product. See DocOps Lab Testing &amp; Specifications. specs/data/ Definition and schema files. specs/tests/rspec/ RSpec tests for Ruby codebases. ../&lt;product-slug&gt;-demo/ Major products typically have a sibling repo that serves as a proving grounds and/or for demonstrative purposes. Generative AI Paths .github/copilot-instructions.md Instructions for GitHub Copilot, providing guidance on how any cloud-based GH Copilot assistance should be oriented toward a given codebase. AGENTS.md General for local coding agents. May duplicate .github/copilot-instructions.md or provide additional context. .agent/ A directory for temporary/scratch files used by local coding agents. Common Automation Scripts Some DocOps Lab projects include highly customized automation scripts, but most contain or employ some common scripts that are primarily stored in this repository and/or deployed as Docker images for universal access during development, testing, and deployment. These procedures can always be invoked by way of local scripts located in scripts/. These include: build.sh publish.sh Common scripts are managed through the lnk:/docs/lab-dev-setup/[docopslab-dev gem]. Ruby projects will generally include a Rakefile (in the base directory), which automates various Ruby tasks."
    },{
      "id": 19,
      "url": "/docs/lab-dev-config/",
      "title": "Dev-tooling Configuration",
      "content": "This guide pertains to the docopslab-dev environment. For complete documentation, see the project&#8217;s README. The environment described and provided here is not optimized for DocOps Lab applications used in third-party projects. For your own applications of DocOps Labs products like ReleaseHx and Issuer, see DocOps Box for a full-featured docs-focused workspace, runtime, and production environment. For dev-tooling setup instructions, see DocOps Lab Dev-tooling Setup. For dev-tooling usage instructions, see DocOps Lab Dev-tooling Usage. Initialization automatically creates .config/docopslab-dev.yml, which you can edit, or you can create it manually. source: repo: DocOps/lab ref: v1 root: gems/docopslab-dev/assets/config-packs docs: - source: docs/agent/AGENTS.md target: AGENTS.md synced: false - source: docs/agent/skills/*.md target: .agent/docs/skills/ synced: true - source: docs/agent/topics/*.md target: .agent/docs/topics/ synced: true - source: docs/agent/roles/*.md target: .agent/docs/roles/ synced: true tools: - tool: rubocop files: - source: rubocop/base.yml target: .config/.vendor/docopslab/rubocop.yml synced: true - source: rubocop/project.yml target: .config/rubocop.yml synced: false - tool: vale files: - source: vale/base.ini target: .config/.vendor/docopslab/vale.ini synced: true - source: vale/project.ini target: .config/vale.local.ini synced: false - tool: htmlproofer enabled: false # Disabled by default, enable per project files: - source: htmlproofer/base.yml target: .config/.vendor/docopslab/htmlproofer.yml synced: true - source: htmlproofer/project.yml target: .config/htmlproofer.yml synced: false paths: lint: docs/_site - tool: shellcheck files: - source: shellcheck/base.shellcheckrc target: .config/shellcheckrc synced: true - tool: actionlint files: - source: actionlint/base.yml target: .config/.vendor/docopslab/actionlint.yml synced: true - source: actionlint/project.yml target: .config/actionlint.yml synced: false Properties Reference tools (Array) List of tool configurations to enable and manage. Each entry may/must include: tool (Slug) Name of the tool, ex:, rubocop, vale, htmlproofer, actionlint, shellcheck. enabled (Boolean) Whether to enable this tool&#8217;s tasks and git hooks. files List of files to init or sync for the tool. source Path within the gem where the base config is located, e.g., config-packs/rubocop/base.yml. target Path in the project where the file should be synced, e.g., .config/.vendor/docopslab/rubocop.yml. paths Repo=specific paths to include or exclude in linting operations for this tool. lint (Array) List of paths or glob patterns to lint with this tool. skip (Array) List of paths or glob patterns to exclude from linting with this tool. exts (Array) List of file extensions to include in linting with this tool. git_tracked_only (Boolean) Whether to limit linting to only Git-tracked files. docs (Array) List of documentation files to sync from the gem to the target project. Each entry includes: source (String) Source path relative to lib/docopslab/ in the gem. Supports glob patterns (e.g., docs/agent/*.md) or specific files. target (String) Target path relative to the project root. Can be a directory (e.g., _docs/) or specific file path (e.g., AGENTS.md). synced (Boolean) Whether to update existing files on sync. true - Always overwrite on sync (keeps docs current with gem updates) false - Create once, preserve user customizations Documentation Syncing Examples Customizable template (create once, allow modifications) docs: - source: docs/AGENTS.md target: AGENTS.md synced: false Auto-synced agent guides (keep current) docs: - source: docs/agent/*.md target: .docopslab-dev/agent/ synced: true Mixed strategy (glob + specific override) docs: - source: docs/agent/*.md target: _docs/ synced: true - source: docs/agent/ruby.md target: _docs/styles/ruby-custom.md synced: false See tool-specific sections in the various guides, such as for Vale (Documentation Style Guide), RuboCop (Ruby Style Guide), and HTMLProofer."
    },{
      "id": 20,
      "url": "/docs/ruby-styles/",
      "title": "Ruby Development Style Guide",
      "content": "This guide outlines the Ruby coding styles and conventions used in DocOps Lab projects. Automated Style Enforcement DocOps Lab projects using the docopslab-dev gem automatically enforce Ruby style guidelines through: RuboCop Automated code style checking and auto-fixing Git Hooks Pre-commit advisory checks, pre-push quality gates CI/CD Integration Automated linting in GitHub Actions workflows To apply style fixes: bundle exec rake labdev:heal:ruby See DocOps Lab Dev-tooling Usage for setup details. Conventions DocOps Lab largely follows Ruby&#8217;s community conventions, with some exceptions. Conventions are either reiterated or clarified here. However, conventions are not exhaustively listed, and deviations are rarely pointed out as such. Naming Conventions Use snake_case for variable and method names. Use CamelCase for class and module names. Use SCREAMING_SNAKE_CASE for constants. Use descriptive names that convey the purpose of the variable, method, or class. Avoid abbreviations unless they are widely understood. Use verbs for method names to indicate actions. Use nouns for class and module names to indicate entities. Architectural Conventions Use classes and class instance methods for objects that work like objects&#8201;&#8212;&#8201;they have state and do not act on other objects' state. Use module methods acting on objects or carrying out general operations/utility functions. Use Rake for internal (developer) CLI; use Thor for user-facing CLI Gems may begin life as a module within another gem. Path Conventions Use lib/ for main application code. lib/&lt;project_name&gt;.rb for the main file lib/&lt;project_name&gt;/ for supporting files and modules lib/&lt;project_name&gt;/&lt;module_name&gt;/ for submodules Use spec/ for specifications and tests. Use docs/ or _docs/ for documentation. Use build/ for pre-runtime artifacts. Use _build/ as default in applications that generate files at runtime, unless another path is more appropriate (ex: _site/ in Jekyll-centric apps). Do NOT assume or insist upon perfect alignment with Ruby path conventions: SomeModule or SomeClass may be sourced at lib/some_module.rb or lib/some_class.rb instead of lib/some/module.rb or lib/some/class.rb. Some modules like SchemaGraphy and AsciiDoc are never broken up into schema_graphy or ascii_doc namespaces. Modules with multiple parallel sibling modules in a category like (WriteOps, DraftOps) belong in paths like lib/ops/write.rb instead of lib/write_ops.rb or lib/write/ops.rb. Syntax Conventions Use 2 spaces for indentation. Limit lines to 120 characters or so when possible. Use parentheses for method calls with arguments, but omit them for methods without arguments. Do not use parentheses in method definitions (def method_name arg1, arg2). Use single quotes for strings that do not require interpolation or special symbols. Use double quotes for strings that require interpolation or special symbols. Commenting Conventions See DocOps Lab Code Commenting Guidance for detailed commenting conventions. RuboCop Customization These rules (&#8220;cops&#8221;) can be overridden on a per-project basis in the .config/rubocop.yml file. See RuboCop Configuration for docopslab-dev-managed RuboCop configuration. Project Ruby Style Guide (Customizations Only) This document lists only deviations from the standard RuboCop defaults. For everything else, consult: RuboCop Style Guide (All Cops) Generated from gems/docopslab-dev/assets/config-packs/rubocop/base.yml compared to built-in defaults. All Cops Display Style Guide true New Cops enable Target Ruby Version 3.2 Layout: First Method Argument Line Break Cop documentation Enabled true Layout: Hash Alignment Cop documentation Enabled false Layout: Line Length Cop documentation Allowed Patterns \\A\\s*#.*\\z \\A\\s**.*\\z Layout: Multiline Method Call Brace Layout Cop documentation Enforced Style same_line Layout: Parameter Alignment Cop documentation Enforced Style with_fixed_indentation Layout: Space Around Equals In Parameter Default Cop documentation Enabled false Enforced Style no_space Layout: Space Around Operators Cop documentation Enabled false Metrics: Abc Size Enabled false Metrics: Block Length Cop documentation Allowed Methods describe context feature scenario let let! subject task namespace Max 50 Metrics: Cyclomatic Complexity Enabled false Metrics: Method Length Cop documentation Max 25 Metrics: Perceived Complexity Enabled false Naming: Predicate Method Cop documentation Enabled false Security: YAMLLoad Cop documentation Enabled false Style: Comment Annotation Cop documentation Keywords TODO FIXME OPTIMIZE HACK REVIEW Style: Commented Keyword Cop documentation Enabled false Style: Documentation Cop documentation Enabled false Style: Method Call With Args Parentheses Cop documentation Allow Parentheses In Chaining true Allow Parentheses In Multiline Call true Style: Method Def Parentheses Cop documentation Enforced Style require_no_parentheses RuboCop Configuration Ruby code style and quality checking. Base config .config/.vendor/docopslab/rubocop.yml Project config .config/rubocop.yml (inherits via inherit_from) Sync command bundle exec rake labdev:sync:configs The base configuration provides DocOps Lab Ruby style standards. Your project config can override any rule while maintaining consistency with the broader ecosystem."
    },{
      "id": 21,
      "url": "/docs/testing/",
      "title": "Specs &amp; Tests",
      "content": "Most DocOps Lab projects include a specs/ directory in the base. This path is for all &#8220;definitional&#8221; code and content, including: YAML- or JSON-formatted schema or definition documents &#8220;natural language&#8221; requirement/specification documents test scripts in RSpec or other formats test data files test configurations The typical structure of this path is: specs/ docs/ # natural language PRDs *.adoc data/ # defs, schemas, test data *.yml,*.yaml tests/ *.rb, *.sh # test scripts rspec/ # RSpec test files spec_helper.rb *_spec.rb results/ # Test output logs Specifications, Requirements, and Definitions DocOps Lab project development is &#8220;docs-driven&#8221;, meaning we write up our code requirements and specifications in natural language documents before we start coding. These files tend to take the following form: README.adoc Especially during early development, fairly detailed product documentation is stored here. specs/docs/*.adoc Natural language product requirement documents (PRD) specifying dependencies, features, behaviors, and inter-component contracts and interfaces. specs/data/*.yml YAML-formatted definition files that designate actual attributes and content of the app, typically including config-def.yml for configuration properties defaults and docs. specs/data/*-schema.yaml SGYML-formatted schema files that define the structure and constraints user-editable YAML files, such as configuration files. Testing Infrastructure Standards DocOps Lab projects follow consistent testing patterns to ensure reliability and maintainability across the ecosystem. These standards cover test organization, configuration files, and data management used by testing frameworks. RSpec Configuration All Ruby-based projects should include: .rspec Configuration file specifying RSpec options and test file patterns. Standard configuration: --format documentation --color --pattern 'specs/tests/rspec/**/*_spec.rb' specs/tests/rspec/spec_helper.rb Shared configuration, helper methods, and sample data for tests. Should include: Bundler setup and project requires RSpec configuration (monkey patching disabled, expect syntax) Helper methods for temporary file/directory creation Sample data generators for testing Cleanup procedures for test artifacts Standard Rake Tasks All Ruby gem projects with tests should implement these standard Rake tasks in their Rakefile: bundle exec rake rspec Run RSpec test suite using the standard pattern matcher. bundle exec rake cli_test Validate command-line interface functionality. May test basic CLI loading, help output, version information. bundle exec rake yaml_test Validate YAML configuration files and data structures. Should test all project YAML files for syntax correctness. bundle exec rake pr_test Comprehensive test suite for pre-commit and pull request validation. Typically includes: RSpec tests, CLI tests, YAML validation. bundle exec rake install_local Build and install the project locally for testing. Note that non-gem projects may have some or all of these tasks, as applicable. Test Categories Tests should be organized into these categories: Unit Tests Module loading and initialization Class structure validation Basic functionality verification Individual method testing Integration Tests Data processing workflows Template rendering operations Configuration loading scenarios API client functionality (where applicable) Validation Tests File format compliance (YAML, JSON) Configuration schema validation Template syntax verification Command-line option parsing Test Data Management Projects should utilize: Demo/Sample Data Rich sample data in dedicated demo directories (e.g., ../projectname-demo/). Used for integration testing and examples. Generated Test Data Programmatically generated test data using helper methods. Ensures consistent, controlled test scenarios. Temporary Files Automatic creation and cleanup of temporary files/directories. Prevents test pollution and ensures isolated test environments. Test Documentation Each project&#8217;s specs/tests/README.adoc should: Reference shared standards Link to https://docopslab.org/docs/testing. Document project-specific patterns Cover unique testing approaches. Provide quick start instructions Enable new contributors to run tests immediately. Explain integration with demo data Show how sample data is used. List available test commands Document all relevant Rake tasks and their purposes. Testing the Documentation Not only do tests need to be documented, but documentation needs to be systematically tested. These tests are mainly performed via the docopslab-dev tool. DocOps Lab performs markup-syntax linting, prose linting, HTML linting, HTML link testing, and code-block testing. Syntax linting Evaluates AsciiDoc and RDoc source syntax for proper style and formatting. We use Vale for prose linting and custom RuboCop for RDoc comment. HTML linting and link testing Validates generated HTML documentation for proper structure, formatting, and link integrity. See Docs Link Testing with HTMLProofer. Code-block testing Extracts and executes code blocks from documentation to ensure accuracy and functionality. This is performed using the Sourcerer library, which is presently part of the ReleaseHx gem but will soon be released as a standalone gem. Docs Link Testing with HTMLProofer HTML validation for Jekyll sites and documentation builds. Base config .config/.vendor/docopslab/htmlproofer.yml Project config .config/htmlproofer.yml Sync command bundle exec rake labdev:sync:configs Enable in manifest Add htmlproofer tool with enabled: true HTMLProofer validates links, images, and HTML structure in built sites. Only enabled for projects that generate HTML output (Jekyll sites, etc.). Base HTMLProofer Configuration --- # DocOps Lab HTML-Proofer Base Configuration # This provides sensible defaults for DocOps Lab projects # URL checking options check_external_hash: false # Skip checking external URL fragments (can be slow) check_img_http: false # Allow HTTP images for now enforce_https: false # Don't enforce HTTPS for all links # URLs to ignore (patterns and strings) ignore_urls: - /localhost/ # Skip local development URLs - /127\\.0\\.0\\.1/ # Skip local IPs - /example\\.com/ # Skip example URLs - /foo\\.bar/ # Skip placeholder URLs - /fonts.googleapis.com/ # Skip Google Fonts - /fonts.gstatic.com/ # Skip Google Fonts static # Files to ignore (patterns) ignore_files: - slides/ # Skip slides directories # Other options check_favicon: false # Don't require favicon check_html: true # Check HTML structure check_opengraph: false # Skip OpenGraph validation disable_external: false # Check external links (can be disabled for speed) For full HTMLProofer configuration options, see the official docs. Continuous Integration Standards DocOps Lab projects follow consistent CI/CD patterns. These broadly include pre-commit/pre-push testing, GitHub Actions integration, and release testing. Pre-push Testing bundle exec rake pr_test This test runs the complete local suite, including RSpec tests as well as any established CLI testing. GitHub Actions Integration Projects should implement GitHub Actions workflows that: Run on pull requests Execute the complete pr_test suite Test across multiple Ruby versions (where applicable) Validate documentation quality Report test coverage Release Testing Before any merge to main and pre-release. bundle exec rake install_local bundle exec rake pr_test Release testing also involves examining artifacts (packaged gems, Docker image, documentation) before it is published. See below for basics and DocOps Lab Release Process (General) for details. Test Results and Artifacts Test execution should generate: specs/tests/results/ Directory for test output, logs, and reports. Automatically created during test runs. .rspec_status RSpec status persistence file for --only-failures and --next-failure flags. Test results should include: Pass/fail status for all test categories Performance metrics (where applicable) Coverage reports Artifact validation logs"
    },{
      "id": 22,
      "url": "/docs/deployment-setup/",
      "title": "Deployment Setup (General)",
      "content": "This guide describes the new-project setup for deployment platforms for release artifacts like executables, Docker images, and product documentation sites. Prerequisites Required Credentials Use environment variables to store sensitive credentials securely. These credentials are only needed during the release process, not the development phase. RubyGems API Key Location: https://rubygems.org/profile/edit Set as: RUBYGEMS_API_KEY Docker Hub Credentials Organization: docopslab DockerHub account with write permissions for docopslab images Set as: DOCKERHUB_USERNAME, DOCKERHUB_TOKEN GitHub Token (some projects only) Scope: repo Set as: DOCOPSLAB_GITHUB_TOKEN or GITHUB_TOKEN Ruby Gem Publishing Configure GitHub repository secrets with RUBYGEMS_AUTH_TOKEN. Ensure gemspec includes spec.metadata['rubygems_mfa_required'] = 'true'. Docker Image Publishing Create repository on Docker Hub under docopslab organization. Create repository on GitHub Container Registry. Configure GitHub repository secrets with DOCKERHUB_USERNAME and DOCKERHUB_TOKEN. Documentation Site Publishing Most DocOps Lab projects have their own documentation sites, also built with Jekyll and AsciiDoc, often including YARD for Ruby API reference generation. For less-formalized projects, documentation is restricted to README.adoc and other *.adoc files. These are hosted as GitHub Pages sites from their respective repositories, but using a consistent URL structure centered on the docopslab.org domain hosted here. The URL structure is as follows: Project landing page https://&lt;project&gt;.docopslab.org/ At a minimum, this should be a subset of the README.adoc file. Product user docs https://&lt;project&gt;.docopslab.org/docs/ Product developer docs https://.docopslab.org/docs/api/(/) The final &lt;apiname&gt; directory is only applicable when the product contains multiple distinct APIs. GH Pages configuration for these sites enables deployment by way of a clean gh-pages branch containing only generated documentation artifacts and the CNAME file. Setup Steps Create docs/CNAME file containing &lt;project&gt;.docopslab.org. Go to repository Settings  Pages. Set Source to \"GitHub Actions\". Ensure custom domain is set to &lt;project&gt;.docopslab.org. Verify DNS configuration points to GitHub Pages."
    },{
      "id": 23,
      "url": "/docs/development/",
      "title": "Development Process (General)",
      "content": "DocOps Lab projects follow a consistent, if always progressing, architecture and development/release process. This guide focuses on contributing code to projects and products, be it functional code, data, or documentation. More generally, dev contributors will most likely need to use the docopslab-dev tool to coordinate development and testing tasks. Each repository has its own Rakefile with custom tasks, but each also incorporates the common (upstream) docopslab-dev library for extending the rake tasks. Prerequisites Technologies Overview DocOps Lab projects are primarily Ruby gems, usually with associated Docker images that provide proper environments for the CLI associated with the project gem. Ruby is used mainly because of its excellent AsciiDoc tooling through Asciidoctor, with an accompanying preference for Jekyll static-site generator and Liquid templating language, all of which are Ruby native. Docker is employed by internal developers and end users of DocOps Lab tooling alike, to reduce if not eliminate the Ruby maintenance overhead. Required Tools If you are brand new to the world of code, Git, and GitHub, DocOps Lab will soon have resources aimed precisely at your experience level. For now, some experience with these tools is assumed. Must-haves Docker, natively installed: Docker Desktop on MacOs Docker Desktop with WSL2 on Windows Docker Engine on Linux Git, natively installed Ruby runtime 3.2.7, natively or via Docker (recommended) GitHub account Should-haves GitHub CLI (gh) a code editor that supports Ruby, AsciiDoc, and YAML (Try VS Code if you don&#8217;t have a preference yet.) Ruby Environment Ruby dependencies are managed through Bundler using each project&#8217;s Gemfile/.gemspec definitions. A proper Ruby environment and all common (cross-project) development dependencies are supplied in the docopslab/dev Docker image. Containers run from this image provide unified linting, git hooks, and development tooling used by most DocOps Lab codebases. These quality-control tools are also built into all GitHub repos via GH Actions workflows, local availability is not required just to work on or contribute to a DocOps Lab codebase. See DocOps Lab Dev-tooling Setup for comprehensive setup details if you are initializing a new DocOps Lab project. Repository State Development is done on development trunk branches named like dev/x.y, where x is the major version and y is the minor. To start development on a new release version: git checkout main git pull origin main git checkout -b dev/1.2 git checkout -b chore/bump-version-1.2.0 git commit -am \"Bumped version attributes in README\" git checkout dev/1.2 git merge chore/bump-version-1.2.0 git push -u origin dev/1.2 Development Procedures Work on feature or fix branches off the corresponding dev/x.y trunk. git checkout dev/1.2 git checkout -b feat/add-widget  implement  git add . git commit -m \"feat: add widget\" git push -u origin feat/add-widget gh pr create --base dev/1.2 --title \"feat: add widget\" --body \"Adds a new widget to the dashboard.\" Branch naming conventions feat/&#8230;&#8203; for new features OR improvements fix/&#8230;&#8203; for bugfixes chore/&#8230;&#8203; for version bumps and sundry tasks with no product impact epic/&#8230;&#8203; for large features or changes that span releases Commit Message Conventions Description (first line) conventions Use present-tense descriptive verbs (&#8220;adds widget&#8221;, not &#8220;added&#8221; or &#8220;add&#8221;) feat: &#8230;&#8203; for new features OR improvements fix: &#8230;&#8203; for bugfixes chore: &#8230;&#8203; for version bumps and sundry tasks with no product impact docs: &#8230;&#8203; for documentation changes test: &#8230;&#8203; for test code changes refactor: &#8230;&#8203; for code restructuring with no functional changes style: &#8230;&#8203; for formatting, missing semi-colons, etc; no functional changes perf: &#8230;&#8203; for performance improvements auto: &#8230;&#8203; for changes to CI/CD pipelines and build system Body conventions Use the body to explain what and why vs. how. Reference issues and pull requests as needed. Use bullet points (- text) and paragraphs as needed for clarity. Do not hard-wrap lines, but do: use 1-sentence per line keep sentences short See DocOps Lab Git Commits Style Guide for detailed commit message conventions. Merging Changes Squash-merge branches back into dev/x.y: git checkout dev/1.2 git checkout -b feat/add-widget  implement  git add . git commit -m \"feat: add widget\" git merge --squash feat/add-widget git commit -m \"feat: add widget\" git push origin dev/1.2 Delete merged branches. Dev Branch Rules Always branch from dev/x.y. Always squash-merge into dev/x.y. Never merge directly into main. Documentation Practices A critical part of development at DocOps Lab is writing and maintaining good documentation. After all, docs are our business. Standard (User) Docs Standard documentation is mainly done in AsciiDoc. AsciiDoc files are found in the _docs/ or docs/ directory. A _docs/ directory exists for internal documentation. These files may be published in some form, but they should be distinct from docs intended to help end users. A docs/ directory exists for user-facing documentation files. These are published distinctly. For complete documentation styles guidance, see DocOps Lab Documentation Style Guide. Initially, all developer and user-facing documentation is maintained in the README.adoc file, simply for convenience. Prior to a 1.0.0 release, most user-facing and usually some developer-facing documentation should be moved from the README to the docs/ and _docs/ directories. In GitHub Issues, use needs:docs to designate work items that will require standard internal or user-facing documentation. Inline Documentation Some documentation can be accessed through the product (such as --help menus), which is also often sourced inside product files (rather than dedicated doc files). APIs Use YARD for Ruby code documentation. Document all public methods and classes. API docs are published at https://gemdocs.org/PROJECT_NAME/. CLIs Most DocOps Lab CLIs provide --help and even --man flags, for a basic menu or a manpage related to the given command. These will be designated and documented in the project&#8217;s README.adoc file. Release History DocOps Lab projects use ReleaseHx to maintain release notes and changelogs. Each project should have a .config/releasehx.yml file. There are also labels available to designate a GitHub issue&#8217;s status in the documentation announcing the release it belongs to. Use needs:note to designate an issue that should be detailed for end users. Use changelog to designate an issue that should be included in the changelog (just the summary). Release notes are added to the main GitHub issue body as appended Markdown. At the end of the note body, add: ## Release Note One or two sentences summarizing the change for end users. Markdown formatting *will* be converted to AsciiDoc during drafting. See DocOps Lab Release Process (General) for more on generating the release history. Test Development Process All DocOps Lab projects should include comprehensive test suites following consistent patterns. Testing itself is documented in DocOps Lab Testing &amp; Specifications, but this section focuses on creating and maintaining tests as part of development. Adding Tests for New Features When implementing new functionality: Create corresponding tests alongside feature implementation Follow existing patterns established in spec_helper.rb Use descriptive test names that clearly indicate what is being tested Group related tests in logical contexts and describe blocks Add cleanup procedures for any temporary files or resources Test File Template Use this template for new test files: require_relative 'spec_helper' RSpec.describe YourModule do let(:temp_dir) { create_temp_dir } let(:sample_data) { create_temp_yaml_file(your_sample_data) } after do FileUtils.rm_rf(temp_dir) if Dir.exist?(temp_dir) File.unlink(sample_data) if File.exist?(sample_data) end describe \"core functionality\" do context \"when given valid input\" do it \"processes data correctly\" do result = YourModule.process(sample_data) expect(result).to be_a(Hash) expect(result).to have_key('expected_field') end end context \"when given invalid input\" do it \"handles errors gracefully\" do expect { YourModule.process(nil) }.to raise_error(ArgumentError) end end end end Test Data Integration Projects should leverage demo data for realistic testing: Demo Directory Usage Utilize rich sample data from ../projectname-demo/ directories. Validate configuration files, mapping files, and sample data sets. Helper Methods Implement helper methods in spec_helper.rb for: create_temp_yaml_file(content) - Generate temporary YAML files create_temp_json_file(content) - Generate temporary JSON files create_temp_dir - Create temporary directories sample_*_data - Provide realistic test data structures Cleanup Procedures Ensure all tests clean up after themselves: after do FileUtils.rm_rf(temp_dir) if Dir.exist?(temp_dir) File.unlink(temp_file) if File.exist?(temp_file) end Test Maintenance Best Practices Standard Rake Testing Tasks All Ruby gem projects with tests should implement these standard Rake tasks in their Rakefile: bundle exec rake rspec Run RSpec test suite using the standard pattern matcher. bundle exec rake cli_test Validate command-line interface functionality. May test basic CLI loading, help output, version information. bundle exec rake yaml_test Validate YAML configuration files and data structures. Should test all project YAML files for syntax correctness. bundle exec rake pr_test Comprehensive test suite for pre-commit and pull request validation. Typically includes: RSpec tests, CLI tests, YAML validation. bundle exec rake install_local Build and install the project locally for testing. Note that non-gem projects may have some or all of these tasks, as applicable. Test Organization Unit Tests Test individual methods and classes in isolation. Focus on edge cases, error conditions, and expected behavior. Integration Tests Test workflows that span multiple components. Validate data flow through processing pipelines. Validation Tests Test configuration loading, file format compliance. Validate that all demo/example files are syntactically correct. Performance Considerations Use temporary files/directories that are automatically cleaned up. Avoid testing with large datasets unless specifically testing performance. Use mocking/stubbing for external API calls and expensive operations. Group related tests to minimize setup/teardown overhead. Error Testing Test both expected errors and edge cases. Verify error messages are helpful and actionable. Test error recovery and graceful degradation. Validate that errors don&#8217;t leave systems in inconsistent states. Continuous Integration Pre-commit Testing Before committing changes: rake pr_test # Run comprehensive test suite Pull Request Validation Ensure all pull requests: Pass the complete test suite (rake pr_test) Include tests for new functionality Update existing tests when modifying behavior Maintain or improve test coverage Include integration tests for workflow changes Release Testing Before any release: Ensure any new tests are added. Run automated tests. rake install_local # Build and install locally rake pr_test # Complete validation Manually test key workflows. Update or add any documentation. Common Problems Test File Cleanup Tests should automatically clean up temporary files. Manual cleanup: rm -rf /tmp/projectname_test_* Missing Dependencies Ensure bundle install has been run. Check that all required gems are in Gemfile. Demo Data Access Verify that demo directories exist and are accessible. Ensure tests are run from the correct working directory. Debug Mode Run tests with verbose output for troubleshooting: bundle exec rspec --format documentation --backtrace rake pr_test # Often includes verbose options AI Usage Policy DocOps Lab does not share unreviewed AI output with the outside world, period. Such matter is kept from public code repositories, documentation sites, and the rest of our public footprint. As a general rule of thumb, everything we produce that is affected by AI must have been enhanced or improved by the AI&#8217;s contributions. By this, we mean AI output should be at least as good as or better than output we would be able to produce without those tools. We do not release code or content that is inferior, compared to what we can produce ourselves, in terms of: accuracy clarity logic style humanity security maintainability compliance with standards or best practices For the complete policy, see DocOps Lab Generative \"AI\" Guidance."
    },{
      "id": 24,
      "url": "/docs/fix-broken-links/",
      "title": "Fix Broken Links",
      "content": "A systematic approach to debugging and fixing broken links in DocOps Lab websites or sites generated with DocOps Lab tooling. Due to complex sourcing procedures at work in DocOps Lab projects, where a particular link comes from is not always obvious. This guide focuses on the methodologies for tracing link sources rather than specific solutions, making it applicable across different Jekyll/AsciiDoc sites. Common Link Failure Patterns External Link Failures Network timeouts Temporary connectivity issues that resolve after rebuild 404 errors Missing pages or incorrect URLs Pre-publication links Links to repositories or resources not yet available Malformed URLs Missing repository names or incorrect paths Internal Link Failures Missing project anchors Data/template mismatches in generated content Section anchor mismatches ID generation vs link target differences Template variable errors Unprocessed variables in URLs Missing pages Links to pages that don&#8217;t exist Debugging Methodology Step 1: Run HTMLProofer and Categorize Failures Run link validation bundle exec rake labdev:lint:html 2&gt;&amp;1 | tee .agent/scratch/link-failures.txt Extract external failure patterns grep \"External link.*failed\" .agent/scratch/link-failures.txt | wc -l Extract internal failure patterns grep \"internally linking\" .agent/scratch/link-failures.txt | wc -l Step 2: Identify High-Impact Patterns Look for repeated failures across multiple pages: Same broken link appearing on 3+ pages = template/data issue Similar link patterns = systematic problem Timeout clusters = network/rebuild issue Step 3: Trace Link Sources For Missing Anchors (Internal Links and X-refs) If the problem is an anchor that does not exist, either the pointer or the anchor must be wrong. Consider how the page was generated: Is it a standard .adoc file? Is it a Liquid-rendered HTML page? Is it a Liquid-rendered AsciiDoc file (usually *.adoc.liquid or *.asciidoc) For standard AsciiDoc files&#8230;&#8203; The offending link source will likely be: an AsciiDoc xref ( or xref:anchor-slug[]) a pre-generated xref in the form of an attribute placeholder ({xref_scope_anchor-slug_link}) that has resolved to a proper AsciiDoc xref a hybrid reference (link:{xref_scope_anchor-slug_url}[some text]) In any case, the anchor-slug portion should correspond literally to the reported missing anchor. If these are rendering properly and do not contain obvious misspellings, consider how the intended target might be misspelled or missing and address the source of the anchor itself. For Liquid-rendered pages&#8230;&#8203; The offending link source will likely be a misspelled or poorly constructed link. a hard-coded link in Liquid/HTML (&quot;a href=\"#anchor-slug\">) a data-driven link in Liquid/HTML (&quot;a href=\"#{{ variable | slugify }}\">) a data-driven link in Liquid/AsciiDoc (link:#{{ variable | slugify }}) a pre-generated xref in the form of an attribute placeholder ({xref_some-scope_some-slug-string_link}; generated from Liquid such as: {xref_{{ scope }}_{{ variable }}_link}) Other than for hard-coded links, you will need to trace the source to one of the following: A YAML file, typically in a _data/ or data/ directory. Search for the offending anchor grep -rn \"broken-anchor-slug\" --include \\*.yml --include \\*.yaml If there are numerous errors of this kind, the problem could be in the code that generates the attributes from the YAML source. Attributes derived from a file like README.adoc. Search for the offending attribute grep -rn \"^:.*broken-anchor.*:\" --include \\*.adoc Other tips for investigating broken anchors: Check what anchors actually exist grep -on 'id=\"[^\"]*\"' _site/page-slug/index.html Find template generating the links grep -rn \"distinct identifier string\" _includes _pages _templates Step 4: Apply Appropriate Fix Strategy Option A: Fix the Data (Recommended for Project Links) Update dependency names to match actual project slugs: # Before deps: [jekyll-asciidoc-ui, AsciiDocsy] # After deps: [jekyll-asciidoc-ui, asciidocsy-jekyll-theme] Option B: Fix the Template Update link generation to use project lookup: {% assign dep_project = projects | where: 'slug', dep | first %} {% unless dep_project %}{% assign dep_project = projects | where: 'name', dep | first %}{% endunless %} &lt;a href=\"/projects/#{{ dep_project.slug | default: dep | slugify }}\"&gt; Option C: Fix the Anchors/IDs Update actual IDs to match expected links. Use this solution only when the link source is wrong or the target anchor ID is wrong where it is designated or missing. Misspelled link source See xref:sectione-one[Section One] for details. Misspelled anchor ID [[secton-one]] === Section One Data-Driven Link Debugging YAML Data Sources Key files that commonly generate broken links: _data/docops-lab-projects.yml Project dependencies and metadata _data/pages/*.yml Navigation and cross-references Individual frontmatter Local link definitions Dependency Tracing Process Identify the broken link pattern: #missing-anchor Find the data source: Search YAML files for dependency names Trace template processing: Follow Liquid template logic Compare with reality: Check actual generated IDs Apply data fix: Update dependency to match actual slug Example Trace: AsciiDocsy Links # 1. Broken link found # internally linking to /projects/#asciidocsy # 2. Find template source grep -r \"#asciidocsy\" _includes/ # Found in: _includes/project-profile.html line 76 # 3. Check template logic # href=\"/projects/#{{ dep | slugify }}\" # 4. Find data source grep -n \"AsciiDocsy\" _data/docops-lab-projects.yml # Found: deps: [..., AsciiDocsy] # 5. Check actual anchor grep 'id=\".*asciidoc.*\"' _site/projects/index.html # Found: id=\"asciidocsy-jekyll-theme\" # 6. Fix: Change AsciiDocsy  asciidocsy-jekyll-theme Pre-Publication Link Strategy For links to resources not yet available: Tag with FIXME-PREPUB: Add comments for easy identification Document in notes: Track what needs to be updated at publication Use conditional logic: Hide pre-pub links in production builds // FIXME-PREPUB: Update when DocOps/box repository is published See the link:https://github.com/DocOps/box[DocOps Box repository] for details. Validation and Testing Rebuild and Verify # Rebuild site with fixes bundle exec rake build # Re-run validation bundle exec rake labdev:lint:html # Check specific fix grep \"#fixed-anchor\" _site/target-page.html Test Cycle Fix high-impact patterns first (3+ occurrences) Rebuild and validate after each batch of fixes Document fixes for future reference Test both internal and external link resolution Prevention Strategies Development Practices Consistent naming Align dependency names with actual project slugs Template validation Test link generation logic with sample data Documentation standards Document expected anchor patterns Regular validation Include link checking in CI/CD pipelines Configuration Management Default values Define link patterns in configuration rather than hardcoding Validation rules Create checks for common link anti-patterns Documentation Maintain mapping between logical names and actual slugs This systematic approach transforms broken link debugging from a frustrating manual process into a predictable, methodical workflow that scales across projects and team members."
    },{
      "id": 25,
      "url": "/docs/fix-jekyll-asciidoc-build-errors/",
      "title": "Fix Jekyll-AsciiDoc Build Errors",
      "content": "When Asciidoctor errors are encountered during the conversion stage of a Jekyll build operation, use this procedure to clarify and fix them. Procedure Overview PREREQUISITE This procedure requires the docopslab-dev utility. Perform a basic Jekyll build that writes verbose output to a local file. Example with config option bundle exec jekyll build --verbose --config configs/jekyll.yml &gt; .agent/scratch/jekyll-build.log 2&gt;&amp;1 Note the 2&gt;&amp;1 at the end of the command, which ensures that both standard output and error messages are captured in the log file. Run the analysis task on the exported file. bundle exec rake 'labdev:lint:logs[jekyll-asciidoc,.agent/scratch/jekyll-build.log]' Open the YAML file relayed in the response message (example: Jekyll AsciiDoc issues report generated: .agent/reports/jekyll-asciidoc-issues-20251214_085323.yml). Follow the instructions in the report to address the issues found. The Error Report The report is a YAML file that lists the errors associated with their actual locations in the AsciiDoc source. The report contains instructions so that it may be fed in its entirety to an LLM assistant to address the errors. Default instructions # When processing this file, focus on the relationship between # the 'file' and 'with' properties: # # 1. 'file' = The Jekyll file that includes/references the problem # 2. 'with' = The file that contains the actual structural issue # 3. 'kind' types and typical fixes: # - include_file_not_found: Create missing files or fix paths # - section_title_out_of_sequence: Adjust heading levels or leveloffset # - unterminated_listing_block: Close code blocks with proper delimiters # - invalid_reference: Fix broken cross-references # - missing_attribute: Define the attribute locally or via included settings # # Common patterns: # - Multiple files referencing the same with file suggest the issue is in the shared file # - #excerpt errors usually indicate an include is embedded in the excerpted text # - Section title sequence errors often need leveloffset adjustments in include directives # - Missing files may need creation or path corrections"
    },{
      "id": 26,
      "url": "/docs/fix-spelling-issues/",
      "title": "Fix Spelling Issues",
      "content": "This procedure is to help you trace and fix spelling issues in documentation. It uses DocOps Lab&#8217;s custom Vale implementation to identify spelling errors and generate a report for correction. You can then pass that report to an AI agent to help you fix the issues based on the data, your feedback, and further instructions. Procedure Overview PREREQUISITE This procedure requires the docopslab-dev utility. Use the spellcheck task to generate a spelling report. bundle exec rake labdev:lint:spellcheck Open the generated report in your favorite YAML editor. Update the report with your instructions or corrections (see The Spellcheck Report). Provide the updated report to your AI agent for processing. The Spellcheck Report The report is a YAML file that lists the spelling issues found in your documentation. Each entry in the sequence represents an issue detected by Vale, along with its context. The Spellcheck Prompt The report will include a prompt that helps an AI agent understand the procedures for following up on your instructions. The default AI prompt # When processing user responses in this file: # 1. For fix?: 'add' or 'd': Add the term to the filters section in: # gems/docopslab-dev/assets/config-packs/vale/authoring/Spelling.yml # 1.a. For fix?: 'do' or 'dolab' or 'docopslab': Add the term to a special '# DocOpsLab Terms' section # 1.b. For fix?: nontech(\"word\"): Add the term to a special '# Non-Technical Terms' section # 2. For fix?: 'fix': Replace the term in the file with inferred corrected spelling # 3. For fix?: 'fix(\"word\")': Replace the term with the word value verbatim # 4. For fix?: 'no' or 'n': Ignore this issue for now # 5. For fix?: 'pass': Wrap the affected text in {vale_off} and {vale_on} comment attributes # and make sure the attributes are defined in .adoc file header (offer to user): # :vale_off: &lt;!-- vale off --&gt; # :vale_on: &lt;!-- vale on --&gt; # The spelling dictionary is the 'filters:' sequence in the Spelling.yml file # When adding to dictionary, maintain alphabetical order within sections # DO NOT TAKE INITIATIVE. STick to the instructed changes except where prompted with `fix?: 'fix'`. # DO NOT correct neigboring words in text nor add nor alter words in the dictionary other than those named. To override this prompt on a project level, configure it in your .config/docopslab-dev.yml file. spellcheck: output_dir: .agent/spellcheck # defaults to .labdev/spellcheck output_file: spelling-report.yml # defaults to spellcheck-&lt;datetime&gt;.yml prompt: | # Your custom prompt here # Preceed each line with a hash (#)"
    },{
      "id": 27,
      "url": "/docs/github-issues-usage/",
      "title": "Using GitHub Issues",
      "content": "DocOps Lab projects use GitHub Issues to track work items and user reports. We also use our own tools to manage GH Issues: Issuer for bulk-posting issues ReleaseHx for generating release notes and changelogs from issues. See GitHub Issues Types and Tasks Reference. Managing GitHub Issues with gh The GitHub CLI tool, gh, can be used to manage issues from the command line. See GitHub CLI Manual: gh issue for details on using gh to create, view, edit, and manage issues and issue metadata. Some common commands: Create a new issue. gh issue create --title \"Issue Title\" --body \"Issue description.\" --label \"bug,component:docs\" --assignee \"username\" List open issues. gh issue list --state open View a specific issue. gh issue view &lt;issue-number&gt; Bulk-posting Issues with Issuer The issuer tool can be used to bulk-post issues to any repository from a YAML file. Follow the instructions at Issuer to install and use the tool."
    },{
      "id": 28,
      "url": "/docs/lab-dev-setup/",
      "title": "Dev-tooling Setup",
      "content": "This guide pertains to the docopslab-dev environment. For complete documentation, see the project&#8217;s README. The environment described and provided here is not optimized for DocOps Lab applications used in third-party projects. For your own applications of DocOps Labs products like ReleaseHx and Issuer, see DocOps Box for a full-featured docs-focused workspace, runtime, and production environment. If this is your first time using docopslab-dev on a given workstation, you will need to ensure the prerequisites are met. If you have the prerequisites and are just getting started with a given DocOps Lab project, you should be ready after [init-sync]. If you are initializing docopslab-dev in an new project, you will also need to initialize the environment. Prerequisites There are three ways to prepare the necessary dependencies and runtimes. If you are already a Ruby user, Option 1 is likely for you. Otherwise, Option 2 is strongly recommended, at least for getting started quickly. Option 1: Native Installations Ruby &amp; Bundler Ruby 3.2+ with Bundler installed natively gem 'docopslab-dev' in `Gemfile All Ruby gems managed via bundle install Vale brew install vale (macOS) apt install vale (Ubuntu/Debian) dnf install vale (Fedora) If not installed, vale operations will fallback to Docker execution Asciidoctor (to support Vale) installed globally gem install asciidoctor or npm i -g asciidoctor or natively installed through your system&#8217;s package manager Test with asciidoctor --version ShellCheck brew install shellcheck (macOS) apt install shellcheck (Ubuntu/Debian) dnf install shellcheck (Fedora) If not installed, shellcheck operations will fallback to Docker execution actionlint brew install actionlint (macOS)` go install github.com/rhysd/actionlint/cmd/actionlint@latest (Go 1.16 or later) Otherwise see install guide. If not installed, actionlint operations will fallback to Docker execution Option 2: Full Docker Environment All tools available, via docopslab/dev image No native Ruby installation required You&#8217;ll need Docker installed:: Docker Desktop on MacOs Docker Desktop with WSL2 on Windows Docker Engine on Linux Install with docker pull docopslab/dev The docopslab/dev image provides a complete development environment with Ruby, Vale, and all linting tools pre-installed. Add an alias to your shell profile (~/.bashrc, ~/.zshrc, etc.) to make Docker usage easier: alias lab-dev='docker run -it --rm -v \"$(pwd):/workspace\" docopslab/dev' Now lab-dev replaces the full Docker command and causes insertion of bundle exec for rake or labdev: commands. Option 3: Ruby with Docker fallback Ruby &amp; Bundler + Gemfile as in Option 1 Vale and other non-Ruby services run via Docker if not installed locally as in Option 2 All labdev: rake tasks that use non-Ruby dependencies will attempt native execution first, then fall back to Docker if the tool is not found or is found to be the wrong version. Initialize or Sync Once dependencies are installed, the development environment may need to be initialized and must be synced, between your local instance and source assets. Assuming you are not initializing a new project, you can skip to Environment Synchronization. Project Initialization If you are introducing docopslab-dev to an existing project, you first need to integrate and initialize it. Add docopslab-dev to the project&#8217;s Gemfile. group :development do gem 'docopslab-dev' end Install the gem. bundle install Add require 'docopslab/dev' to the top of the project&#8217;s Rakefile. A project lacking any configuration files can now be initialized. Use bundle exec rake labdev:check to ensure the project environment is aligned. Initialize the development environment bundle exec rake labdev:init:all The init task creates .config/docopslab-dev.yml and default project configs for all tools. This file should be Git tracked for the project. Initialization also performs environment synchronization. Environment Synchronization This process is part of the init operation, but on its own it ensures local configs and assets are up to date with their source templates. Install the dependencies (if not done already) bundle install Sync configuration files bundle exec rake labdev:sync:all For configuration details, see DocOps Lab Dev-tooling Configuration. See DocOps Lab Dev-tooling Usage for operational details."
    },{
      "id": 29,
      "url": "/docs/lab-dev-usage/",
      "title": "Dev-tooling Usage",
      "content": "This guide pertains to the docopslab-dev environment. For complete documentation, see the project&#8217;s README. The environment described and provided here is not optimized for DocOps Lab applications used in third-party projects. For your own applications of DocOps Labs products like ReleaseHx and Issuer, see DocOps Box for a full-featured docs-focused workspace, runtime, and production environment. For full setup instructions, see DocOps Lab Dev-tooling Setup. For configuration details, see DocOps Lab Dev-tooling Configuration. This gem mainly supplies rake tasks for performing common development operations across unified configurations and sub-libraries. Standard Usage Run all linters bundle exec rake labdev:lint:all Auto-fix safe issues bundle exec rake labdev:heal Docker Usage The container runs with a base command of /bin/bash in interactive mode. Any command you pass it will assume you are starting at a normal prompt, with the exception of rake, which will always convert to bundle exec rake. Other Ruby commands will either need an explicit lab-dev bundle exec or may run without Bundler, like asciidoctor (globally installed for Vale availability) and bundle itself. Non-Ruby commands like vale and shellcheck are immediately available. First time in a DocOps Lab project lab-dev rake labdev:sync:all Regular development workflow lab-dev rake labdev:sync:all lab-dev rake labdev:lint:all lab-dev rake labdev:heal Irregular commands lab-dev vale --config .config/vale.ini README.adoc lab-dev bundle exec rubocop --config .config/rubocop.yml --only Style/StringLiterals lab-dev asciidoctor -o tmp/docs.html README.adoc Interactive shell for debugging lab-dev The Docker container persists gems on the host machine in the local .bundle/ path for performance. All tools use the host project&#8217;s Gemfile for version consistency. Make sure container-managed paths are not tracked in Git. Add .config/.vendor/ and .bundle/ to .gitignore. Override Commands Most executions of the packaged tools are handled through Rake tasks, but you can always run them directly, especially to pass arguments not built into the tasks. RuboCop bundle exec rubocop --config .config/rubocop.yml [options] bundle exec rubocop --config .config/rubocop.yml --auto-correct-all bundle exec rubocop --config .config/rubocop.yml --only Style/StringLiterals Vale vale --config=.config/vale.ini [options] [files] vale --config=.config/vale.ini README.adoc vale --config=.config/vale.ini --minAlertLevel=error . HTMLProofer bundle exec htmlproofer --ignore-urls \"/www.github.com/,/foo.com/\" ./_site More Example Commands Lint specific Ruby file with specific rule bundle exec rake 'labdev:lint:ruby[lib/myfile.rb,Style/StringLiterals]' Lint all AsciiDoc files for a specific Vale rule bundle exec rake 'labdev:lint:adoc[,DocOpsLab-Authoring.ExNotEg]' Lint specific shell script bundle exec rake 'labdev:lint:bash[scripts/docksh]' Lint specific file with Vale (text mode) bundle exec rake 'labdev:lint:text[_docs/myfile.adoc]' Show a specific lint rule profile bundle exec rake 'labdev:show:rule[vale,RedHat]' Tasks and Workflow bundle exec rake --tasks | grep labdev: To hide the labdev: tasks from the standard rake --tasks output for an integrated project, use: bundle exec rake --tasks | grep -v labdev: Typical Workflow This tool is for working on DocOps Lab projects or possibly unrelated projects that wish to follow our methodology. A typical workflow might look as follows. Normal development git add . git commit -m \"Add new feature\" + This should yield warnings and errors if active linters find issues. Auto-fix what you can. bundle exec rake labdev:heal Review the changes. git diff Commit the fixes. git add -A git commit -m \"Auto-fix linting issues\" Handle any remaining manual fixes. bundle exec rake labdev:lint:all Fix remaining issues manually. git add -A git commit -m \"Fix remaining linting issues\" Try pushing. git push If all blocking issues are cleared, the push should succeed. Otherwise, more cleanup is needed. Bypass the pre-push gates (usually to test or demo the failure at origin): git push --no-verify"
    },{
      "id": 30,
      "url": "/docs/product-change-docs/",
      "title": "Product Change Tracking and Documentation",
      "content": "All DocOps Lab products use DocOps Lab&#8217;s ReleaseHx utility to generate release notes and changelogs. However, each product implements ReleaseHx in a customized manner, so always refer to any given project&#8217;s documentation for specific protocols. Check for a file like docs/content/_doc/release-history-management.adoc file or else fall back to README.adoc (search for ReleaseHx). Product Contributor Documentation Responsibilities Each contributor of product code or docs changes is responsible for preparing that change to be included in release documentation, when applicable. GitHub Issues Labels GitHub Issues are use specific labels to indicate documentation expectations. needs:docs The issue requires documentation updates as part of its resolution. Documentation updates will likely be in a sub-issue with a documentation label. needs:note The issue requires a note in the release history when resolved. Release notes are appended to the description body under ## Release Note. changelog The issue summary should be included in the changelog for the next release, even if no release note is included. Issues labeled changelog will automatically appear in the Changelog section of the Release History document. Release notes must be manually entered. Change Documentation When a change to the product affects user-facing functionality, the documentation needs to change. For early product versions, most documentation appears in the root README.adoc file. When a product has a docs/content/ path, documentation changes usually have a home in an AsciiDoc (.adoc) file in a subdirectory. Reference matter should be documented where it is defined, such as in specs/data/*.yml files. Release Note Entry User-facing product changes that deserve explanation (not just notice) require a release note. Add a release note for a given issue by appending it to the issue body following a ## Release Note heading. Example ## Release Note The content of the release note goes here, in Markdown format. Try to keep it to one paragraph with minimal formatting. Release History Document Creation ReleaseHx automatically generates release notes and changelogs from GitHub Issues and PRs when properly labeled. Every DocOps Lab project implements ReleaseHx differently as a way of &#8220;eating our own dog food&#8221;. Refer to any given project&#8217;s documentation for specific instructions on how to prepare changes for inclusion in release notes and changelogs. The general procedure is as follows: Generate a draft release history in YAML. bundle exec rhx &lt;version&gt; --yaml --fetch Edit the generated YAML to ensure clarity and completeness. Generate the Markdown version. bundle exec rhx &lt;version&gt; --md docs/release/&lt;version&gt;.md"
    },{
      "id": 31,
      "url": "/docs/release/",
      "title": "Release Process (General)",
      "content": "DocOps Lab projects follow a consistent, if always progressing, architecture and development/release process. This guide focuses on the release process. Prerequisites Technologies Overview DocOps Lab projects are primarily Ruby gems, usually with associated Docker images that provide proper environments for the CLI associated with the project gem. Ruby is used mainly because of its excellent AsciiDoc tooling through Asciidoctor, with an accompanying preference for Jekyll static-site generator and Liquid templating language, all of which are Ruby native. Docker is employed by internal developers and end users of DocOps Lab tooling alike, to reduce if not eliminate the Ruby maintenance overhead. Required Tools If you are brand new to the world of code, Git, and GitHub, DocOps Lab will soon have resources aimed precisely at your experience level. For now, some experience with these tools is assumed. Must-haves Docker, natively installed: Docker Desktop on MacOs Docker Desktop with WSL2 on Windows Docker Engine on Linux Git, natively installed Ruby runtime 3.2.7, natively or via Docker (recommended) GitHub account Should-haves GitHub CLI (gh) a code editor that supports Ruby, AsciiDoc, and YAML (Try VS Code if you don&#8217;t have a preference yet.) Ruby Environment Ruby dependencies are managed through Bundler using each project&#8217;s Gemfile/.gemspec definitions. A proper Ruby environment and all common (cross-project) development dependencies are supplied in the docopslab/dev Docker image. Containers run from this image provide unified linting, git hooks, and development tooling used by most DocOps Lab codebases. These quality-control tools are also built into all GitHub repos via GH Actions workflows, local availability is not required just to work on or contribute to a DocOps Lab codebase. See DocOps Lab Dev-tooling Setup for comprehensive setup details if you are initializing a new DocOps Lab project. Required Credentials Use environment variables to store sensitive credentials securely. These credentials are only needed during the release process, not the development phase. RubyGems API Key Location: https://rubygems.org/profile/edit Set as: RUBYGEMS_API_KEY Docker Hub Credentials Organization: docopslab DockerHub account with write permissions for docopslab images Set as: DOCKERHUB_USERNAME, DOCKERHUB_TOKEN GitHub Token (some projects only) Scope: repo Set as: DOCOPSLAB_GITHUB_TOKEN or GITHUB_TOKEN Platform Setup Deployment platforms must be initialized for each new project, as instructed in Product Artifact and Documentation Deployment Setup. Release Procedure Manual Double-Checks No local paths in Gemfile. All documentation changes merged. Version attribute bumped and propagated. Conditions (\"Definition of Done\") All target issues are closed. CI builds and tests pass on dev/x.y. Documentation updated and merged. Step 1. Prepare Release History Generate release notes and changelog using ReleaseHx. bundle update releasehx bundle exec releasehx &lt;$tok.majmin&gt;.&lt;$tok.patch&gt; --md docs/release/&lt;$tok.majmin&gt;.&lt;$tok.patch&gt;.md Edit the Markdown file at docs/release/&lt;$tok.majmin&gt;.&lt;$tok.patch&gt;.md. This step may vary significantly depending on project&#8217;s implementation of ReleaseHx. See the project&#8217;s README.adoc; seek for releasehx. Step 2. Merge to Main git checkout main git pull origin main git merge --no-ff dev/&lt;$tok.majmin&gt; git push origin main Step 3. Tag Release git tag -a v&lt;$tok.majmin&gt;.&lt;$tok.patch&gt; -m \"Release &lt;$tok.majmin&gt;.&lt;$tok.patch&gt;\" git push origin v&lt;$tok.majmin&gt;.&lt;$tok.patch&gt; Step 4. Create GitHub Release Use the GitHub CLI to create a release: gh release create v&lt;$tok.majmin&gt;.&lt;$tok.patch&gt; --title \"Release &lt;$tok.majmin&gt;.&lt;$tok.patch&gt;\" --notes-file docs/releases/&lt;$tok.majmin&gt;.&lt;$tok.patch&gt;.md --target main Or else use the GitHub web interface to manually register the release, and copy/paste the contents of docs/releasehx/&lt;$tok.majmin&gt;.&lt;$tok.patch&gt;.md into the release notes field. Step 5. Publish Remaining Artifacts Use the publish.sh script with proper credentials in place. ./scripts/publish.sh This step concludes the release process. Post-Release Tasks Cut a release branch for patching (release/&lt;$tok.majmin&gt;). Update :next_prod_vrsn: in docs. Create next development branch (dev/&lt;next&gt;). Notify stakeholders. Patch Procedure Rollback Failsafe If a release must be rolled back and retracted, you must revert the changes and &#8220;yank&#8221; the artifacts. git tag -d v&lt;$tok.majmin&gt;.&lt;$tok.patch&gt; git push origin :refs/tags/v&lt;$tok.majmin&gt;.&lt;$tok.patch&gt; git revert -m 1 &lt;merge-commit&gt; git push origin main Retract or yank the artifacts (DockerHub, RubyGems, etc) and nullify the GH release. gh release delete v&lt;$tok.majmin&gt;.&lt;$tok.patch&gt; gem yank --version &lt;$tok.majmin&gt;.&lt;$tok.patch&gt; &lt;gemname&gt; docker rmi &lt;image&gt;:&lt;$tok.majmin&gt;.&lt;$tok.patch&gt; Be sure to un-publish any additional artifacts specific to the project. Standard Patching Perform patch work against the earliest affected release/x.y. These examples use 1.1, 1.2, and 1.2.1 as example versions. Patch development procedure git checkout release/1.1 git checkout -b fix/parser-typo #  FIX  git add . git commit -m \"fix: correct parser typo\" git push origin fix/parser-typo #  TEST  git checkout release/1.1 git merge --squash fix/parser-typo git commit -m \"fix: correct parser typo\" git push origin release/1.1 git tag -a v1.2 -m \"Patch release 1.2\" git push origin v1.2 Example forward porting procedure git checkout release/1.2 git cherry-pick &lt;commit-hash&gt; #  TEST  git push origin release/1.2 git tag -a v1.2.1 -m \"Patch release 1.2.1\" git push origin v1.2.1 Be sure to change 1.1, 1.2, and 1.2.1 to the actual affected branches and versions. Repeat for every affected branch then release the patched versions. Between minor versions, patch versions may vary due to inconsistent applicability of patches. Patch Releasing Perform Steps 1, 4, and 5 of the standard release procedure: Step 1. Prepare Release History Step 4. Create GitHub Release Step 5. Publish Remaining Artifacts"
    },{
      "id": 32,
      "url": "/docs/templates/AGENTS.md/",
      "title": "AGENTS.md Template",
      "content": "AGENTS.md AI Agent Guide for &lt;% Project Name %&gt; development. TEMPLATE NOTICES This document is a TEMPLATE. It is intended for DocOps Lab projects, but you are welcome to use it for your unrelated work. Copy it to AGENTS.md or similar in your project repository and modify it to suit your project. This template is published as a rendered document at https://docopslab.org/docs/templates/AGENTS.md just for transparencys sake. All are welcome to do what DocOps Lab does and commit/share your version of AGENTS.md, which is inspired by https://agents.md as a standard for AI agent prompting. NOTE: The version of this document you are reading is a template meant to be copied and customized for each project it is used on. Search for characters like &lt;% and change those placeholders to suit the specific project. NOTE: Use the raw version of this file instead of the rendered version. IMPORTANT: Remove this entire section of the document before committing it to Git. AI Agency As an LLM-backed agent, your primary mission is to assist a human OPerator in the development, documentation, and maintenance of &lt;% Project Name %&gt; by following best practices outlined in this document. Philosophy: Documentation-First, Junior/Senior Contributor Mindset As an AI agent working on &lt;% Project Name %&gt;, approach this codebase like an inquisitive and opinionated junior engineer with senior coding expertise and experience. In particular, you values: Documentation-first development: Always read the docs first, understand the architecture, then propose solutions at least in part by drafting docs changes Investigative depth: Do not assume: investigate, understand, then act. Architectural awareness: Consider system-wide impacts of changes. Test-driven confidence: Validate changes; dont break existing functionality. User-experience focus: Changes should improve the downstream developer/end-user experience. Operations Notes IMPORTANT: This document is augmented by additional agent-oriented files at .agent/docs/. Be sure to tree .agent/docs/ and explore the available documentation: skills/: Specific techniques for upstream tools (Git, Ruby, AsciiDoc, GitHub Issues, testing, etc.) topics/: DocOps Lab strategic approaches (dev tooling usage, product docs deployment) roles/: Agent specializations and behavioral guidance (Product Manager, Tech Writer, DevOps Engineer, etc.) missions/: Cross-project agent procedural assignment templates (new project setup, conduct-release, etc.) NOTE: Periodically run bundle exec rake labdev:sync:docs to generate/update the library. For any task session for which no mission template exists, start by selecting an appropriate role and relevant skills from the Agent Docs library. Local Override Priority: Always check docs/{_docs,topics,content/topics}/agent/ for project-specific agent documentation that may override or supplement the universal guidance. Ephemeral/Scratch Directory There should always be an untracked .agent/ directory available for writing paged command output, such as git diff &gt; .agent/tmp/current.diff &amp;&amp; cat .agent/tmp/current.diff. Use this scratch directory as you may, but dont get caught up looking at documents you did not write during the current session or that you were not pointed directly at by the user or other docs. Typical subdirectories include: docs/: Generated agent documentation library (skills, roles, topics, missions) tmp/: Scratch files for current session logs/: Persistent logs across sessions (ex: task run history) reports/: Persistent reports across sessions (ex: spellcheck reports) team/: Shared (Git-tracked) files for multi-agent/multi-operator collaboration AsciiDoc, not Markdown DocOps Lab is an AsciiDoc shop. All READMEs and other user-facing docs, as well as markup inside YAML String nodes, should be formatted as AsciiDoc. Agents have a frustrating tendency to create .md files when users do not want them, and agents also write Markdown syntax inside .adoc files. Stick to the AsciiDoc syntax and styles you find in the README.adoc files, and you wont go too far wrong. ONLY create .md files for your own use, unless Operator asks you to. Essential Reading Order (Start Here!) Before making any changes, read these documents in order: 1. Core Documentation ./README.adoc Main project overview, features, and workflow examples: Pay special attention to any AI prompt sections (// tag::ai-prompt[]// end::ai-prompt[]) Study the example CLI usage patterns Review &lt;% project-slug %&gt;.gemfile and Dockerfile for dependencies and environment context 2. Architecture Understanding ./specs/tests/README.adoc Test framework and validation patterns: Understand the test structure and helper functions See how integration testing works with demo data Note the current test coverage and planned expansions 3. Practical Examples &lt;% TODO: Where to find example files and demo data %&gt; 4. Agent Roles and Skills README.adoc section: == Development Use tree .agent/docs/ for index of roles, skills, and other topics pertinent to your task. Codebase Architecture Core Components &lt;% TODO: Base-level file tree and comments %&gt; Auxiliary Components These components (modules, scripts, etc) are to be spun off as their own gems after a later &lt;% Project Name %&gt; release: &lt;% TODO: Tree for lib/side-modules %&gt; Configuration System &lt;% Most DocOpsLab projects use a common configuration management pattern:  delete this section otherwise %&gt; Default values: Defined in specs/data/config-def.yml User overrides: Via .&lt;% project-slug %&gt;.yml or --config flag Defined in lib/&lt;% project-slug %&gt;/configuration.rb: Configuration class loads and validates configs Uses SchemaGraphy::Config and SchemaGraphy::CFGYML: For schema validation and YAML parsing No hard-coded defaults outside config-def.yml: All defaults come from the Configuration class; whether in Liquid templates or Ruby code expressing config properties, any explicit defaults will at best duplicate the defaults set in config-def.yml and propagated into the config object, so avoid expressing || 'some-value' in Ruby or | default: 'some-value' in Liquid for core product code. Agent Development Approach Before starting development work: Adopt an Agent Role: If the Operator has not assigned you a role, review .agent/docs/roles/ and select the most appropriate role for your task. Gather Relevant Skills: Examine &lt;% agent_docs_path | default: '.agent/docs/' %&gt;skills/ for techniques needed: Understand Strategic Context: Check &lt;% agent_docs_path | default: '.agent/docs/' %&gt;topics/ for DocOps Lab approaches to development tooling and documentation deployment Read relevant project documentation for the area youre changing For substantial changes, check in with the Operator - lay out your plan and get approval for risky, innovative, or complex modifications Working with Demo Data &lt;% TODO: Instructions for using demo data/repo to validate changes %&gt; General Agent Responsibilities Question Requirements: Ask clarifying questions about specifications. Propose Better Solutions: If you see architectural improvements, suggest them. Consider Edge Cases: Think about error conditions and unusual inputs. Maintain Backward Compatibility: Dont break existing workflows. Improve Documentation: Update docs when adding features. Test Thoroughly: Use both unit tests and demo validation. DO NOT assume you know the solution to anything big. Cross-role Advisories During planning stages, be opinionated about: Code architecture and separation of concerns User experience, especially: CLI ergonomics Error handling and messaging Configuration usability Logging and debug output Documentation quality and completeness Test coverage and quality When troubleshooting or planning, be inquisitive about: Why existing patterns were chosen Future proofing and scalability What the user experience implications are How changes affect different API platforms Whether configuration is flexible enough What edge cases might exist Remember &lt;% TODO: Reiterate the user base and mission of the project %&gt; Your primary mission is to improve &lt;% Project Name %&gt; while maintaining operational standards: Reliability: Dont break existing functionality Usability: Make interfaces intuitive and helpful Flexibility: Support diverse team workflows and preferences Performance: Respect system limits and optimize intelligently Documentation: Keep the docs current and comprehensive Most importantly: Read the documentation first, understand the system, then propose thoughtful solutions that improve the overall architecture and user experience."
    },{
      "id": 33,
      "url": "/docs/universal-attributes/",
      "title": "Universal Project Attributes",
      "content": "These are the current URLs of all live DocOps Lab Projects, formatted as copy-pastable to a given project&#8217;s README.adoc. // tag::universals[] :docopslab_hub_url: https://github.com/DocOps :docopslab_domain: docopslab.org // end::universals[]"
    },{
      "id": 34,
      "url": "/metablog/lab-projects-source-site/",
      "title": "Single Sourcing and Rendering DocOps Lab Project Profiles",
      "content": "Much of this website consists of representations of DocOps Lab projects, in different orders and forms. Project metadata is stored in the _data/docops-lab-projects.yml file, which is the canonical source of truth about the numerous codebases DocOps Lab maintains. At least, it represents the sole source of most of the data it contains, though at any given point some part of that file is likely out of sync with reality. Nevertheless, some aspects of this sourcing and output strategy are pretty extreme and therefore worth blogging about. Source Files Effectively all of the information about DocOps Lab projects on this site is single-sourced, though some of it is sourced (once) in different places. Most of it is sourced in the docops-lab-projects.yml file, which we will dive into shortly. The content of the code snippets used in this post is subject to change. When the blog next builds, the live data will be transcluded into the code blocks on this page, and the text around them should remain accurate. The above note is single sourced at _snippets-may-change-note.adoc so it can be embedded in multiple posts. A small amount is sourced in the README.adoc file, as data attributes. This includes the text for each of the Featured Projects cards on the landing page, which are also expressed in the README file itself. Snippet from README.adoc :docops-box-desc: A Docker-containerized environment and shell script for reducing the complexity of setting up developer tools. Non-developers can run a single command (`docksh run`) and instantly access whole runtimes and specialized documentation tools in a pre-configured shell environment. :docs-as-code-school-desc: Structured education in modern technical documentation and document processing. Starting with &#8220;Deep Semantics&#8221; (Fall 2025?) and expanding to courses on version management, code-like workflow adoption, and legal document operations, this project uses docs-as-code to teach docs-as-code principles. :ayl-docstack-desc: AsciiDoc. YAML. Liquid. A three-language approach to managing complex, multi-variant documentation. This &#8220;tech stack&#8221; maximizes power while minimizing syntax overhead, making advanced documentation techniques accessible to beginners while remaining powerful enough for enterprise needs. :schemagraphy-desc: Extends YAML through SGYML and accompanying libraries, providing advanced data typing and document transclusion capabilities. Provides a full-featured schema language that allows users to define complex data structures, document structures, and whole interfaces in a single, unified format. :jekyll-asciidoc-ui-desc: A set of Jekyll plugins and themes that enrich AsciiDoc web output. Includes themes like AsciiDocsy and Just The AsciiDocs, plus plugins for Jekyll-OpenAPI integration, adocBook document converter, and 25 UI extensions for AsciiDoc. Data Structure The docops-lab-projects.yml file is basically a large array of tabular data. Each project&#8217;s record has a number of parameters. - name: DocOps Box slug: docops-box type: environment desc: | Everything needed for managing customized Ruby, Node.js, Python, and Pandoc environments via Docker containers, ready for interactive work. Including the popular ZShell with handy configurations. Alternatively builds a production-ready (\"live\") image for automation/deployment. line: Up and running with Ruby, Node, Zsh, Pandoc, and Git for document operations with interactive and production-ready Docker images. star: true page: true done: 90% vrsn: 0.1.0 wave: 0 card: readme icon: container tech: [Docker,Bash,Zsh,Ruby,Git,Pandoc,Node.js,Python] tags: [development,automation,containers,virtualization,Dockerized] sort: 1 I think most of the above data record is mostly self-explanatory, but a few fields merit highlighting. I also added a concept of custom cards, which are defined in another file: _data/cards.yml. As of now, it contains only one card record. - name: Issuer and ReleaseHx type: utility card: | Issue-ticket creation and release-history management tools that integrate with Jira, GitHub, and GitLab. Bulk-create work items from a single YAML file, then generate release notes and changelogs in AsciiDoc, Markdown, YAML, JSON, HTML, or PDF formats at release time. icon: logs sort: 6 href: /blog/issuer-releasehx-news/ This is all pulled together using simple Liquid assignments and filters. {% assign starred_projects = site.data.docops-lab-projects.projects | where: 'star', true %} {% assign featured_projects = starred_projects | concat: site.data.cards | sort: 'sort' %} Project Profiles Project profiles come in micro, mini, and page sizes. The micros are for card-based representations, such as the Featured Projects section on the landing page, or pages that group projects in ways that might involve repeated instances of the same project in the output, such as the Projects by Technology and Projects by Category pages. The minis are for the project detail sections that appear on the Projects Overview page, which includes a lot more information about each project, but still in a compact format. Finally, some projects have their own dedicated pages, such as Docs-as-Code School. This is the most comprehensive representation of a project. I can even re-create one here in this page using Liquid templates. DocOps Box 0.1.0 Up and running with Ruby, Node, Zsh, Pandoc, and Git for document operations with interactive and production-ready Docker images. A Docker-containerized environment and shell script for reducing the complexity of setting up developer tools. Non-developers can run a single command (docksh run) and instantly access whole runtimes and specialized documentation tools in a pre-configured shell environment. Type: environment Status: 90% Wave: 0 # development # automation # containers # virtualization # Dockerized Docker Bash Zsh Ruby Git Pandoc Node.js Python The above &#8220;mini-profile&#8221; expression was called with this Liquid code: {% include project-profile.html slug='docops-box' %} Note how the style for this page remains largely intact in affecting the profile rendering."
    },{
      "id": 35,
      "url": "/metablog/tech-blogging-in-asciidoc/",
      "title": "Why Tech Blogging is Better in AsciiDoc",
      "content": "This post is an ongoing experiment demonstrating the &#8220;front end&#8221; output produced by various expressions of AsciiDoc syntax, and it will show off some of the CSS and JavaScript work I&#8217;ve done in this area. The styling and effects you see here are not &#8220;vanilla&#8221; AsciiDoc output. After rendering AsciiDoc to HTML, it still needs to be styled with CSS, and most interactive effects need to be defined with CSS or JavaScript. Styling and effects are outside this post, but you can find the sources for all of this at https://github.com/DocOps/lab/tree/main/_sass/asciidoc.scss and https://github.com/DocOps/lab/tree/main/assets/js/main.js. Table of Contents The Examples Example Block Example Transclusion Examples Admonition Examples Callout Example Blockquote Examples Table Examples Description List Example Other List Types Ordered Lists Unordered Lists Sidebar Example Examples Wrapup AsciiDoc is Better than What, Though? Markdown vs AsciiDoc Hyperlink Syntax Code Blocks Syntax? (Nope) HTML Embedding? (Nope) Strikethrough Syntax (Gotcha, AsciiDoc!) Conclusion The Examples It&#8217;s time to examine the AsciiDoc awesome sauce. Example Block Example First, let&#8217;s address something you don&#8217;t see every day in tech blogging: an actual, semantically (and visually) distinct block for exhibiting examples. Example block containing a code block I typically use example blocks to show results, but you can also use them for code listings as well. Let&#8217;s make this one a combo. name: example array: - item1 - item2 - item3 How that was coded&#8230;&#8203; .Example block containing a code block ==== I typically use example blocks to show _results_, but you can also use them for code listings as well. Let's make this one a combo. [source,yaml] ---- name: example array: - item1 - item2 - item3 ---- ==== We will be using example blocks throughout this post to show off rendered output, in contrast to the code blocks we&#8217;ll use to expose the source of those blocks. Transclusion Examples The AsciiDoc include:: directive is awesome. It embeds the content of another file&#8201;&#8212;&#8201;in whole or in part&#8201;&#8212;&#8201;into a parent document. Such includes are also used throughout the source for this post. Nearly every example of block content in this post is transcluded from an adjacent file called _asciidoc-snippets.adoc, which is subvidivded using AsciiDoc include tags, demarcated like so: An example of tagged content This part of the document will not be included. // tag::segment-name[] Everything between tagged lines is included in the \"calling\" document. // end::segment-name[] Nothing outside the tags will be included in a tagged include directive The desired line of AsciiDoc source in a nearby or even online file can be transcluded with a line like: Embedding tagged content include::_asciidoc-snippets.adoc[tag=\"segment-name\"] Which resolves as&#8230;&#8203; Everything between tagged lines is included in the \"calling\" document. But this works for files of any other format, as well. I use it a lot for YAML files. Take a YAML file that looks like this: properties: # tag::arbitrary-tagname[] - key: tagged-content description: | This exemplifies demarcated tagging in YAML, for inclusion into AsciiDoc files. status: awesome! # end::arbitrary-tagname[] Use a tagged include directive&#8230;&#8203; [source,yaml] include::_metablog/_asciidoc-yaml-snippet.yml[tag=arbitrary-tagname] Those 2 lines of code result in: - key: tagged-content description: | This exemplifies demarcated tagging in YAML, for inclusion into AsciiDoc files. status: awesome! Admonition Examples What some systems call &#8220;alerts&#8221; or &#8220;callouts&#8221; or &#8220;notices&#8221;, AsciiDoc calls &#8220;admonitions&#8221;. Example: Let&#8217;s kick things off with an example of a simple note admonition. This is a NOTE admonition with a Lucide info circle icon. It should display with a blue color scheme and an information icon. It can contain multiple paragraphs, like this second one. How it was made&#8230;&#8203; Let's kick things off with an example of a simple note admonition. [NOTE] ==== This is a NOTE admonition with a Lucide info circle icon. It should display with a blue color scheme and an information icon. It can contain multiple paragraphs, like this second one. ==== That wasn&#8217;t simple enough for you? Check this out&#8230;&#8203; One-liner admonition markup Those `====` demarcation characters and the `[NOTE]` heading are only necessary when there is something special about your admonition content, such as multiple paragraphs or distinct styling. TIP: Normally, this works just fine to create an admonition block. And blogging in standardized lightweight markup doesn't get much simpler than that. Renders&#8230;&#8203; Those ==== demarcation characters and the [NOTE] heading are only necessary when there is something special about your admonition content, such as multiple paragraphs or distinct styling. Normally, this works just fine to create an admonition block. And blogging in standardized lightweight markup doesn&#8217;t get much simpler than that. Callout Example Speaking of &#8220;callouts&#8221;&#8201;&#8212;&#8201;when AsciiDoc uses that term, it means annotations inside a code block. Example of a code listing with callouts def hello_world message=\"Hello, world!\", case=\"upper\" (1) message_cased = case == \"upper\" ? message.upcase : message.downcase (2) puts message_cased (3) end 1 defines the method with default parameters 2 processes the message based on the case parameter 3 outputs the processed message If you highlight the contents and use your operating system&#8217;s copy-to-clipboard, or if you click the copy button, you&#8217;ll see that only the source code copies, not the callout numbers. Click here to reveal a textarea box you can paste into See? No callouts were picked up in the copy operation! The code that made that code: [source,ruby] ---- def hello_world message=\"Hello, world!\", case=\"upper\" # message_cased = case == \"upper\" ? message.upcase : message.downcase # puts message_cased # end ---- defines the method with default parameters processes the message based on the case parameter outputs the processed message Blockquote Examples Block quotes are a staple of blogging. Here are two of my favorite ways to use them. Blockquote with citation This is a blockquote with a citation. Not something you use in tech writing very often, but quite common in all kinds of blogging. &#8212; Dr Meta Blockquote + citation code: [quote, Dr Meta] This is a blockquote with a citation. Not something you use in tech writing very often, but quite common in all kinds of blogging. I have always loved blockquotes, so I made my own style that can be designated with a simple role argument in AsciiDoc. Blockquote with a pullquote role This is some text that I want to highlight as a pull quote. I&#8217;ve styled it to be distinct from a standard blockquote, and I use them to preview interesting content that appears later in a post. Pullquote code: [quote,role=pullquote] This is some text that I want to highlight as a pull quote. I've styled it to be distinct from a standard blockquote, and I use them to preview interesting content that appears later in a post. Table Examples There is no question that AsciiDoc&#8217;s lightweight table markup is excellent. Simple table code: .This table has an automatically numbered caption [cols=\"1,1,1\",options=\"header\"] |=== | Header 1 | Header 2 | Header 3 | Cell 1 | Cell 2 | Cell 3 | Cell 4 | Cell 5 | Cell 6 | Cell 7 | Cell 8 | Cell 9 |=== Simple table Table 1. This table has an automatically numbered caption Header 1 Header 2 Header 3 Cell 1 Cell 2 Cell 3 Cell 4 Cell 5 Cell 6 Cell 7 Cell 8 Cell 9 AsciiDoc table syntax doesn&#8217;t rely on mimicking the shape of the table or its columns and cells, so you can break it down and organize the markup in a way that makes sense to you. AsciiDoc does not require you to make ASCII art like some table syntaxes. But if you need to make it complex, you can do that too. Now let&#8217;s get a little crazy&#8230;&#8203; Table 2. DocOps Lab  Table of Delightfully Over-Engineered Examples Multiverse Documentation Planning Matrix  Because simple tables are for mortals Persona Toolkit Sample Artifact KPI End-User Pain points: Where is the button? Whats a token? Why is my coffee YAML-flavored? AsciiDoc + Antora Quickstart Guide Keep it friendly. If the docs feel like a boss fight, you already lost.  Time-to-First-Success  Admin User Paligo + Snagit Troubleshooting Article  Support Tickets/1k users   Sprint 42: Docs Ops Experiments  Developer AsciiDoc + Make + PlantUML Code-Adjacent HOWTO def deploy env='prod' puts \"Shipping to #{env}\" end  Build Failures Blamed on Docs  Jekyll + Kramdown API Changelog Sections grouped by part Rendered to HTML/PDF Includes {%- include -%} sorcery  PRs Merged w/ Docs  Antora + Lunr Docs as Feature RFC If it isnt documented, did it even ship? &#8212; A slightly dramatic PM  Feature Discoverability   Release Train: 2.3 Quantum Marmot  Composite Artifact: One Doc to Rule Them All Installation (for Humans) API (for Machines) Architecture (for Historians) This cell spans three columns. The rightmost cell is independent.  Adoption Rate  Internal Auditor AsciiDoc + CSV + a dash of awk Compliance Matrix (SOC 2, ISO 27001)  = 2.3 [1] Site Reliability Engineer Runbook Pack Incident Bundle Playbooks Postmortem template Pager-Yoga breathing guide  Incident Response Time  In conclusion: Tables are for communicating structure, not suffering. Use colspans/rowspans judiciously, and never apologize for beautiful grids. Click to expose the source code for the crazy table .DocOps Lab  Table of Delightfully Over-Engineered Examples [cols=\"^1,&lt;2,3,&gt;1\",width=100%,options=\"header,footer\",frame=all,grid=all,stripes=even] |=== 4+^h| Multiverse Documentation Planning Matrix  Because simple tables are for mortals ^s| Persona ^s| Toolkit ^s| Sample Artifact ^s| KPI //  A multirow persona cell with block content; spans 2 body rows. &lt;a| *End-User* *Pain points*: * Where is the button? * Whats a token? * Why is my coffee YAML-flavored? | AsciiDoc + Antora a| Quickstart Guide TIP: Keep it friendly. If the docs feel like a boss fight, you already lost. ^|  *Time-to-First-Success*  | *Admin User* | Paligo + Snagit | Troubleshooting Article |  *Support Tickets/1k users*  //  A section header row that spans all four columns. 4+^|  Sprint 42: Docs Ops Experiments  //  A persona cell that spans three rows; note the dot-prefix count for rowspan. .3+^| *Developer* | AsciiDoc + Make + PlantUML a| Code-Adjacent HOWTO [source,ruby] ---- def deploy env='prod' puts \"Shipping to #{env}\" end ---- ^|  *Build Failures Blamed on Docs*  | Jekyll + Kramdown a| API Changelog * Sections grouped by `part` * Rendered to HTML/PDF * Includes `{%- include -%}` sorcery ^|  *PRs Merged w/ Docs*  | Antora + Lunr a| Docs as Feature RFC [quote, 'A slightly dramatic PM'] ____ If it isnt documented, did it even ship? ____ ^|  *Feature Discoverability*  //  Another section header with full-row colspan. 4+^.^|  Release Train: 2.3 Quantum Marmot  //  Here we demonstrate a single row that contains a 3-column cell + one regular cell. 3+&lt;a| **Composite Artifact: One Doc to Rule Them All** * Installation (for Humans) * API (for Machines) * Architecture (for Historians) + NOTE: This cell spans *three* columns. The rightmost cell is independent. ^|  *Adoption Rate*  //  Fun with inline formatting, footnotes, and subscripts/superscripts. | *Internal Auditor* | AsciiDoc + CSV + a dash of awk | Compliance Matrix (SOC 2, ISO 27001) |  = 2.3^^ footnote:[Margin of error calculated by a friendly spreadsheet goblin.] //  A cell that spans two columns on the right; pair with two normal cells on the left. | *Site Reliability Engineer* | Runbook Pack &lt;a| *Incident Bundle* * Playbooks * Postmortem template * Pager-Yoga breathing guide ^|  *Incident Response Time*  //  Closing totals/notes row spanning all columns as a footer. 4+^s| In conclusion: Tables are for communicating *structure*, not suffering. Use colspans/rowspans judiciously, and never apologize for beautiful grids. |=== Yes, that really was all done with AsciiDoc markup. However, I am pleased to admit that ChatGPT actually wrote all of that content based on a prompt to create &#8220;really complex AsciiDoc table&#8221; and to &#8220;make the content interesting, maybe clever/cute/funny&#8221;. I was so shocked, I decided to use it as-is. Description List Example One feature of AsciiDoc that Markdown does not even attempt is the description or definition list: &lt;DL&gt; in HTML terms. Description list example AsciiDoc A lightweight markup language that is particularly well-suited for technical documentation and blogging, offering a rich set of features for structuring content. Markdown A widely-used lightweight markup language known for its simplicity and ease of use, but with fewer features compared to AsciiDoc. lightweight markup A category of markup languages designed to be easy to read and write in plain text, while still allowing for formatting and structuring of content. The underlying AsciiDoc markup AsciiDoc:: A lightweight markup language that is particularly well-suited for technical documentation and blogging, offering a rich set of features for structuring content. Markdown:: A widely-used lightweight markup language known for its simplicity and ease of use, but with fewer features compared to AsciiDoc. lightweight markup:: A category of markup languages designed to be easy to read and write in plain text, while still allowing for formatting and structuring of content. DLs are a semantically powerful format for &#8220;parameterized data&#8221;&#8201;&#8212;&#8201;I use it all the time in my docs, and whenever an LLM tries to do this with bulleting and bold, I cringe. The Markdown version: - **AsciiDoc:** A lightweight markup language that is particularly well-suited for technical documentation and blogging, offering a rich set of features for structuring content. Importantly, this will not generate a DL list, which means screen readers and interpreters may not even associate the bolded term with its description. Other List Types AsciiDoc also supports ordered lists (numbered), unordered lists (bulleted), and checklists (task lists), along with nesting of each type, even across types. Ordered Lists Ordered lists can be pretty powerful, in that you can start an re-start them at any number. Ordered list example An ordered list broken up by an admonition: Install the NPM packages. npm install Build the app. npm run build Your app is now built and ready to deploy! Deploy the app. npm run deploy Under the hood&#8230;&#8203; An ordered list broken up by an admonition: . Install the NPM packages. + [.prompt] npm install . Build the app. + [.prompt] npm run build [NOTE] Your app is now built and ready to deploy! [start=3] . Deploy the app. + [.prompt] npm run deploy Unordered Lists For unordered lists, AsciiDoc supports using either - or * characters for bullet markup. Nesting is done by adding bullets&#8201;&#8212;&#8201;no need to keep track of indentation. Unordered list example An unordered list with deep nesting square bullets First item Sub-item A Sub-sub-item i Sub-sub-item ii Sub-item B Second item Hyphen bullets with nested ordered list Another first-level item Nested with dashes Back to dashes at first level Final item The code: .An unordered list with deep nesting square bullets [square] * First item [square] ** Sub-item A *** Sub-sub-item i *** Sub-sub-item ii ** Sub-item B * Second item .Hyphen bullets with nested ordered list - Another first-level item . Nested with dashes . Back to dashes at first level - Final item Sidebar Example Now we get to what might be my favorite AsciiDoc feature, which is a lovely holdover from DocBook: sidebars. Sidebar example This would be mainline text that pertains very directly and prominently to the subject. The reader is reading or scanning along, but at this point the author wants to at least introduce some auxiliary content that is relevant but not strictly necessary to the main point, or not a priority at this stage. A sidebar about sidebar authoring Sidebars are for textbooks and magazines, at least in the sense of being placed alongside the main content to provide additional context, definitions, or related information. In HTML, sidebars tend to be for navigation and others static content like masthead info. But the concept of a content-adjacent sidebars is useful in all kinds of technical writing, including blogging. Sidebars are for content that is pinned or otherwise relevant to the main topic, but which might be a distraction if dumped inline. It doesn&#8217;t quite belong as a section or even a subsection. The content is not totally required knowledge, but it relates to the content and is not yet a whole other lesson. You want readers to at least be able to make note of it, even at risk of interrupting their reading flow. In AsciiDoc, sidebars can contain just about any other block or inline content. When it comes to layout, sidebars should either be collapsed or they should actually be pinned to a bar that runs alongside the content. And in either case, they should be dismissable and stashable so a reader can skip them now but come back to them later. Below that sidebar element, the mainline text would resume, and the reader can choose to read the sidebar, stash it for later, or skip it altogether. You can also just stick all your sidebars at the end of the post like little appendices, optionally linking to them from the point in the text where they are relevant. Click to see how the sidebar was coded This would be mainline text that pertains very directly and prominently to the subject. The reader is reading or scanning along, but at this point the author wants to at least introduce some auxiliary content that is relevant but not strictly necessary to the main point, or not a priority at this stage. .A sidebar about sidebar authoring **** Sidebars are for textbooks and magazines, at least in the sense of being placed alongside the main content to provide additional context, definitions, or related information. In HTML, sidebars tend to be for navigation and others static content like masthead info. But the _concept_ of a content-adjacent sidebars is useful in all kinds of technical writing, including blogging. Sidebars are for content that is pinned or otherwise relevant to the main topic, but which might be a distraction if dumped inline. It doesn't quite belong as a section or even a subsection. The content is not totally required knowledge, but it relates to the content and is not yet a whole other lesson. You want readers to at least be able to make note of it, even at risk of interrupting their reading flow. In AsciiDoc, sidebars can contain just about any other block or inline content. When it comes to layout, sidebars should either be collapsed or they should actually be pinned to a bar that runs alongside the content. And in either case, they should be dismissable and stashable so a reader can skip them now but come back to them later. **** Below that sidebar element, the mainline text would resume, and the reader can choose to read the sidebar, stash it for later, or skip it altogether. You can also just stick all your sidebars at the end of the post like little appendices, optionally linking to them from the point in the text where they are relevant. Examples Wrapup Those were several of my favorite semantic block elements supported by AsciiDoc. Not even mentioned were the enumerable inline semantic features like inline roles and what can be done with them on the front end, nor AsciiDoc&#8217;s terrific built-in support for footnotes[2]. Both the DocOps Blog and this MetaBlog will continue to explore AsciiDoc syntax and output from time to time, but those were some of the highlights. However, this post is not done yet, and neither are the AsciiDoc examples. We still have to answer the big question&#8230;&#8203; AsciiDoc is Better than What, Though? Well, it&#8217;s definitely better than rich-text editors (&#8220;WYSIWYG&#8221;) like those typically used in WordPress and other blogging platforms&#8201;&#8212;&#8201;at least if your subject matter is code. I guess you might be a tech blogger who writes about gadgets, in which case I have no idea, you do you. But since the vast majority of blogging about coding is done in Markdown, let&#8217;s compare AsciiDoc to the leading brand. Markdown vs AsciiDoc Admittedly, I have never blogged in Markdown, but since AsciiDoc is basically a superset of Markdown, you&#8217;d be hard-pressed to find advantages of Markdown itself. There surely are probably syntax situations where Markdown has the edge over AsciiDoc, though I am only aware of a couple. I&#8217;ll take you through them here, with some meanderings planned along the way. Hyperlink Syntax Let&#8217;s look at some Markdown. You're reading along in Markdown and then you come to a link like [Asciidoctor](https://asciidoctor.org), and that seems natural Whereas in AsciiDoc, you're reading along and then https://asciidoctor.org[Asciidoctor] BAM! you got hit with a URL that didn't really matter. Mind you, both of these markups generate the same HTML output: Asciidoctor. But I have to admit, I have come to slightly prefer the label-first Markdown approach, mainly for the effect when I&#8217;m reading the text as code, even though it uses two extra characters. Click here for a mega-meta mini tangent The optimal lightweight link syntax I tend to write AsciiDoc links in the optionally more explicit format: link:https://asciidoctor.org[Asciidoctor] Which makes me wish the following invalid format were actually proper: link:[Asciidoctor,href=https://asciidoctor.org] Alas, no lightweight markup is perfect&#8230;&#8203; What other ways is Markdown syntax thought to be stronger than AsciiDoc? Code Blocks Syntax? (Nope) You may be thinking you prefer Markdown&#8217;s simple ``` notation for demarcating code blocks, which are after all among the most frequently employed elements in blogging about code. Not so fast. If you prefer that style, Asciidoctor supports it perfectly. Take a look: Here is the effect of using just 3 backticks to sandwich literal content. Two code blocks sourced using Markdown-style syntax in AsciiDoc def hello_world(): print(\"Hello, world!\") This was written in a code block denoted with `+++```+++` literals. Under the hood&#8230;&#8203; ```python def hello_world(): print(\"Hello, world!\") ``` ```asciidoc This was written in a code block denoted with `+++```+++` literals. ``` What&#8217;s up with the +++ markup? You are probably noticing something funny about that syntax. All those extra +++ characters might look like something went wrong, but they&#8217;re actually one of the countless advantages of AsciiDoc markup. Those are &#8220;passthrough&#8221; indicators, which I used outside the code block above to show you 3 backticks in a row without triggering the standard effects of backticks in both AsciiDoc and Markdown. I&#8217;ll do it again here: ```. I could also simplify this by making it an attribute and just calling it later with a placeholder. :3ticks: ``` Here come the backticks: `{3ticks}` Which renders: Here come the backticks: ``` When it comes to literal blocks, which are typically used to represent plain text or terminal input/output in docs, AsciiDoc has two or three ways of expressing this element, whereas Markdown has only one that I am aware of. Converters tend to render this as a &lt;pre&gt; block without a &lt;code&gt; inset. This is a literal block, the syntax for which was a simple 1-space indent .... This literal block is fenced by four dots (periods), which allows for non-contiguous multiline text. .... [literal,role=prompt,title=Properly labeled command literal] Use a heading line if you wish to assign a role or other options. Literal blocks rendered This is a literal block, the syntax for which was a simple 1-space indent This literal block is fenced by four dots (periods), which allows for non-contiguous multiline text. Properly labeled command literal Use a heading line if you wish to assign a role or other options. There really is no guaranteed Markdown equivalent to this, but you can try something like what follows: This line was indented by 4 spaces to indicate a literal. You can keep adding lines that are indented by 4 spaces. Some converters will render this embedded in `&lt;code&gt;` tags anyway. If you want your literal blocks to be handled distinctly from code blocks, or if you want to do anything cool with them or add caption/labels, you&#8217;re going to have to jerry rig it in Markdown, probably by adding explicit HTML. HTML Embedding? (Nope) Speaking of passthroughs, you may have been thinking all along that you prefer the way Markdown supports HTML tags throughout. You can just write &lt;div class=\"foo\"&gt; and it works, right? Cool. Same with AsciiDoc. You just use those passthrough markers inline, or the 4-char version (++++) for a block. Example of HTML passthrough You're looking at HTML right now, sourced inside an AsciiDoc file. It can be used for anything, including to embed adorable SVG images you come up with after an all-night coding session, like this: Cute! And the code for that&#8230;&#8203; ++++ &lt;!-- HTML block with a gray background and white text and an embedded SVG file of a cute logo --&gt; &lt;div style=\"background-color: #414141; color: #333; padding: 10px; border-radius: 5px;\"&gt; &lt;p style=\"font-weight: 900\"&gt;You're looking at HTML right now, sourced inside an AsciiDoc file.&lt;/p&gt; &lt;p&gt;It can be used for anything, including to embed adorable SVG images you come up with after an all-night coding session, like this:&lt;/p&gt; &lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"100\" height=\"100\" viewBox=\"0 0 100 100\"&gt; &lt;circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"#ffcc00\"/&gt; &lt;text x=\"50%\" y=\"50%\" font-size=\"20\" text-anchor=\"middle\" fill=\"#333\" dy=\".3em\"&gt;Cute!&lt;/text&gt; &lt;/svg&gt; &lt;/div&gt; ++++ So, anything you can do in an HTML page, you can do in AsciiDoc files. Strikethrough Syntax (Gotcha, AsciiDoc!) Okay, okay, I realize I only gave one example of where Markdown syntax is superior, and then I slipped right back into showing how great AsciiDoc is. So I will give you truthfully the only other syntax advantage of Markdown over AsciiDoc that I am aware of, and that is bloggers&#8217; favorite, ubiquitous strikethrough effect. In Markdown, it&#8217;s fun and easy: This is ~~strikethrough~~ text. That same syntax in AsciiDoc produces: This is ~strikethrough~ text. Oof. Not cool, AsciiDoc. An inline role is required to do this in my beloved format: This is the built-in role for [line-through]#strikethrough#. This is an alternate method [.strike]#for slightly# with fewer characters. Which renders: This is the built-in role for strikethrough. This is an alternate method for slightly with fewer characters. Then again, if you&#8217;re into using inline highlighting in your text, AsciiDoc makes that trivial with #inline highlighting# syntax, while you have to use &lt;mark&gt;inline highlighting&lt;/mark&gt; HTML tags in Markdown. Sorry, I realize I have done it again, but this is why I&#8217;m so confident that if you like blogging in Markdown, you will love blogging in AsciiDoc. Conclusion AsciiDoc is nearly as advantageous for blogging about code as it is for officially documenting software. You can do so much more without missing literally anything you can do with Markdown. I encourage anyone to provide other examples of superior Markdown syntax or capability. The biggest thing going for Markdown is surely that every programming language and code-oriented content platform supports some flavor of it. That is a huge plus for Markdown, but AsciiDoc&#8217;s widespread advantages make it worth setting up if you blog in any of these SSGs. I would say it&#8217;s even worth choosing or migrating to a platform from that list, with Jekyll, Hugo, and 11ty being my favorites for blogging. 1. Margin of error calculated by a friendly spreadsheet goblin. 2. The text of a footnote is recorded inline but appears collected at a place in the page of your choosing. Now click the number of this footnote to return!"
    },{
      "id": 36,
      "url": "/projects/adocbook/",
      "title": "adocBook",
      "content": "Publishing tool that paginates AsciiDoc documents into chunked sites, inspired by mdBook Technically a plugin for Jekyll SSG that paginates AsciiDoc documents, inspired by mdBook. Basically, it breaks a book down into a website, wrapping it in Just the AsciiDocs theme. Project Details Version: 0.1.0 Status: 85% Complete Wave: 1 Technologies Jekyll AsciiDoc Ruby Liquid HTML CSS Categories Jekyll plugin pagination documentation Dockerized API CLI Depends on Just the AsciiDocs AsciiDoc-friendly extension of Just the Docs Jekyll theme with dark mode Project Profile Jekyll Extensions Front-end assets and components for adding AsciiDoc support to Jekyll themes Project Profile"
    },{
      "id": 37,
      "url": "/projects/asciidoc-edu/",
      "title": "AsciiDoc EDU",
      "content": "Framework for authoring curriculum with AsciiDoc, YAML, and Liquid The Educational Document Utilities framework for authoring and maintaining curriculum matter, such as textbooks, workbooks, slide presentations, quizzes, exams, grades, and so forth. Share curriculum in forkable repos, which other instructors can use to adapt to their own situations. Generate study materials, presentations, and tests, all from a single source of truth. Will extract framework from docs-as-code-school repo. Project Details Version: 0.1.0 Status: 30% Complete Wave: 3 Note: Currently developed as part of Docs-as-Code School. Technologies YAML AsciiDoc Liquid Git Reveal.js HTML Categories education curriculum presentations Depends on clide Domain-customized CLI for automating document management workflows Project Profile LiquiDoc Template parser and build tool for Liquid and AsciiDoc, scriptable with YAML Project Profile SchemaGraphy A framework for defining, validating, and parsing data and text objects using YAML and AsciiDoc. Project Profile FormaGraphy Generate dynamic HTML5 forms from YAML definitions with jQuery backing Project Profile AYL DocStack An open-source tech stack, frameworks, and guidance for planning and implementing professional docs-as-code projects Project Profile"
    },{
      "id": 38,
      "url": "/projects/asciidoc-ops/",
      "title": "AsciiDoc Ops for Tech Docs",
      "content": "True single-sourcing framework for technical documentation sites, slides, and PDFs using AsciiDoc, YAML, and Liquid True single-sourcing framework for managing technical documents using YAML, AsciiDoc, Jekyll, Liquid, and clide. Builds modern websites and PDFs. Enables highly customized instances with version control, scripted builds, integrated search. Includes schemas for YAML objects and lots of Liquid templates for generating YAML, AsciiDoc, and HTML output. Project Details Version: 0.1.0 Status: 50% Complete Wave: 3 Technologies YAML AsciiDoc Liquid Git Jekyll Ruby Categories technical writing single-sourcing Depends on clide Domain-customized CLI for automating document management workflows Project Profile LiquiDoc Template parser and build tool for Liquid and AsciiDoc, scriptable with YAML Project Profile SchemaGraphy A framework for defining, validating, and parsing data and text objects using YAML and AsciiDoc. Project Profile AYL DocStack An open-source tech stack, frameworks, and guidance for planning and implementing professional docs-as-code projects Project Profile"
    },{
      "id": 39,
      "url": "/projects/ayl-docstack/",
      "title": "AYL DocStack",
      "content": "An open-source tech stack, frameworks, and guidance for planning and implementing professional docs-as-code projects AsciiDoc. YAML. Liquid. A three-language approach to managing complex, multi-variant documentation. This &#8220;tech stack&#8221; maximizes power while minimizing syntax overhead, making advanced documentation techniques accessible to beginners while remaining powerful enough for enterprise needs. Project Details Version: V1 Status: 90% Complete Wave: 1 Content Explainers AYL Manifesto 95% References Matrix of SSGs that support AsciiDoc markup 100% API Decision Matrix 100% Free, Open Source Software Licenses 99% Glossary 80% Style guides Docs-as-Code Style Guides Content Semantics 90% Block Semantics 90% Inline Semantics 90% Technologies AsciiDoc YAML Liquid Git Docker Categories tech stack lightweight markup Depends on DocOps Box Up and running with Ruby, Node, Zsh, Pandoc, and Git for document operations with interactive and production-ready Docker images. Project Profile"
    },{
      "id": 40,
      "url": "/projects/docops-box/",
      "title": "DocOps Box",
      "content": "Up and running with Ruby, Node, Zsh, Pandoc, and Git for document operations with interactive and production-ready Docker images. A Docker-containerized environment and shell script for reducing the complexity of setting up developer tools. Non-developers can run a single command (docksh run) and instantly access whole runtimes and specialized documentation tools in a pre-configured shell environment. Project Details Version: 0.1.0 Status: 90% Complete Wave: 0 Technologies Docker Bash Zsh Ruby Git Pandoc Node.js Python Categories development automation containers virtualization Dockerized"
    },{
      "id": 41,
      "url": "/projects/docs-as-code-school/",
      "title": "Docs-as-Code School",
      "content": "Learn to author and manage documents the way progrmamers do. Structured education in modern technical documentation and document processing. Starting with &#8220;Deep Semantics&#8221; (Fall 2025?) and expanding to courses on version management, code-like workflow adoption, and legal document operations, this project uses docs-as-code to teach docs-as-code principles. Project Details Version: 0.1.0 Status: 60% Complete Wave: 1 Content Courses Deep Semantics: Structure, Markup, Styling, and Effects for Technical Content 80% Divergence Handling: Versioning Strategy for Software Products and Documentation 60% Work Like a Coder: Exploit Developers' Tools and Code-like Documentation Practices 40% Code the Docs: Dynamism, Structure, and Semantics for Technical Writing with AsciiDoc 10% DocOps for Law: Managing Legal Documents the Coder's Way 10% Docs as Defs: Using YAML to Define and Document UIs and APIs 20% Next Level READMEs: Seeding and Single-Sourcing Codebases with AsciiDoc 10% Lessons Internal Docs: First-classing and Single-sourcing Documentation Across the Public/Private Divide 50% Tutorials Using Jekyll for OpenAPI Documentation Delivery 50% Mapping Your Product and Docs Versioning Scheme with YAML 30% Supercharge Your Jekyll Docs Site 0% README.adoc-Driven Development and Documentation (RADDD) 0% Modern Documentation from Structured AsciiDoc 0% Technologies AsciiDoc YAML Liquid Git GitHub Reveal.js HTML CSS/Sass JavaScript Categories education training technical writing versioning semantics Dockerized"
    },{
      "id": 42,
      "url": "/projects/issuer/",
      "title": "Issuer",
      "content": "Bulk create issues in Jira and GitHub from YAML definitions A CLI for bulk creating issues in cloud-based systems like Jira and GitHub Issues. Introduces an open standard called IMYML for defining individual issue entries and default settings such as global labels, milestones, and assignees. Project Details Version: 0.2.1 Status: Released Wave: 0 Technologies YAML IMYML GitHub Issues Jira Ruby Categories issues issue management CLI automation Dockerized API"
    },{
      "id": 43,
      "url": "/projects/jekyll-asciidoc-ui/",
      "title": "Jekyll AsciiDoc UI Extensions",
      "content": "Front-end assets and components for adding AsciiDoc support to Jekyll themes A set of Jekyll plugins and themes that enrich AsciiDoc web output. Includes themes like AsciiDocsy and Just The AsciiDocs, plus plugins for Jekyll-OpenAPI integration, adocBook document converter, and 25 UI extensions for AsciiDoc. Project Details Version: 0.1.0 Status: 70% Complete Wave: 1 Libraries Admonition Extension Enhances AsciiDoc admonition elements with truncation and semantic tags and icons. Asciidoctor Tabs The official Asciidoctor extension with performance similar to the tabbed-panes component. Enable this component to activate the Asciidoctor Tabs extension. Badge Turn any text element into a Bootstrap badge. Button Turn a link or span element into a button with Bootstrap styling. Card Transform an AsciiDoc block into a Bootstrap card component. Clipable Adds a &#8220;Copy&#8221; button with copy-to-clipboard functionality to designated elements/classes on the hover state. Code Highlight Universal syntax highlighting configuration for code listings. Code Truncate Optionally truncate code listings and literal blocks to a maximum number of lines, after which a \"Show more\" link will be displayed. Collapse Adds Bootstrap collapse functionality to sections of the document body. Content Typing System Adds a document type to the document body. This is a semantic tag that can be used to classify the document for search, navigation/architecture, display theming, and other purposes. GDPR Dialog Adds a GDPR-compliant cookie consent dialog to the site. The dialog is displayed to users on their first visit to the site, and can be dismissed or accepted. The dialog will not appear if there are no cookies listed. GDPR Enforce Keeps pages from planting cookies until/unless the user has accepted the cookie consent dialog. Adds a cookie called blocked-cookies that records the list of cookies not (yet) accepted by the user. Glossaries Adds glossary pages or documents to the site. The glossary page is a single page or series of pages listing and defining any terms added to the collection. Use config-terms-ext to auto-detect and classify terms in the document body. File Semantics Add the appropriate file icon to any mention of a file. Inline Term Semantics A term to highlight inline in the document body, optionally displaying or linking to a definition or description of that term. Literal Prompt Adds a prompt string to the beginning of a literal string or each line in a literal block. Literal Sudo Highlight For any line in a literal block that starts with sudo, wraps in a SPAN.sudo element. Property Handler Specially parses text designated with a role like ppty, indicating the content is to be treated like a reference to a property in the same reference context. Use the settings of ppty-ext to configure the syntax and behavior of this semantic. ReleaseHX Adds a release history page or document to the site. The release history a page or pages serializing the product releases as Release Notes and/or Changelog. ReleaseHX is configured in its own data file at site.data.ui.releasehx. Mardown Resourcer Generates an alternate Markdown file from the rendered HTML, reverse engineering back to a lightweight markup, especially designed for LLM (large language model) crawlers and RAG (retrieval-augmented generation) systems. Optionally (by default) reates a &lt;link rel=\"alternate\" type=\"text/markdown\" href=\"/alt/markdown.md\"&gt; link in the document head. SEO Extension Detects specifically classed terms or phrases in the document body for use in the keywords meta tag for search-engine optimization (SEO). Writes directly to the head element of the document (not JavaScript). Sidebar Extension Enhances AsciiDoc sidebar elements with truncation and semantic tags and icons. Optionally adds a \"move to\" button to each sidebar block, allowing users to send the sidebar to the bottom of the page for later reading. Tabbed Panes Enables showing and hiding blocks of content in place, usually for switching between language-variant versions of the same data or code. See also asciidoctor-tabs. Term Extension Uses glossary data to proactively highlight (or not) any terms found on a given page. Adds wrapper elements and classes to the DOM where matching terms are found. See config-glossary and config-term for context. Theme Extension Adds theme files to calling project&#8217;s assets. Alternate Table of Contents Infers a ToC from your rendered headlines instead of using the built-in Asciidoctor ToC for a given page. Trademarker Handle trademark (&#8482;) symbols and trademarked terms. Technologies Jekyll AsciiDoc Liquid YAML Bootstrap HTML CSS/Sass JavaScript OpenAPI Categories Jekyll plugin documentation UI components Bootstrap Depends on ReleaseHx Manage product release notes and changelog with CLI and Ruby API Project Profile AYL DocStack An open-source tech stack, frameworks, and guidance for planning and implementing professional docs-as-code projects Project Profile"
    },{
      "id": 44,
      "url": "/projects/jekyll-extender/",
      "title": "Jekyll Extender",
      "content": "Core Jekyll extension capabilities for Jekyll projects A plugin that extends Jekyll with core capabilities key to DocOps Lab Jekyll extension projects but broadly useful to Jekyll plugin and theme developers. Project Details Version: 0.1.0 Status: 80% Complete Wave: 1 Libraries migrate-assets config-defaults sgyml-support schemagraphy-filters Technologies Jekyll Ruby Liquid Categories Jekyll plugin documentation assets configuration Depends on SchemaGraphy A framework for defining, validating, and parsing data and text objects using YAML and AsciiDoc. Project Profile"
    },{
      "id": 45,
      "url": "/projects/legaldoc-ops/",
      "title": "LegalDoc Ops",
      "content": "Manage legal documents like code with AsciiDoc and YAML Framework for managing legal documents like code, with AsciiDoc and YAML. Includes client intake, metadata handling, and document/contract drafting, rendering, and even digital signing procedures. Even includes Creative Commons-licensed starter templates for end-of-life planning documents and employment contracts, with proceeds from professional use to benefit National Lawyers Guild. Project Details Version: 0.1.0 Status: 40% Complete Wave: 3 Technologies YAML AsciiDoc Liquid Git GPG Categories legal tech contracts document management legal writing Depends on clide Domain-customized CLI for automating document management workflows Project Profile LiquiDoc Template parser and build tool for Liquid and AsciiDoc, scriptable with YAML Project Profile SchemaGraphy A framework for defining, validating, and parsing data and text objects using YAML and AsciiDoc. Project Profile FormaGraphy Generate dynamic HTML5 forms from YAML definitions with jQuery backing Project Profile AYL DocStack An open-source tech stack, frameworks, and guidance for planning and implementing professional docs-as-code projects Project Profile"
    },{
      "id": 46,
      "url": "/projects/releasehx/",
      "title": "ReleaseHx",
      "content": "Manage product release notes and changelog with CLI and Ruby API A utility for managing product release history, notes, and changelog. Includes a Ruby API and CLI, releasehx/rhx. Includes SchemaGraphy prototype. Project Details Version: 0.1.0 Status: 90% Complete Wave: 0 Technologies YAML Ruby AsciiDoc GitHub Issues GitLab Issues Jira Liquid ERB JMESPath JSONPath Categories issues release history changelog product management versioning CLI API Dockerized"
    },{
      "id": 47,
      "url": "/projects/schemagraphy/",
      "title": "SchemaGraphy",
      "content": "A framework for defining, validating, and parsing data and text objects using YAML and AsciiDoc. Extends YAML through SGYML and accompanying libraries, providing advanced data typing and document transclusion capabilities. Provides a full-featured schema language that allows users to define complex data structures, document structures, and whole interfaces in a single, unified format. Project Details Version: V1-beta Status: 40% Complete Wave: 2 Technologies SGYML YAML JSON JMESPath JSONPath JSON Schema AsciiDoc Ruby Categories schemas data text validation parsing serialization Dockerized API Depends on Sourcerer Ruby API for pre-build and build-time AsciiDoc/YAML processing Project Profile"
    }]